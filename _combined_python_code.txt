========================================================================
  Combined Python Files from Git Repository: /home/mit/persona_rag_chatbot
  Generated on: Thu 07 Aug 2025 08:47:20 AEST
========================================================================



########################################################################
### FILE: src/__init__.py
########################################################################




########################################################################
### FILE: src/agent.py
########################################################################

# FILE: src/agent.py
# V10.1 (Final Production Architecture): Implements a robust, state-aware, and
# strategic agent with graceful degradation and intelligent tool flow.
# - Prioritizes the Knowledge Graph for suitable queries, bypassing vector search
#   and re-ranking if a definitive answer is found. This fixes factual lookups.
# - Uses ThreadPoolExecutor for efficient, parallel execution of sub-queries.
# - Implements full resilience against transient API errors (e.g., 503 Overloaded).
# - Corrected the ValueError on variable unpacking.

import json
import logging
import time
import re
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor

from google.api_core import exceptions as google_exceptions
from src.tools.clients import get_generative_model, get_flash_model, DEFAULT_REQUEST_OPTIONS
from src.models import ToolResult, QueryMetadata, ToolPlanItem
from src.planner.query_classifier import QueryClassifier
from src.planner.tool_planner import ToolPlanner
from src.planner.persona_classifier import PersonaClassifier
from src.planner.query_rewriter import QueryRewriter
from src.router.tool_router import ToolRouter
from src.prompts import DECOMPOSITION_PROMPT, REASONING_SYNTHESIS_PROMPT, DIRECT_SYNTHESIS_PROMPT, RERANKING_PROMPT, SUMMARIZATION_PROMPT

logger = logging.getLogger(__name__)
LOG_PATH = Path("trace_logs.jsonl")

def extract_json_from_response(text: str) -> dict | list:
    """Finds and parses the first valid JSON object or array from a string."""
    match = re.search(r'```json\s*([\[\{].*?[\]\}])\s*```', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        logger.warning(f"Could not parse JSON from response text: {text}")
        return {}


class Timer:
    def __init__(self, name): self.name = name
    def __enter__(self): self.start = time.perf_counter(); return self
    def __exit__(self, *args): self.end = time.perf_counter(); logger.info(f"[TIMER] {self.name} took {(self.end - self.start) * 1000:.2f} ms")

def log_trace(query: str, persona: str, query_meta: QueryMetadata, tool_plan: List[ToolPlanItem], tool_results: List[ToolResult], final_answer: str, total_latency_sec: float):
    trace_record = { "timestamp": datetime.utcnow().isoformat() + "Z", "query": query, "persona": persona, "intent": query_meta.intent if query_meta else "classification_failed", "graph_suitable": query_meta.question_is_graph_suitable if query_meta else "unknown", "tool_plan": [t.model_dump() for t in tool_plan] if tool_plan else [], "tool_results": [r.model_dump() for r in tool_results] if tool_results else [], "final_answer_preview": final_answer[:200] + "..." if final_answer else "N/A", "total_latency_sec": round(total_latency_sec, 3) }
    try:
        with open(LOG_PATH, "a", encoding="utf-8") as f: f.write(json.dumps(trace_record) + "\n")
    except Exception as e:
        logger.error(f"Failed to write to trace log: {e}", exc_info=True)


class Agent:
    def __init__(self, confidence_threshold: float = 0.85):
        self.classifier = QueryClassifier()
        self.planner = ToolPlanner(coverage_threshold=confidence_threshold)
        self.router = ToolRouter()
        self.persona_classifier = PersonaClassifier()
        self.rewriter = QueryRewriter()
        self.llm = get_generative_model('gemini-1.5-pro-latest')
        self.synthesis_llm = get_flash_model('gemini-1.5-flash-latest')
        self.reranker_llm = get_flash_model('gemini-1.5-flash-latest')
        
    def _call_llm_with_retry(self, model, prompt: str) -> Optional[str]:
        """A wrapper for generate_content that handles API retries and timeouts gracefully."""
        if not model:
            logger.error("LLM model is not available.")
            return None
        try:
            response = model.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            return response.text
        except google_exceptions.RetryError as e:
            logger.error(f"API call timed out after multiple retries: {e}")
            return None
        except Exception as e:
            logger.error(f"An unexpected error occurred during LLM call: {e}", exc_info=True)
            return None

    def _rerank_with_gemini(self, query: str, documents: List[str]) -> List[str]:
        if not self.reranker_llm or not documents: return documents
        formatted_docs = "\n\n".join([f"DOCUMENT[{i}]:\n{doc}" for i, doc in enumerate(documents)])
        prompt = RERANKING_PROMPT.format(question=query, documents=formatted_docs)
        
        with Timer("Re-ranking with Gemini"):
            response_text = self._call_llm_with_retry(self.reranker_llm, prompt)
            if response_text is None:
                logger.warning("Gemini re-ranking failed due to API issues. Falling back to top 5.")
                return documents[:5]

            best_indices = extract_json_from_response(response_text)
            if not isinstance(best_indices, list):
                logger.warning("Gemini re-ranker did not return a list. Falling back.")
                return documents[:5]
            
            reranked_docs = [documents[i] for i in best_indices if i < len(documents)]
            logger.info(f"Re-ranked {len(documents)} snippets down to {len(reranked_docs)} using Gemini.")
            return reranked_docs

    def _run_single_rag_step(self, query: str, persona: str) -> Tuple[str, QueryMetadata, List[ToolPlanItem], List[ToolResult]]:
        with Timer(f"Single RAG Step for '{query[:30]}...'"):
            query_meta = self.classifier.classify(query)
            if not query_meta: return "I had trouble understanding the query.", None, [], []

            tool_plan = self.planner.plan(query_meta, persona)
            if not tool_plan: return "I don't have a strategy for this query.", query_meta, [], []
            
            # --- START OF DEFINITIVE FIX: Intelligent, Conditional Tool Flow ---
            final_results = []
            kg_success = False
            # Step 1: Prioritize the Knowledge Graph if it's suitable and planned
            if query_meta.question_is_graph_suitable and any(t.tool_name == "query_knowledge_graph" for t in tool_plan):
                kg_result = self.router.execute_tool("query_knowledge_graph", query, query_meta)
                final_results.append(kg_result)
                # Step 2: Evaluate the KG result. If it's a "golden" answer, we are done with retrieval.
                if kg_result.success and kg_result.content and len(kg_result.content.strip()) > 10:
                    logger.info("Knowledge Graph provided a definitive answer. Bypassing vector search and re-ranking.")
                    kg_success = True

            # Step 3: Fallback to Vector Search ONLY if the KG failed or wasn't suitable
            if not kg_success and any(t.tool_name == "vector_search" for t in tool_plan):
                vector_result = self.router.execute_tool("vector_search", query, query_meta)
                final_results.append(vector_result)
            # --- END OF DEFINITIVE FIX ---

            all_docs = []
            for res in final_results:
                if res and res.success and res.content and res.content.strip():
                    splitter = "\n---\n" if res.tool_name == "vector_search" else "\n"
                    all_docs.extend(res.content.split(splitter))
            
            all_docs = [doc for doc in all_docs if doc and doc.strip()]

            if not all_docs:
                return "I searched but could not find any relevant details.", query_meta, tool_plan, final_results

            if not kg_success:
                ranked_docs = self._rerank_with_gemini(query, all_docs)
            else:
                ranked_docs = all_docs
                
            if not ranked_docs:
                return "I found some information, but it did not seem relevant.", query_meta, tool_plan, final_results
            
            evidence_texts, citation_links = [], []
            for doc in ranked_docs:
                parts = doc.split("\nCitation: ")
                evidence_texts.append(parts[0])
                if len(parts) > 1:
                    citation_links.append(parts[1])

            formatted_context = "\n\n".join([f"EVIDENCE [{i+1}]:\n{text}" for i, text in enumerate(evidence_texts)])
            
            if query_meta.intent == "simple_summary":
                synthesis_prompt = SUMMARIZATION_PROMPT.format(context_str=formatted_context)
            else:
                synthesis_prompt = DIRECT_SYNTHESIS_PROMPT.format(question=query, context_str=formatted_context)
            
            with Timer("Synthesis LLM Call (Flash)"):
                answer_text = self._call_llm_with_retry(self.synthesis_llm, synthesis_prompt)
            
            if answer_text is None:
                raise google_exceptions.RetryError("Synthesis LLM failed due to API overload.", cause=None)
            
            final_response = answer_text.strip()
            if query_meta.intent != "simple_summary":
                used_indices = {int(m) - 1 for m in re.findall(r'\[(\d+)\]', answer_text)}
                if used_indices:
                    references_section = "\n\n**References**\n"
                    unique_used_links = set()
                    used_links_ordered = []
                    for idx in sorted(list(used_indices)):
                        if idx < len(citation_links):
                            link = citation_links[idx]
                            if link not in unique_used_links:
                                unique_used_links.add(link)
                                used_links_ordered.append(link)
                    references_section += "\n".join([f"{i+1}. {link}" for i, link in enumerate(used_links_ordered)])
                    final_response += references_section

            return final_response, query_meta, tool_plan, final_results

    def run(self, query: str, persona: str, chat_history: List[str]) -> str:
        run_start_time = time.perf_counter()
        final_answer, final_query_meta, final_tool_plan, final_tool_results = "", None, [], []
        try:
            with Timer("Full Agent Run"):
                rewritten_query = self.rewriter.rewrite(query, chat_history)
                
                chosen_persona_key = self.persona_classifier.classify(rewritten_query)
                if chosen_persona_key is None:
                    logger.warning("Persona classification failed due to API error. Defaulting to 'regulatory_specialist'.")
                    chosen_persona_key = "regulatory_specialist"
                chosen_persona = chosen_persona_key if persona == "automatic" else persona

                persona_display_name = " ".join(word.capitalize() for word in chosen_persona.split("_"))
                
                with Timer("Decomposition"):
                    decomp_prompt = DECOMPOSITION_PROMPT.format(chat_history="\n- ".join(chat_history), question=rewritten_query)
                    decomp_response_text = self._call_llm_with_retry(self.synthesis_llm, decomp_prompt)
                
                if decomp_response_text is None:
                    logger.warning("Decomposition failed due to API error. Degrading to single-step plan.")
                    plan_data = {"requires_decomposition": False, "plan": [rewritten_query]}
                else:
                    plan_data = extract_json_from_response(decomp_response_text)

                requires_decomposition = plan_data.get("requires_decomposition", False)
                plan = plan_data.get("plan", [rewritten_query])

                if not requires_decomposition or len(plan) <= 1:
                    logger.info(f"Executing single-step plan for query: '{plan[0]}'")
                    synthesis_result, final_query_meta, final_tool_plan, final_tool_results = self._run_single_rag_step(plan[0], chosen_persona)
                else:
                    logger.info(f"Executing multi-step plan for query: '{rewritten_query}'")
                    scratchpad = []
                    
                    retrieval_steps = []
                    logic_instruction = ""
                    LOGIC_KEYWORDS = ["identify", "compare", "contrast", "common", "difference", "both"]
                    
                    for sub_q in plan:
                        if any(keyword in sub_q.lower() for keyword in LOGIC_KEYWORDS):
                            logic_instruction = sub_q
                        else:
                            retrieval_steps.append(sub_q)
                    
                    if not retrieval_steps and logic_instruction:
                        retrieval_steps = plan

                    with ThreadPoolExecutor(max_workers=len(retrieval_steps) or 1) as executor:
                        sub_futures = [executor.submit(self._run_single_rag_step, sub_q, chosen_persona) for sub_q in retrieval_steps]
                        sub_results_list = [future.result() for future in sub_futures]
                    
                    # --- START OF DEFINITIVE FIX: Corrected result unpacking ---
                    for i, sub_q in enumerate(retrieval_steps):
                        # The function returns 4 items, we need to unpack all of them
                        sub_answer, _, _, sub_tool_results = sub_results_list[i]
                        observation = f"Observation for the question '{sub_q}':\n{sub_answer}"
                        scratchpad.append(observation)
                        if sub_tool_results: final_tool_results.extend(sub_tool_results)
                    # --- END OF DEFINITIVE FIX ---

                    if logic_instruction and logic_instruction not in retrieval_steps:
                        scratchpad.append(f"Final Instruction: {logic_instruction}")

                    with Timer("Reasoning Synthesis LLM Call (Pro)"):
                        synthesis_prompt = REASONING_SYNTHESIS_PROMPT.format(question=rewritten_query, scratchpad="\n\n---\n\n".join(scratchpad))
                        synthesis_result = self._call_llm_with_retry(self.llm, synthesis_prompt)
                        if synthesis_result is None:
                           raise google_exceptions.RetryError("Final synthesis LLM failed due to API overload.", cause=None)

                final_answer = f"Acting as a **{persona_display_name}**, here is what I found:\n\n{synthesis_result}" if persona == "automatic" else synthesis_result
                return final_answer
        except google_exceptions.RetryError as e:
            logger.error(f"A recoverable API error occurred and timed out: {e}", exc_info=False)
            return "I'm sorry, the AI models I rely on are currently experiencing high traffic. Please try your question again in a few moments."
        except Exception as e:
            logger.error(f"An unexpected error occurred during agent run: {e}", exc_info=True)
            return "I encountered a critical error processing your request. Please check the system logs for more details."
        finally:
            run_end_time = time.perf_counter()
            log_trace(query, persona, final_query_meta, final_tool_plan, final_tool_results, final_answer, run_end_time - run_start_time)


########################################################################
### FILE: src/common/utils.py
########################################################################

# FILE: src/common/utils.py
import yaml
from pathlib import Path

def load_config(file_path: str) -> dict:
    with open(Path(file_path), 'r') as f:
        return yaml.safe_load(f)


########################################################################
### FILE: src/fallback.py
########################################################################

# FILE: src/fallback.py
# Phase 3.1: FallbackLayer — graceful UX when all tools return empty or fail

import logging
from typing import List
from src.models import ToolResult

logger = logging.getLogger(__name__)

FALLBACK_QUESTIONS = [
    "Would you like to compare two drugs instead?",
    "Can I help you find a sponsor for a specific medicine?",
    "Do you want to search the original PDF documents directly?"
]


def should_trigger_fallback(results: List[ToolResult]) -> bool:
    """
    Returns True if all tools failed or produced no meaningful content.
    """
    if not results:
        logger.info("Fallback triggered: no tool results returned.")
        return True

    empty_or_failed = [r for r in results if not r.success or not r.content or len(r.content.strip()) < 5]
    if len(empty_or_failed) == len(results):
        logger.info("Fallback triggered: all tools failed or content was empty.")
        return True

    return False


def render_fallback_message(query: str) -> str:
    """
    Returns a polite fallback message with suggested next questions.
    """
    msg = f"""
I'm sorry — based on the current documents and tools, I couldn't find sufficient information to answer your question:

"{query}"

However, here are some things you can try next:

"""
    for i, q in enumerate(FALLBACK_QUESTIONS, start=1):
        msg += f"{i}. {q}\n"

    msg += "\nYou can also try rephrasing your question for better results."
    return msg.strip()



########################################################################
### FILE: src/models.py
########################################################################

from pydantic import BaseModel, Field
from typing import List, Optional, Literal


QueryIntent = Literal[
    "specific_fact_lookup",
    "simple_summary",
    "comparative_analysis",
    "general_qa",
    "unknown"
]


class QueryMetadata(BaseModel):
    intent: QueryIntent
    keywords: List[str]
    question_is_graph_suitable: bool
    themes: Optional[List[str]] = Field(default_factory=list, description="High-level themes for metadata filtering.")


class ToolPlanItem(BaseModel):
    tool_name: str
    estimated_coverage: float


class ToolResult(BaseModel):
    tool_name: str
    success: bool
    content: str
    estimated_coverage: float = 0.0


class ContextItem(BaseModel):
    content: str
    source: Optional[dict] = None


class Source(BaseModel):
    type: str
    document_id: Optional[str] = None
    page_numbers: Optional[List[int]] = None
    source_url: Optional[str] = None
    retrieval_score: Optional[float] = None
    query: Optional[str] = None

class NamespaceConfig(BaseModel):
    namespace: str
    weight: float
    top_k: int

class RetrievalPlan(BaseModel):
    namespaces: List[NamespaceConfig] = []


########################################################################
### FILE: src/planner/__init__.py
########################################################################




########################################################################
### FILE: src/planner/persona_classifier.py
########################################################################

# FILE: src/planner/persona_classifier.py
# V2.1 (Final Production Grade): Completes the prompt centralization refactoring.
# This file now correctly imports the PERSONA_CLASSIFICATION_PROMPT from the
# central `src.prompts` module, making the system more maintainable and robust.

import logging
from typing import Literal, Optional

from google.api_core import exceptions as google_exceptions
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS
# --- START OF DEFINITIVE FIX: Import from the central prompts module ---
from src.prompts import PERSONA_CLASSIFICATION_PROMPT
# --- END OF DEFINITIVE FIX ---


logger = logging.getLogger(__name__)

Persona = Literal["clinical_analyst", "health_economist", "regulatory_specialist"]


class PersonaClassifier:
    def __init__(self):
        self.llm = get_flash_model()
        if not self.llm:
            logger.error("FATAL: Gemini client not initialized, PersonaClassifier will not work.")

    def _call_llm_with_retry(self, prompt: str) -> Optional[str]:
        """A wrapper for generate_content that handles API retries and timeouts gracefully."""
        if not self.llm:
            logger.error("LLM model for PersonaClassifier is not available.")
            return None
        try:
            response = self.llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            return response.text
        except google_exceptions.RetryError as e:
            logger.error(f"PersonaClassifier API call timed out after multiple retries: {e}")
            return None # Return None to signal a recoverable failure
        except Exception as e:
            logger.error(f"An unexpected error occurred during PersonaClassifier LLM call: {e}", exc_info=True)
            return None

    def classify(self, query: str) -> Optional[Persona]:
        """
        Classifies the query and returns the most appropriate persona key.
        Returns None if the API call fails irrecoverably.
        """
        if not self.llm:
            return "regulatory_specialist"

        prompt = PERSONA_CLASSIFICATION_PROMPT.format(question=query)
        persona_key_text = self._call_llm_with_retry(prompt)
        
        if persona_key_text is None:
            # API call failed after retries, signal this to the agent
            return None 
            
        persona_key = persona_key_text.strip()

        if persona_key in ["clinical_analyst", "health_economist", "regulatory_specialist"]:
            logger.info(f"Query classified for persona: '{persona_key}'")
            return persona_key
        else:
            logger.warning(f"Persona classification returned an invalid key: '{persona_key}'. Falling back to default.")
            return "regulatory_specialist"


########################################################################
### FILE: src/planner/query_classifier.py
########################################################################

# FILE: src/planner/query_classifier.py
# V2.0 (Resilient Architecture): Refactored to use a centralized, resilient LLM
# calling mechanism. Can now gracefully handle transient API errors (e.g., 503
# Overloaded) by returning None, allowing the agent to terminate the run cleanly.

import logging
import json
import re
from typing import Optional

from google.api_core import exceptions as google_exceptions
from src.models import QueryMetadata
from src.prompts import QUERY_CLASSIFICATION_PROMPT_V2 as QUERY_CLASSIFICATION_PROMPT
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS

logger = logging.getLogger(__name__)

def extract_json_from_response(text: str) -> dict:
    """Finds and parses the first valid JSON object from a string."""
    match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        logger.warning(f"Could not parse JSON from response text: {text}")
        return {}


class QueryClassifier:
    def __init__(self):
        self.model = get_flash_model()

    # --- START OF DEFINITIVE FIX: Resilient LLM Call Helper ---
    def _call_llm_with_retry(self, prompt: str) -> Optional[str]:
        """A wrapper for generate_content that handles API retries and timeouts gracefully."""
        if not self.model:
            logger.error("LLM model for QueryClassifier is not available.")
            return None
        try:
            response = self.model.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            return response.text
        except google_exceptions.RetryError as e:
            logger.error(f"QueryClassifier API call timed out after multiple retries: {e}")
            return None # Signal recoverable failure
        except Exception as e:
            logger.error(f"An unexpected error occurred during QueryClassifier LLM call: {e}", exc_info=True)
            return None
    # --- END OF DEFINITIVE FIX ---

    def classify(self, query: str) -> Optional[QueryMetadata]:
        if not self.model: return None
        logger.info(f"Classifying query: {query}")
        
        prompt = QUERY_CLASSIFICATION_PROMPT + f"\n\nUser Query: {query}"
        
        # --- START OF DEFINITIVE FIX: Use the resilient wrapper ---
        response_text = self._call_llm_with_retry(prompt)
        
        if response_text is None:
            logger.error("Query classification failed due to API issues.")
            return None # Propagate the failure signal
        # --- END OF DEFINITIVE FIX ---

        try:
            json_data = extract_json_from_response(response_text)
            
            if "intent" in json_data and json_data["intent"] == "comparison":
                json_data["intent"] = "comparative_analysis"
            
            metadata = QueryMetadata.model_validate(json_data)
            
            logger.info(f"Classification result: {metadata.model_dump_json(indent=2)}")
            return metadata
        except Exception as e:
            # This catches Pydantic validation errors or other unexpected issues
            logger.error(f"Query classification failed on response data: {e}", exc_info=True)
            return None


########################################################################
### FILE: src/planner/query_rewriter.py
########################################################################

# FILE: src/planner/query_rewriter.py
# V2.0 (Performance Fix): Added a heuristic check to bypass the LLM call for non-conversational queries.

import logging
from typing import List, Optional
# --- DEFINITIVE FIX: Import the config and model getter ---
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS

logger = logging.getLogger(__name__)

# --- DEFINITIVE FIX: Heuristic pre-flight check ---
# A list of common words that indicate a query is conversational and likely needs context from chat history.
# We check for these words (as whole words, case-insensitively) before making a slow API call.
CONVERSATIONAL_TRIGGERS = {"it", "its", "they", "them", "that", "those", "this", "these"}

REWRITE_PROMPT = """
You are an expert query analyst. Your task is to rewrite a user's latest question into a standalone question that can be understood without the context of the chat history.

**CRITICAL RULES:**
1.  **If the "Latest User Question" is already a complete, standalone question, you MUST return it exactly as it is.** Do not modify it.
2.  If the "Latest User Question" contains pronouns (like "it", "its", "they") or ambiguous references ("this drug", "that"), use the "Chat History" to resolve these references and create a complete question.
3.  Your output MUST be only the rewritten question. Do not add any commentary.

**EXAMPLE 1 (Rewrite Needed):**
- **Chat History:**
  - user: Who is the sponsor for Esketamine?
  - assistant: Janssen-Cilag Pty Ltd. is the sponsor for Esketamine.
- **Latest User Question:** what is its use?
- **Your Rewritten Question:** What is the use of Esketamine?

**EXAMPLE 2 (No Rewrite Needed):**
- **Chat History:**
  - user: Who is the sponsor for Esketamine?
  - assistant: Janssen-Cilag Pty Ltd. is the sponsor for Esketamine.
- **Latest User Question:** What is the dosage form for Fruquintinib?
- **Your Rewritten Question:** What is the dosage form for Fruquintinib?

**TASK:**
- **Chat History:**
{chat_history}
- **Latest User Question:** {question}

**Your Rewritten Question:**
"""

class QueryRewriter:
    def __init__(self):
        self.llm = get_flash_model()
        if not self.llm:
            logger.error("FATAL: Gemini client not initialized, QueryRewriter will not work.")

    # --- START OF DEFINITIVE FIX: Resilient LLM Call ---
    def _call_llm_with_retry(self, prompt: str) -> Optional[str]:
        """A wrapper for generate_content that handles API retries and timeouts gracefully."""
        if not self.llm:
            logger.error("LLM model for QueryRewriter is not available.")
            return None
        try:
            response = self.llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            return response.text
        except google_exceptions.RetryError as e:
            logger.error(f"QueryRewriter API call timed out after multiple retries: {e}")
            return None # Signal recoverable failure
        except Exception as e:
            logger.error(f"An unexpected error occurred during QueryRewriter LLM call: {e}", exc_info=True)
            return None
    # --- END OF DEFINITIVE FIX ---

    def rewrite(self, query: str, chat_history: List[str]) -> str:
        """Rewrites a conversational query into a standalone query, with a performance-enhancing pre-check."""
        if not self.llm or not chat_history:
            return query

        query_words = set(query.lower().split())
        if not CONVERSATIONAL_TRIGGERS.intersection(query_words):
            logger.info(f"Query deemed standalone. Bypassing LLM rewrite. Query: '{query}'")
            return query
        
        formatted_history = "\n  - ".join(chat_history)
        prompt = REWRITE_PROMPT.format(chat_history=formatted_history, question=query)
        
        # --- START OF DEFINITIVE FIX: Use the resilient wrapper ---
        rewritten_query_text = self._call_llm_with_retry(prompt)

        if rewritten_query_text is None:
            logger.warning("Query rewrite failed due to API issues. Using original query as fallback.")
            return query # Fallback to original query on API failure

        rewritten_query = rewritten_query_text.strip()
        # --- END OF DEFINITIVE FIX ---
        
        if rewritten_query:
            logger.info(f"Original query: '{query}' -> Rewritten query: '{rewritten_query}'")
            return rewritten_query
        else:
            logger.warning("Query rewrite resulted in an empty string. Using original query.")
            return query


########################################################################
### FILE: src/planner/tool_planner.py
########################################################################

# FILE: src/planner/tool_planner.py
# V2.0: Persona-Aware Tool Planner
import logging
import yaml
from pathlib import Path
from typing import List, Dict

from src.models import QueryMetadata, ToolPlanItem

logger = logging.getLogger(__name__)

# Base scores for tools based on query intent. A higher score is better.
# These tool names MUST match the names in `persona_tool_map.yml` and the tool router.
INTENT_TOOL_SCORES = {
    # If the user wants a specific fact...
    "specific_fact_lookup": {
        "query_knowledge_graph": 0.9,
        "retrieve_clinical_data": 0.7,
        "retrieve_general_text": 0.6,
        "retrieve_summary_data": 0.4,
    },
    # If the user wants a high-level summary...
    "simple_summary": {
        "retrieve_summary_data": 0.9,
        "retrieve_general_text": 0.8,
        "retrieve_clinical_data": 0.5,
        "query_knowledge_graph": 0.4,
    },
    # If the user wants to compare things...
    "comparative_analysis": {
        "retrieve_clinical_data": 0.8,
        "retrieve_summary_data": 0.8,
        "retrieve_general_text": 0.7,
        "query_knowledge_graph": 0.5,
    },
    # For general questions...
    "general_qa": {
        "retrieve_general_text": 0.9,
        "retrieve_summary_data": 0.7,
        "query_knowledge_graph": 0.6,
        "retrieve_clinical_data": 0.5,
    },
}
DEFAULT_INTENT_SCORE = 0.5
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

class ToolPlanner:
    def __init__(self, coverage_threshold: float = 0.9):
        self.coverage_threshold = coverage_threshold
        self._load_persona_map()

    def _load_persona_map(self):
        """Loads the persona-to-tool mapping from the central YAML config."""
        map_file = PROJECT_ROOT / "config" / "persona_tool_map.yml"
        try:
            with open(map_file, 'r') as f:
                self.persona_map = yaml.safe_load(f)
            logger.info(f"Successfully loaded persona-tool map from '{map_file}'.")
        except Exception as e:
            logger.error(f"FATAL: Could not load or parse persona-tool map from '{map_file}': {e}", exc_info=True)
            self.persona_map = {}

    def plan(self, query_meta: QueryMetadata, persona: str) -> List[ToolPlanItem]:
        """
        Creates a ranked tool plan by combining query intent with user persona preferences.
        """
        logger.info(f"Planning tools for intent '{query_meta.intent}' and persona '{persona}'")
        
        # 1. Get the list of preferred tools and their weights for the given persona
        persona_key = persona.lower().replace(" ", "_")
        persona_prefs = self.persona_map.get(persona_key, self.persona_map.get("default", []))

        if not persona_prefs:
            logger.warning(f"No tool preferences found for persona '{persona_key}' or default. Returning empty plan.")
            return []
            
        persona_tool_weights: Dict[str, float] = {p["tool_name"]: p["weight"] for p in persona_prefs}
        
        # 2. Get intent-based scores for tools relevant to the current query intent
        intent_scores = INTENT_TOOL_SCORES.get(query_meta.intent, {})

        # 3. Calculate a final score for each tool by multiplying persona weight and intent score
        scored_tools = []
        for tool_name, persona_weight in persona_tool_weights.items():
            intent_score = intent_scores.get(tool_name, DEFAULT_INTENT_SCORE)
            
            # The final score reflects both the persona's general preference and the tool's suitability for the task
            final_score = persona_weight * intent_score
            scored_tools.append({"name": tool_name, "score": final_score})

        # 4. Sort tools by their final score in descending order
        scored_tools.sort(key=lambda x: x["score"], reverse=True)

        # 5. Build the final plan, adding tools until the cumulative coverage threshold is met
        final_plan: List[ToolPlanItem] = []
        total_coverage = 0.0
        for tool in scored_tools:
            # We treat the score as the estimated coverage for this planning step
            estimated_coverage = round(tool["score"], 2)

            # Do not add tools with negligible contribution
            if estimated_coverage <= 0.1:
                continue

            plan_item = ToolPlanItem(tool_name=tool["name"], estimated_coverage=estimated_coverage)
            final_plan.append(plan_item)
            
            total_coverage += estimated_coverage
            if total_coverage >= self.coverage_threshold:
                logger.info(f"Coverage threshold of {self.coverage_threshold} met. Finalizing plan.")
                break
        
        logger.info(f"Generated tool plan: {[t.model_dump_json(indent=2) for t in final_plan]}")
        return final_plan


########################################################################
### FILE: src/prompts.py
########################################################################

# FILE: src/prompts.py
# V3.3 (Production Grade): Centralized the Persona Classification Prompt,
# fixing a critical ImportError and making this module the single source of
# truth for all agent-related prompts.

"""
Production-grade prompts for a robust RAG agent. This version supports both
direct RAG and a multi-step ReAct (Reason+Act) style reasoning loop.
"""

# ==============================================================================
# PROMPT 1: QUERY CLASSIFICATION
# ==============================================================================
QUERY_CLASSIFICATION_PROMPT_V2 = """
You are an expert query analysis agent. Your task is to analyze the user's question and provide a structured JSON output with four fields.

**Fields to Generate:**

1.  `intent`: Classify the user's goal. Choose exactly one from: ["specific_fact_lookup", "simple_summary", "comparative_analysis", "general_qa", "unknown"].
2.  `keywords`: Extract key nouns and proper nouns like drug names, company names, etc.
3.  `themes`: Extract high-level conceptual themes from the question. Choose from this list: ["Oncology", "Regulatory History", "Efficacy Results", "Safety Results", "Clinical Trial Design", "Pharmacoeconomic Analysis", "Dosage and Administration", "Drug/Therapy Description", "Indication/Population Description"]. Return an empty list if no theme applies.
4.  `question_is_graph_suitable`: Return `true` if the question asks for a direct relationship between two specific entities (e.g., "Who sponsors DrugX?", "What does DrugY treat?"). Return `false` for summaries, comparisons, or general questions.

**CRITICAL INSTRUCTIONS:**
- Output ONLY the raw JSON object. Do not add explanations or markdown code fences.
- If the question is "What is Amivantamab used to treat?", the relationship is (Amivantamab -> used to treat -> ?), so `question_is_graph_suitable` MUST be `true`.
- If the question is "Summarize the May meeting", there is no direct relationship, so `question_is_graph_suitable` MUST be `false`.
"""

# --- START OF DEFINITIVE FIX: Moved prompt to its correct central location ---
# ==============================================================================
# PROMPT 1.5: PERSONA CLASSIFICATION
# ==============================================================================
PERSONA_CLASSIFICATION_PROMPT = """
You are an expert request router. Your task is to analyze the user's question and determine which specialist persona is best equipped to answer it. You must choose from the available personas and provide ONLY the persona's key name as your response.

**Available Personas & Their Expertise:**

1.  **`clinical_analyst`**:
    *   Focuses on: Clinical trial data, drug efficacy, safety profiles, patient outcomes, medical conditions, and mechanisms of action.
    *   Keywords: treat, condition, indication, dosage, patients, trial, effective, side effects.
    *   Choose this persona for questions about the medical and scientific aspects of a drug.

2.  **`health_economist`**:
    *   Focuses on: Cost-effectiveness, pricing, market access, economic evaluations, and healthcare policy implications.
    *   Keywords: cost, price, economic, budget, financial, value, policy, summary.
    *   Choose this persona for questions about the financial or policy-level impact of a drug.

3.  **`regulatory_specialist`**:
    *   Focuses on: Submission types, meeting agendas, regulatory pathways (e.g., PBS listing types), sponsors, and official guidelines.
    *   Keywords: sponsor, submission, listing, agenda, meeting, guideline, change, status.
    *   Choose this persona for questions about the process and logistics of drug approval and listing.

**User Question:**
"{question}"

**Instructions:**
- Read the user's question carefully.
- Compare it against the expertise of each persona.
- Return ONLY the single key name (e.g., `clinical_analyst`) of the best-fitting persona. Do not add any explanation or other text.
"""
# --- END OF DEFINITIVE FIX ---

# ==============================================================================
# PROMPT 2: CYPHER QUERY GENERATION
# ==============================================================================
CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema and examples.

**Live Graph Schema:**
{schema}

**CRITICAL Instructions:**
1.  **Analyze the question deeply.** Identify all entities and the relationships between them.
2.  **Construct a valid Cypher query** to find the answer. The query must be read-only.
3.  **Always query against the `name_normalized` property for nodes** (e.g., `WHERE drug.name_normalized = 'abaloparatide'`).
4.  **To filter by properties on a relationship, you MUST name the relationship in the MATCH clause** (e.g., `MATCH (d)-[r:HASSPONSOR]->(s)`) and then use it in the WHERE clause (e.g., `WHERE r.doc_id CONTAINS 'March-2025'`).
5.  **Return Path and Properties:** Your `RETURN` clause must always be `RETURN p, properties(r) as rel_props`. The `r` must be the primary relationship in the path `p`.
6.  **Handle Failure:** If the question cannot be answered with a Cypher query from the schema, you MUST return the single word: `NONE`.
7.  **Output ONLY the Cypher query or the word `NONE`.**

---
**Example Gallery:**

**# Example 1: Simple Fact Retrieval**
Question: "What company sponsors Abaloparatide?"
Cypher: MATCH p=(drug:Entity)-[r:HASSPONSOR]->(sponsor:Entity) WHERE drug.name_normalized = 'abaloparatide' RETURN p, properties(r) as rel_props

**# Example 2: Multi-Hop / "Bridge" Query**
Question: "What is the indication for the drug whose trade name is Cabometyx?"
Cypher: MATCH p=(trade_name:Entity)<-[r:HASTRADENAME]-(drug:Entity)-[:HASINDICATION]->(indication:Entity) WHERE trade_name.name_normalized = 'cabometyx' RETURN p, properties(r) as rel_props

**# Example 3: Filtering by Relationship Property**
Question: "List all sponsors who made submissions in the March 2025 PBAC meeting."
Cypher: MATCH p=(drug:Entity)-[r:HASSPONSOR]->(sponsor:Entity) WHERE r.doc_id CONTAINS 'March-2025' RETURN p, properties(r) as rel_props
---

**Current Task:**
Question: {question}
"""

# ==============================================================================
# PROMPT 3: QUERY DECOMPOSITION
# ==============================================================================
DECOMPOSITION_PROMPT = """
You are a master query planner. Your goal is to determine if a user's question can be answered in a single step or if it requires decomposition into multiple, simpler sub-questions for data retrieval.

Analyze the user's question.

**Decision Criteria:**
- **Single Step:** If the question asks for a direct fact about a single subject.
- **Decomposition:** If the question requires comparing or combining information from two or more distinct subjects (e.g., two drugs, two meetings).

**Output Schema:**
You MUST output a single, valid JSON object with two keys:
1.  `requires_decomposition`: A boolean (`true` or `false`).
2.  `plan`: A list of strings.
    - If `requires_decomposition` is `false`, the plan should be a list with the original question as the single item.
    - If `requires_decomposition` is `true`, the plan should be a list of simple, factual retrieval questions, one for each subject. **Do not include a final step to combine the results; the final synthesizer will do that.**

**Example 1 (Single Step):**
- User Question: "What is the use of Esketamine?"
- Your JSON Output:
{{
  "requires_decomposition": false,
  "plan": ["What is the use of Esketamine?"]
}}

**Example 2 (Decomposition for Comparison/Intersection):**
- User Question: "Compare the submission purposes for Acalabrutinib and Alectinib."
- Your JSON Output:
{{
  "requires_decomposition": true,
  "plan": [
    "What was the submission purpose for Acalabrutinib?",
    "What was the submission purpose for Alectinib?"
  ]
}}

**TASK:**
- Chat History: {chat_history}
- User Question: {question}

Now, generate the JSON output.
"""

# ==============================================================================
# PROMPT 4: REASONING SYNTHESIS (REFINED)
# ==============================================================================
REASONING_SYNTHESIS_PROMPT = """
You are a highly intelligent synthesis agent. Your task is to provide a final, comprehensive answer to the user's original question by reasoning over the observations you have collected.

**User's Original Question:** "{question}"

**Your Observations (Scratchpad):**
---
{scratchpad}
---

**CRITICAL INSTRUCTIONS:**
1.  **Analyze the User's Original Question** to understand the final logical operation required (e.g., comparison, intersection, summarization).
2.  **Review all Observations.** These are the facts you have gathered.
3.  **Perform the Required Logic.** If the original question was a comparison, compare the facts. If it asked for items in common, find the intersection of the lists in your observations.
4.  **Handle Partial or Missing Information Gracefully.** This is your most important task. If you have an answer for one part of the question but not another, you **MUST** state that clearly. For example: "The submission purpose for Alectinib was a change to the existing listing [cite]. However, I could not find any information regarding the submission purpose for Acalabrutinib." Do not give a generic failure message.
5.  **Synthesize the Final Answer.** Do not show your step-by-step reasoning. Just provide the final, clean, and comprehensive answer.
6.  Include citations from your observations where appropriate.

**Final Answer:**
"""

# ==============================================================================
# PROMPT 5: DIRECT SYNTHESIS (FOR FACTUAL ANSWERS)
# ==============================================================================
DIRECT_SYNTHESIS_PROMPT = """
You are a precise, professional document analysis bot. Your ONLY job is to answer the user's question based strictly on the provided evidence.

**TASK:**
1.  Read the User's Question.
2.  Read the numbered evidence blocks (`EVIDENCE [1]`, `EVIDENCE [2]`, etc.).
3.  Synthesize a direct, professional answer to the question.
4.  When you use information from a piece of evidence, you **MUST** cite it by placing its corresponding number in brackets, like `[1]`.
5.  Cite each piece of evidence you use. If multiple pieces of evidence support a single point, you can cite them together, like `[1][2]`.
6.  Your answer must be based **ONLY** on the provided evidence. Do not add outside knowledge.
7.  If the evidence is insufficient to answer the question, you **MUST** state that the provided evidence does not contain the answer.

**EXAMPLE:**
User's Question: "What is the sponsor and dosage form for Abaloparatide?"
Evidence:
EVIDENCE [1]:
Evidence from graph: ABALOPARATIDE has sponsor THERAMEX AUSTRALIA PTY LTD.

EVIDENCE [2]:
Evidence from document: The submission for Abaloparatide was for a 3mg dosage form.

Your Answer:
The sponsor for Abaloparatide is Theramex Australia Pty Ltd [1]. The dosage form submitted was 3 mg [2].

---
**User's Question:** "{question}"
---
**Evidence:**
{context_str}
---
**Your Answer:**
"""

# ==============================================================================
# PROMPT 6: SUMMARIZATION
# ==============================================================================
SUMMARIZATION_PROMPT = """
You are a professional medical and regulatory writer. Your task is to synthesize a concise, well-written summary based on the provided blocks of evidence.

**CRITICAL INSTRUCTIONS:**
1.  Read all the provided evidence blocks.
2.  Identify the key themes, topics, and conclusions.
3.  Write a professional summary in clear, paragraph form.
4.  Do not invent information. Your summary must be based **ONLY** on the evidence.
5.  Do not cite the evidence blocks with numbers like `[1]`. Write a narrative summary.
6.  If the evidence is contradictory or insufficient, state that clearly in your summary.

---
**Evidence to Summarize:**
{context_str}
---
**Your Summary:**
"""

# ==============================================================================
# PROMPT 7: RE-RANKING
# ==============================================================================
RERANKING_PROMPT = """
You are a highly intelligent and precise relevance-ranking model. Your task is to analyze a user's question and a list of retrieved documents, and then return a JSON list of the document indices that are most relevant for answering the question.

**CRITICAL INSTRUCTIONS:**
1.  Read the user's question to understand their core intent.
2.  Read each document, identified by its index (e.g., `DOCUMENT[0]`, `DOCUMENT[1]`).
3.  Determine which documents contain direct, explicit information that helps answer the question.
4.  Your output **MUST** be a single, valid JSON array of integers, representing the indices of the most relevant documents, sorted from most relevant to least relevant.
5.  Include **ONLY** the indices of documents that are directly relevant. If a document is only tangentially related, do not include its index.
6.  If **NO** documents are relevant, return an empty JSON array `[]`.
7.  Do not include more than the top 5 most relevant document indices.

**EXAMPLE:**
- **User Question:** "What is the dosage form for Apomorphine?"
- **Documents:**
  DOCUMENT[0]:
  Evidence from document: The sponsor for Apomorphine is STADA...
  Citation: <a...>
  
  DOCUMENT[1]:
  Evidence from document: Movapo® (apomorphine hydrochloride hemihydrate) is available as a solution for subcutaneous infusion...
  Citation: <a...>
  
  DOCUMENT[2]:
  Evidence from document: The PBAC recommended the listing of Abaloparatide...
  Citation: <a...>

- **Your JSON Output:**
  [1]

---
**TASK:**

**User Question:** "{question}"

**Documents:**
{documents}

**Your JSON Output:**
"""


########################################################################
### FILE: src/router/__init__.py
########################################################################




########################################################################
### FILE: src/router/tool_router.py
########################################################################

# FILE: src/router/tool_router.py
# V5.0 (Unified Tooling): Refactored to use only the two primary tools.

import logging
from typing import Callable, Dict

from src.models import QueryMetadata, ToolResult
from src.tools import retrievers

logger = logging.getLogger(__name__)

class ToolRouter:
    def __init__(self):
        # --- DEFINITIVE FIX: Register only the tools that now exist ---
        self.registry: Dict[str, Callable[[str, QueryMetadata], ToolResult]] = {
            "vector_search": retrievers.vector_search,
            "query_knowledge_graph": retrievers.query_knowledge_graph,
        }
        logger.info(f"ToolRouter initialized with {len(self.registry)} tools.")

    def execute_tool(self, tool_name: str, query: str, query_meta: QueryMetadata) -> ToolResult:
        logger.info(f"[ToolRouter] Executing tool: '{tool_name}'")
        tool_function = self.registry.get(tool_name)
        if not tool_function:
            logger.warning(f"Tool '{tool_name}' not found in registry.")
            return ToolResult(tool_name=tool_name, success=False, content="[Error: Tool not implemented]")
        try:
            return tool_function(query, query_meta)
        except Exception as e:
            logger.error(f"[ToolRouter] Tool '{tool_name}' failed: {e}", exc_info=True)
            return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred: {e}")


########################################################################
### FILE: src/tools/__init__.py
########################################################################




########################################################################
### FILE: src/tools/clients.py
########################################################################

# FILE: src/tools/clients.py
# V3.0 (Final Version): Removed all unnecessary Cohere client code.

import os
import logging
from functools import lru_cache

import pinecone
import neo4j
import google.generativeai as genai
from google.generativeai.client import get_default_generative_client
from google.api_core.retry import Retry
from google.api_core.client_options import ClientOptions
from google.api_core import exceptions as google_exceptions
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# --- Resilience Configuration ---
def is_service_unavailable(exc: Exception) -> bool:
    """Predicate function to check if an exception is a ServiceUnavailable error."""
    return isinstance(exc, google_exceptions.ServiceUnavailable)

DEFAULT_RETRY = Retry(
    predicate=is_service_unavailable,
    initial=1.0,      # Start with a 1-second delay
    maximum=10.0,     # Maximum delay of 10 seconds
    multiplier=2.0,   # Double the delay each time
    deadline=30.0,    # Total deadline for all retries
)

DEFAULT_REQUEST_OPTIONS = {"retry": DEFAULT_RETRY, "timeout": 15.0}


# --- Client Initializers (Cached for Performance) ---

@lru_cache(maxsize=1)
def get_google_ai_client() -> genai:
    """Initializes and returns the Google AI client."""
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")
        genai.configure(api_key=api_key)
        # This line helps prevent certain region-based connection issues.
        get_default_generative_client()._client_options = ClientOptions(api_endpoint="generativelanguage.googleapis.com")
        logger.info("Google AI client configured successfully.")
        return genai
    except Exception as e:
        logger.error(f"Failed to configure Google AI client: {e}")
        return None

@lru_cache(maxsize=2)
def get_generative_model(model_name: str = 'gemini-1.5-pro-latest') -> genai.GenerativeModel:
    client = get_google_ai_client()
    if not client: return None
    logger.info(f"Requesting GenerativeModel: {model_name}")
    return client.GenerativeModel(model_name)

@lru_cache(maxsize=2)
def get_flash_model(model_name: str = 'gemini-1.5-flash-latest') -> genai.GenerativeModel:
    client = get_google_ai_client()
    if not client: return None
    logger.info(f"Requesting Flash Model: {model_name}")
    return client.GenerativeModel(model_name)

@lru_cache(maxsize=1)
def get_pinecone_index() -> pinecone.Index:
    """Initializes and returns the Pinecone index client."""
    try:
        api_key = os.getenv("PINECONE_API_KEY")
        index_name = os.getenv("PINECONE_INDEX_NAME")
        if not api_key or not index_name:
            raise ValueError("PINECONE_API_KEY or PINECONE_INDEX_NAME not set.")
        
        pc = pinecone.Pinecone(api_key=api_key)
        index = pc.Index(index_name)
        logger.info(f"Pinecone index '{index_name}' connected successfully.")
        return index
    except Exception as e:
        logger.error(f"Failed to connect to Pinecone index: {e}")
        return None

@lru_cache(maxsize=1)
def get_neo4j_driver() -> neo4j.Driver:
    """Initializes and returns the Neo4j graph database driver."""
    try:
        uri = os.getenv("NEO4J_URI")
        user = os.getenv("NEO4J_USERNAME", "neo4j")
        password = os.getenv("NEO4J_PASSWORD")
        if not all([uri, user, password]):
            raise ValueError("Neo4j connection details (URI, USERNAME, PASSWORD) not set.")

        driver = neo4j.GraphDatabase.driver(uri, auth=(user, password))
        driver.verify_connectivity()
        logger.info("Neo4j driver connected successfully.")
        return driver
    except Exception as e:
        logger.error(f"Failed to create Neo4j driver: {e}")
        return None


########################################################################
### FILE: src/tools/retrievers.py
########################################################################

# FILE: src/tools/retrievers.py
# V7.2 (Final Production Grade): Corrected the delimiter used for joining multiple
# knowledge graph results. It now uses "\n---\n" to match the vector search tool,
# ensuring each fact-citation pair is treated as a single, atomic unit by the agent.
# This resolves the bug of missing references for KG-derived answers.

import logging
import time
from typing import List, Dict, Any, Union
import neo4j
import google.generativeai as genai

from src.tools.clients import get_flash_model, get_pinecone_index, get_neo4j_driver, DEFAULT_REQUEST_OPTIONS
from src.models import ToolResult, QueryMetadata
from src.prompts import CYPHER_GENERATION_PROMPT

logger = logging.getLogger(__name__)


class Timer:
    def __init__(self, name): self.name = name
    def __enter__(self): self.start = time.perf_counter(); return self
    def __exit__(self, *args): self.end = time.perf_counter(); logger.info(f"[TIMER] {self.name} took {(self.end - self.start) * 1000:.2f} ms")


def _format_citation(doc_id: str, page_numbers: Union[List[int], str, None], source_url: str) -> str:
    """Creates a standardized, clickable HTML citation link."""
    page_str = "N/A"
    link_url = source_url if source_url else "#"
    
    if page_numbers:
        # Handle list of ints from Pinecone
        if isinstance(page_numbers, list) and all(isinstance(p, int) for p in page_numbers):
            unique_pages = sorted(list(set(page_numbers)))
            if unique_pages:
                page_str = f"Page {unique_pages[0]}" if len(unique_pages) == 1 else f"Pages {', '.join(map(str, unique_pages))}"
                if source_url:
                    link_url = f"{source_url}#page={unique_pages[0]}"
        # Handle string from Neo4j
        elif isinstance(page_numbers, str):
            page_str = f"Page {page_numbers}"
            first_page = page_numbers.split(',')[0].split('-')[0].strip()
            if first_page.isdigit() and source_url:
                 link_url = f"{source_url}#page={first_page}"

    return f'<a href="{link_url}" target="_blank">{doc_id} ({page_str})</a>'

def _format_pinecone_results(matches: List[dict]) -> List[str]:
    contents = []
    for match in matches:
        metadata = match.get('metadata', {})
        text = metadata.get('text', 'No content available.')
        doc_id = metadata.get('doc_id', 'Unknown Document')
        page_numbers = metadata.get('page_numbers')
        url = metadata.get('source_pdf_url')
        
        citation = _format_citation(doc_id, page_numbers, url)
        contents.append(f"Evidence from document: {text}\nCitation: {citation}")
    return contents

def _serialize_neo4j_path(record: Dict[str, Any]) -> str:
    path_data, rel_props = record.get("p"), record.get("rel_props")
    if not path_data: return ""
    
    try:
        if isinstance(path_data, neo4j.graph.Path):
            subject_name = path_data.start_node.get('name')
            predicate_type = path_data.relationships[0].type
            object_name = path_data.end_node.get('name')
        elif isinstance(path_data, list) and len(path_data) == 3:
            subject_name, predicate_type, object_name = path_data[0].get('name'), path_data[1], path_data[2].get('name')
        else:
            return ""

        if not all([subject_name, predicate_type, object_name]): return ""
        
        predicate_str = predicate_type.replace('_', ' ').lower()
        text_representation = f"{subject_name} {predicate_str} {object_name}."
        
        doc_id = rel_props.get('doc_id', 'Knowledge Graph')
        page_numbers = rel_props.get('page_numbers')
        url = rel_props.get('source_pdf_url')
        
        citation = _format_citation(doc_id, page_numbers, url)
        return f"Evidence from graph: {text_representation}\nCitation: {citation}"
    except Exception as e:
        logger.warning(f"Could not serialize Neo4j path: {e}")
        return ""

def vector_search(query: str, query_meta: QueryMetadata) -> ToolResult:
    tool_name = "vector_search"; namespace = "pbac-text"
    with Timer(f"Tool: {tool_name}"):
        pinecone_index = get_pinecone_index()
        if not pinecone_index: return ToolResult(tool_name=tool_name, success=False, content="Pinecone not available.")
        metadata_filter = {}
        if query_meta and query_meta.themes:
            metadata_filter["semantic_purpose"] = {"$in": query_meta.themes}
            logger.info(f"Applying metadata filter: {metadata_filter}")
        try:
            query_embedding = genai.embed_content(model='models/text-embedding-004', content=query, task_type="retrieval_query")
            response = pinecone_index.query(namespace=namespace, vector=query_embedding['embedding'], top_k=10, include_metadata=True, filter=metadata_filter or None)
            if not response.get('matches'): return ToolResult(tool_name=tool_name, success=True, content="")
            content_list = _format_pinecone_results(response['matches'])
            return ToolResult(tool_name=tool_name, success=True, content="\n---\n".join(content_list))
        except Exception as e:
            logger.error(f"Error in vector search: {e}", exc_info=True)
            return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred: {e}")

def query_knowledge_graph(query: str, query_meta: QueryMetadata) -> ToolResult:
    tool_name = "query_knowledge_graph"
    with Timer(f"Tool: {tool_name}"):
        llm, driver = get_flash_model(), get_neo4j_driver()
        if not llm or not driver: return ToolResult(tool_name=tool_name, success=False, content="Clients not available.")
        try:
            with driver.session() as session:
                nodes_schema_result = session.run("CALL db.schema.nodeTypeProperties() YIELD nodeLabels, propertyName, propertyTypes RETURN nodeLabels, propertyName, propertyTypes").data()
                rels_schema_result = session.run("CALL db.schema.relTypeProperties() YIELD relType, propertyName, propertyTypes RETURN relType, propertyName, propertyTypes").data()

            schema_str = "Node Properties:\n"
            processed_nodes: Dict[str, List[str]] = {}
            for node in nodes_schema_result:
                label = node['nodeLabels'][0]
                if label not in processed_nodes: processed_nodes[label] = []
                prop_name = node.get('propertyName')
                prop_type = node.get('propertyTypes', ['String'])[0]
                if prop_name:
                    processed_nodes[label].append(f"{prop_name}: {prop_type}")
            for label, props in processed_nodes.items():
                schema_str += f"- Label: {label}, Properties: {', '.join(props)}\n"
            
            schema_str += "\nRelationship Properties:\n"
            processed_rels: Dict[str, List[str]] = {}
            for rel in rels_schema_result:
                rel_type = rel.get('relType', '').strip('`')
                if not rel_type: continue
                if rel_type not in processed_rels: processed_rels[rel_type] = []
                prop_name = rel.get('propertyName')
                prop_type = rel.get('propertyTypes', ['String'])[0]
                if prop_name:
                    processed_rels[rel_type].append(f"{prop_name}: {prop_type}")
            for rel_type, props in processed_rels.items():
                props_str = ", ".join(props)
                schema_str += f"- (:Entity)-[:{rel_type} {{{props_str}}}]->(:Entity)\n"

            prompt = CYPHER_GENERATION_PROMPT.format(schema=schema_str, question=query)
            response = llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            cypher_query = response.text.strip().replace("```cypher", "").replace("```", "").replace("`", "")
            
            if "none" in cypher_query.lower() or "match" not in cypher_query.lower(): 
                return ToolResult(tool_name=tool_name, success=True, content="")
                
            logger.info(f"Generated Cypher: {cypher_query}")
            with driver.session() as session: records = session.run(cypher_query).data()
            if not records: return ToolResult(tool_name=tool_name, success=True, content="")
            
            results = [_serialize_neo4j_path(record) for record in records if record.get("p")]
            
            # --- START OF DEFINITIVE FIX: Use the correct delimiter ---
            # This ensures each evidence/citation pair from the KG is treated as one atomic document by the agent.
            return ToolResult(tool_name=tool_name, success=True, content="\n---\n".join(filter(None, results)))
            # --- END OF DEFINITIVE FIX ---
            
        except Exception as e:
            logger.error(f"Error in KG tool: {e}", exc_info=True)
            return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred: {e}")


########################################################################
### FILE: streamlit_app.py
########################################################################

# FILE: streamlit_app.py
# V3.1 (Definitive Fix): Corrected the agent call to be synchronous, fixing the asyncio ValueError.

import streamlit as st
import logging
import os
from dotenv import load_dotenv

# --- CRITICAL: Load environment variables at the very top ---
load_dotenv()

# --- Page and Logging Configuration ---
st.set_page_config(
    page_title="Persona RAG Chatbot",
    page_icon="🧠",
    layout="wide",
    initial_sidebar_state="expanded",
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - [%(name)s:%(lineno)d] - %(message)s'
)
logging.getLogger('streamlit').setLevel(logging.WARNING)
logging.getLogger('watchdog').setLevel(logging.WARNING)
logging.getLogger('PIL').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)

# Now import project modules
from src.agent import Agent
from src.tools.clients import get_google_ai_client # Used for a pre-flight check


# --- Session State Initialization ---
if "agent" not in st.session_state:
    st.session_state.agent = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_persona" not in st.session_state:
    st.session_state.current_persona = "automatic"

# --- Helper Functions ---
@st.cache_resource
def initialize_agent():
    if not get_google_ai_client():
        st.error("Google API Key is not configured. Please set the GOOGLE_API_KEY in your .env file.", icon="🚨")
        return None
    try:
        agent = Agent()
        logger.info("Unified agent initialized successfully and cached for the session.")
        return agent
    except Exception as e:
        st.error(f"Fatal error during agent initialization: {e}", icon="🚨")
        logger.error(f"Agent initialization failed: {e}", exc_info=True)
        return None

def reset_chat(persona_name: str):
    display_name = "Automatic" if persona_name == "Automatic (Recommended)" else persona_name
    st.session_state.messages = [
        {"role": "assistant", "content": f"Hi! I'm now acting in **{display_name}** mode. How can I help you?"}
    ]

# --- Sidebar ---
with st.sidebar:
    st.header("🤖 Persona RAG Chatbot")
    st.markdown("Select a persona to tailor my retrieval strategy and answers to your specific role.")
    persona_options = {
        'Automatic (Recommended)': 'automatic',
        'Clinical Analyst': 'clinical_analyst',
        'Health Economist': 'health_economist',
        'Regulatory Specialist': 'regulatory_specialist',
    }
    current_display_name = [k for k, v in persona_options.items() if v == st.session_state.current_persona][0]
    selected_persona_name = st.radio(
        "**Choose your Mode:**",
        options=persona_options.keys(),
        index=list(persona_options.keys()).index(current_display_name),
        key="persona_selector"
    )
    selected_persona_key = persona_options[selected_persona_name]
    if selected_persona_key != st.session_state.current_persona:
        st.session_state.current_persona = selected_persona_key
        reset_chat(selected_persona_name)
        st.rerun()
    st.divider()
    if st.button("🔄 Clear Chat History", use_container_width=True):
        reset_chat(selected_persona_name)
        st.rerun()
    st.divider()
    st.header("🧪 Evaluation Questions")
    st.markdown("Use these questions to test the agent's capabilities with the 2025 data.")

    with st.expander("🎯 Fact Retrieval (2025 Data)", expanded=True):
        questions = {
            "Sponsor Lookup (July 2025)": "What company is the sponsor for Abaloparatide?",
            "Indication Lookup (March 2025)": "What is Amivantamab used to treat?",
            "Trade Name Lookup (May 2025)": "What is the trade name for Dostarlimab?",
            "Dosage Form (July 2025)": "What is the dosage form of Apomorphine?",
        }
        for name, q in questions.items():
            if st.button(f"{name}: {q}", key=q, use_container_width=True):
                st.session_state.run_prompt = q

    with st.expander("⚖️ Multi-Step Reasoning (2025 Data)"):
        questions = {
            "Intersection Query": "Which sponsors made submissions in both the March 2025 and July 2025 meetings?",
            "Comparative Query": "Compare the submission purposes for Acalabrutinib and Alectinib in the 2025 meetings.",
            "Multi-Hop Query": "What is the indication for the drug whose trade name is Cabometyx?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q
    
    with st.expander("📋 Summarization (2025 Data)"):
        questions = {
            "Summarize a Meeting": "Provide a summary of the key submissions from the May 2025 PBAC meeting.",
            "Summarize by Theme": "Summarize all submissions related to oncology in the March 2025 documents.",
            "Synthesize High-Level Goal": "Based on the agendas, what appears to be the main focus of the PBAC's work in 2025?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q

    with st.expander("🤔 Challenging / Ambiguous Questions"):
        questions = {
            "Test Data Boundaries": "What was the final PBAC decision on Ribociclib from the July 2025 meeting?",
            "Test Fallback Logic (No Price)": "What is the price of Tirzepatide?",
            "Out of Scope (External Knowledge)": "What are the latest FDA guidelines on biosimilars?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q

# --- Main Chat Interface ---
st.title("Persona-Aware RAG Agent")
if st.session_state.current_persona == "automatic":
    st.caption("Currently in **Automatic Mode** (selects best persona per query)")
else:
    st.caption(f"Currently acting as: **{selected_persona_name}**")
with st.container(border=True):
    st.info("""
    **Welcome! This is an advanced chatbot designed to answer questions about pharmaceutical and regulatory documents.** 
    
    Its unique feature is the ability to tailor its information retrieval strategy based on the professional role you select in the sidebar.

    **How to use this demo:**
    1.  **Choose Your Mode:** Select 'Automatic' (Recommended) or a specific persona from the sidebar.
    2.  **Ask a Question:** Use the pre-defined 'Evaluation Questions' or type your own question in the chat box below.
    """)
    st.markdown("<p style='text-align: center; color: grey;'>A not-for-profit demonstration project by <b>EVIL_MIT</b></p>", unsafe_allow_html=True)

if st.session_state.agent is None:
    st.session_state.agent = initialize_agent()
    if not st.session_state.messages:
        reset_chat(selected_persona_name)

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"], unsafe_allow_html=True)

prompt_from_button = st.session_state.pop("run_prompt", None)
prompt_from_input = st.chat_input("Ask your question...")
prompt = prompt_from_button or prompt_from_input

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.agent:
            spinner_text = "Thinking..."
            if st.session_state.current_persona != "automatic":
                 spinner_text = f"Thinking as a {selected_persona_name}..."
            
            with st.spinner(spinner_text):
                history_for_rewrite = [f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:-1]]
                
                # --- START OF DEFINITIVE FIX ---
                # REMOVED asyncio.run() as agent.run is now a synchronous function.
                response = st.session_state.agent.run(
                    prompt, 
                    persona=st.session_state.current_persona,
                    chat_history=history_for_rewrite
                )
                # --- END OF DEFINITIVE FIX ---
                
                st.markdown(response, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            st.error("Agent is not available due to an initialization error. Please check the terminal logs.")
            st.stop()
    
    if prompt_from_button:
        st.rerun()


########################################################################
### FILE: verify_pinecone.py
########################################################################

# FILE: verify_pinecone.py
import os
import pinecone
from dotenv import load_dotenv

# --- Configuration ---
# Change this to the document ID you want to check for.
DOCUMENT_ID_TO_CHECK = "May-2025-PBAC-Meeting-v5"
NAMESPACE_TO_CHECK = "pbac-text" # This is our unified namespace

# --- Main Script ---
def verify_document_in_pinecone():
    """Connects to Pinecone and verifies if vectors for a specific doc_id exist."""
    print("--- Pinecone Data Verifier ---")
    
    # 1. Load Environment Variables
    load_dotenv()
    api_key = os.getenv("PINECONE_API_KEY")
    index_name = os.getenv("PINECONE_INDEX_NAME")

    if not api_key or not index_name:
        print("❌ ERROR: PINECONE_API_KEY or PINECONE_INDEX_NAME not set in your .env file.")
        return

    print(f"Connecting to index '{index_name}'...")
    try:
        pc = pinecone.Pinecone(api_key=api_key)
        index = pc.Index(index_name)
        stats = index.describe_index_stats()
        print(f"✅ Successfully connected. Index has {stats['total_vector_count']} total vectors.")
    except Exception as e:
        print(f"❌ ERROR: Could not connect to Pinecone. Details: {e}")
        return

    # 2. Perform the Metadata-Filtered Query
    print(f"\nSearching for vectors with doc_id: '{DOCUMENT_ID_TO_CHECK}' in namespace '{NAMESPACE_TO_CHECK}'...")
    
    try:
        # We don't need a real query vector; the filter is what matters.
        # We query for a zero vector of the correct dimension (768 for mpnet).
        query_vector = [0.0] * 768 
        
        response = index.query(
            namespace=NAMESPACE_TO_CHECK,
            vector=query_vector,
            top_k=10,
            include_metadata=True,
            filter={
                "doc_id": {"$eq": DOCUMENT_ID_TO_CHECK}
            }
        )

        # 3. Analyze and Report Results
        matches = response.get('matches', [])
        if not matches:
            print("\n" + "="*50)
            print(f"🔴 VERIFICATION FAILED: No vectors found for '{DOCUMENT_ID_TO_CHECK}'.")
            print("   This confirms the data was NOT uploaded correctly.")
            print("="*50 + "\n")
        else:
            print("\n" + "="*50)
            print(f"🟢 VERIFICATION SUCCESSFUL: Found {len(matches)} vectors for '{DOCUMENT_ID_TO_CHECK}'.")
            print("   The data is present in your Pinecone index.")
            print("\n--- Sample of first result's metadata ---")
            print(matches[0].metadata)
            print("="*50 + "\n")

    except Exception as e:
        print(f"❌ ERROR: An error occurred during the query. Details: {e}")

if __name__ == "__main__":
    verify_document_in_pinecone()