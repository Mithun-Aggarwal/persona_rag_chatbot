========================================================================
  Combined Python Files from Git Repository: /home/mit/persona_rag_chatbot
  Generated on: Mon 28 Jul 2025 17:45:43 AEST
========================================================================



########################################################################
### FILE: src/__init__.py
########################################################################




########################################################################
### FILE: src/agent.py
########################################################################

# FILE: src/agent.py
# V2.1: Unified Agent with Integrated Trace Logging
import logging
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List

from src.tools.clients import get_google_ai_client
from src.models import ToolResult, QueryMetadata, ToolPlanItem
from src.planner.query_classifier import QueryClassifier
from src.planner.tool_planner import ToolPlanner
from src.router.tool_router import ToolRouter
from src.fallback import should_trigger_fallback, render_fallback_message
from src.prompts import SYNTHESIS_PROMPT

logger = logging.getLogger(__name__)

### NEW: Logging Integration - Logic from middleware is now here ###
LOG_PATH = Path("trace_logs.jsonl")

class Timer:
    """A context manager for timing code blocks."""
    def __enter__(self):
        self.start = time.perf_counter()
        return self

    def __exit__(self, *args):
        self.end = time.perf_counter()
        self.duration = self.end - self.start

def log_trace(
    query: str,
    persona: str,
    query_meta: QueryMetadata,
    tool_plan: List[ToolPlanItem],
    tool_results: List[ToolResult],
    final_answer: str,
    total_latency_sec: float
):
    """Writes a structured JSON line capturing the full agent loop."""
    trace_record = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "query": query,
        "persona": persona,
        "intent": query_meta.intent if query_meta else "classification_failed",
        "graph_suitable": query_meta.question_is_graph_suitable if query_meta else "unknown",
        "tool_plan": [t.model_dump() for t in tool_plan],
        "tool_results": [r.model_dump() for r in tool_results],
        "final_answer_preview": final_answer[:200] + "..." if final_answer else "N/A",
        "total_latency_sec": round(total_latency_sec, 3)
    }

    try:
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(trace_record) + "\n")
        logger.info(f"Trace logged successfully to {LOG_PATH.resolve()}")
    except Exception as e:
        logger.error(f"Failed to write trace log: {e}", exc_info=True)
### End of Logging Integration ###


class Agent:
    def __init__(self, confidence_threshold: float = 0.85):
        self.classifier = QueryClassifier()
        self.planner = ToolPlanner(coverage_threshold=confidence_threshold)
        self.router = ToolRouter()
        
        genai_client = get_google_ai_client()
        if genai_client:
            self.llm = genai_client.GenerativeModel('gemini-1.5-pro-latest')
        else:
            self.llm = None
            logger.error("FATAL: Gemini client could not be initialized. Agent synthesis will fail.")

    def _format_context_for_synthesis(self, results: List[ToolResult]) -> str:
        context_str = ""
        for res in results:
            if res.success and res.content and "no relevant information" not in res.content.lower():
                context_str += f"--- Evidence from Tool: {res.tool_name} ---\n"
                context_str += f"{res.content.strip()}\n\n"
        return context_str.strip()

    def run(self, query: str, persona: str) -> str:
        """Executes the full agent loop with integrated timing and logging."""
        # Initialize variables for the log trace
        query_meta, tool_plan, results, final_answer = None, [], [], ""
        
        with Timer() as timer:
            try:
                logger.info(f"Agent starting run for persona '{persona}' with query: '{query}'")

                if not self.llm:
                    return "Error: The AI model for synthesizing answers is not available. Please check API key configuration."

                query_meta = self.classifier.classify(query)
                if not query_meta:
                    final_answer = "I'm sorry, I had trouble understanding your question. Could you please rephrase it?"
                    return final_answer

                tool_plan = self.planner.plan(query_meta, persona)
                if not tool_plan:
                    final_answer = "I'm sorry, I don't have a configured strategy to answer that question for your selected persona."
                    return final_answer

                logger.info(f"Executing tool plan: {[item.tool_name for item in tool_plan]}")
                for plan_item in tool_plan:
                    results.append(self.router.execute_tool(plan_item.tool_name, query, query_meta))

                if should_trigger_fallback(results):
                    final_answer = render_fallback_message(query)
                    return final_answer

                formatted_context = self._format_context_for_synthesis(results)
                if not formatted_context:
                    final_answer = "I was able to search for information but could not find any relevant details to answer your question."
                    return final_answer
                
                final_prompt = SYNTHESIS_PROMPT.format(question=query, context_str=formatted_context)
                final_answer = self.llm.generate_content(final_prompt).text
                logger.info("Agent run completed successfully.")
                return final_answer

            except Exception as e:
                logger.error(f"An unexpected error occurred during agent run: {e}", exc_info=True)
                final_answer = f"I encountered a critical error: {e}. Please check the system logs."
                return final_answer

            finally:
                # This block ensures that a trace is logged regardless of success or failure.
                log_trace(query, persona, query_meta, tool_plan, results, final_answer, timer.duration)


########################################################################
### FILE: src/common_utils.py
########################################################################

# src/common_utils.py

"""
Common utility functions shared across the application.
"""

from pathlib import Path

def get_project_root() -> Path:
    """
    Returns the absolute path to the project root directory.

    This is a robust way to reference files (like configs) from anywhere
    within the project, regardless of where the script is run from.
    """
    # We assume this file is in 'src/'. The project root is one level up.
    return Path(__file__).parent.parent.resolve()


########################################################################
### FILE: src/fallback.py
########################################################################

# FILE: src/fallback.py
# Phase 3.1: FallbackLayer — graceful UX when all tools return empty or fail

import logging
from typing import List
from src.models import ToolResult

logger = logging.getLogger(__name__)

FALLBACK_QUESTIONS = [
    "Would you like to compare two drugs instead?",
    "Can I help you find a sponsor for a specific medicine?",
    "Do you want to search the original PDF documents directly?"
]


def should_trigger_fallback(results: List[ToolResult]) -> bool:
    """
    Returns True if all tools failed or produced no meaningful content.
    """
    if not results:
        logger.info("Fallback triggered: no tool results returned.")
        return True

    empty_or_failed = [r for r in results if not r.success or not r.content or len(r.content.strip()) < 5]
    if len(empty_or_failed) == len(results):
        logger.info("Fallback triggered: all tools failed or content was empty.")
        return True

    return False


def render_fallback_message(query: str) -> str:
    """
    Returns a polite fallback message with suggested next questions.
    """
    msg = f"""
I'm sorry — based on the current documents and tools, I couldn't find sufficient information to answer your question:

"{query}"

However, here are some things you can try next:

"""
    for i, q in enumerate(FALLBACK_QUESTIONS, start=1):
        msg += f"{i}. {q}\n"

    msg += "\nYou can also try rephrasing your question for better results."
    return msg.strip()



########################################################################
### FILE: src/middleware/logging.py
########################################################################

# FILE: src/middleware/logging.py
# Phase 3.2: LoggingMiddleware — structured trace logs per query execution

import json
import time
import logging
from pathlib import Path
from typing import List
from datetime import datetime

from src.models import ToolResult, ToolPlanItem, QueryMetadata

logger = logging.getLogger(__name__)

LOG_PATH = Path("trace_logs.jsonl")  # Can be adjusted per environment


def log_trace(
    query: str,
    query_meta: QueryMetadata,
    tool_plan: List[ToolPlanItem],
    tool_results: List[ToolResult],
    total_latency_sec: float
):
    """
    Writes a structured JSON line capturing the full agent loop.
    """
    trace_record = {
        "timestamp": datetime.utcnow().isoformat(),
        "query": query,
        "intent": query_meta.intent,
        "graph_suitable": query_meta.question_is_graph_suitable,
        "keywords": query_meta.keywords,
        "tool_plan": [t.model_dump() for t in tool_plan],
        "tool_results": [r.model_dump() for r in tool_results],
        "total_latency_sec": round(total_latency_sec, 3)
    }

    try:
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(trace_record) + "\n")
        logger.info(f"Trace logged to {LOG_PATH.resolve()}")
    except Exception as e:
        logger.error(f"Failed to write trace log: {e}", exc_info=True)


# Optional context manager for timing
class Timer:
    def __enter__(self):
        self.start = time.time()
        return self

    def __exit__(self, *args):
        self.end = time.time()
        self.duration = self.end - self.start


########################################################################
### FILE: src/models.py
########################################################################

from pydantic import BaseModel
from typing import List, Optional, Literal


QueryIntent = Literal[
    "specific_fact_lookup",
    "simple_summary",
    "comparative_analysis",
    "general_qa",
    "unknown"
]


class QueryMetadata(BaseModel):
    intent: QueryIntent
    keywords: List[str]
    question_is_graph_suitable: bool


class ToolPlanItem(BaseModel):
    tool_name: str
    estimated_coverage: float


class ToolResult(BaseModel):
    tool_name: str
    success: bool
    content: str
    estimated_coverage: float = 0.0


class ContextItem(BaseModel):
    content: str
    source: Optional[dict] = None


class Source(BaseModel):
    type: str
    document_id: Optional[str] = None
    page_numbers: Optional[List[int]] = None
    source_url: Optional[str] = None
    retrieval_score: Optional[float] = None
    query: Optional[str] = None

class NamespaceConfig(BaseModel):
    namespace: str
    weight: float
    top_k: int

class RetrievalPlan(BaseModel):
    namespaces: List[NamespaceConfig] = []


########################################################################
### FILE: src/planner/__init__.py
########################################################################




########################################################################
### FILE: src/planner/confidence.py
########################################################################

# FILE: src/planner/confidence.py
# Phase 1.3: ToolConfidenceScorer — measures cumulative information coverage from tool results

import logging
from typing import List
from src.models import ToolResult

logger = logging.getLogger(__name__)

class ToolConfidenceScorer:
    """
    Recomputes cumulative coverage score after each tool reply based on:
    - Number of unique content blocks retrieved
    - Citation span coverage (placeholder)
    - Matching keywords (optional)
    """
    def __init__(self, min_threshold: float = 0.9):
        self.coverage_threshold = min_threshold

    def compute_total_coverage(self, results: List[ToolResult]) -> float:
        """Computes cumulative estimated coverage based on tool-level metadata."""
        total_coverage = sum([r.estimated_coverage for r in results if r.success])
        logger.info(f"Total cumulative coverage: {total_coverage:.2f}")
        return round(total_coverage, 3)

    def has_sufficient_coverage(self, results: List[ToolResult]) -> bool:
        coverage = self.compute_total_coverage(results)
        return coverage >= self.coverage_threshold

# Test block
if __name__ == "__main__":
    from src.models import ToolResult

    results = [
        ToolResult(tool_name="pinecone", estimated_coverage=0.6, success=True),
        ToolResult(tool_name="neo4j", estimated_coverage=0.35, success=True),
        ToolResult(tool_name="pdf", estimated_coverage=0.2, success=False)
    ]

    scorer = ToolConfidenceScorer()
    print("Coverage met:", scorer.has_sufficient_coverage(results))



########################################################################
### FILE: src/planner/query_classifier.py
########################################################################

# FILE: src/planner/query_classifier.py
# Phase 1.1: QueryClassifier — interprets user query using Gemini and returns structured metadata

import logging
import google.generativeai as genai
from typing import Optional
from src.models import QueryMetadata
from src.prompts import QUERY_CLASSIFICATION_PROMPT

logger = logging.getLogger(__name__)

class QueryClassifier:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-flash-latest')

    def classify(self, query: str) -> Optional[QueryMetadata]:
        """Classifies a user query into intent, keywords, and graph suitability."""
        logger.info(f"Classifying query: {query}")
        try:
            prompt = QUERY_CLASSIFICATION_PROMPT + f"\n\nUser Query: {query}"
            response = self.model.generate_content(prompt)
            parsed_json = response.text.strip()
            metadata = QueryMetadata.model_validate_json(parsed_json)
            logger.info(f"Classification result: {metadata.model_dump_json(indent=2)}")
            return metadata
        except Exception as e:
            logger.error(f"Query classification failed: {e}")
            return None

# Unit test: test with canned queries
if __name__ == "__main__":
    qc = QueryClassifier()
    test_queries = [
        "What company sponsors Abaloparatide?",
        "Compare the clinical outcomes of Drug A vs Drug B",
        "Tell me about submissions for lung cancer.",
        "What is the patient population for the March 2025 submission?"
    ]
    for q in test_queries:
        print(qc.classify(q))



########################################################################
### FILE: src/planner/tool_planner.py
########################################################################

# FILE: src/planner/tool_planner.py
# V2.0: Persona-Aware Tool Planner
import logging
import yaml
from pathlib import Path
from typing import List, Dict

from src.models import QueryMetadata, ToolPlanItem

logger = logging.getLogger(__name__)

# Base scores for tools based on query intent. A higher score is better.
# These tool names MUST match the names in `persona_tool_map.yml` and the tool router.
INTENT_TOOL_SCORES = {
    # If the user wants a specific fact...
    "specific_fact_lookup": {
        "query_knowledge_graph": 0.9,
        "retrieve_clinical_data": 0.7,
        "retrieve_general_text": 0.6,
        "retrieve_summary_data": 0.4,
    },
    # If the user wants a high-level summary...
    "simple_summary": {
        "retrieve_summary_data": 0.9,
        "retrieve_general_text": 0.8,
        "retrieve_clinical_data": 0.5,
        "query_knowledge_graph": 0.4,
    },
    # If the user wants to compare things...
    "comparative_analysis": {
        "retrieve_clinical_data": 0.8,
        "retrieve_summary_data": 0.8,
        "retrieve_general_text": 0.7,
        "query_knowledge_graph": 0.5,
    },
    # For general questions...
    "general_qa": {
        "retrieve_general_text": 0.9,
        "retrieve_summary_data": 0.7,
        "query_knowledge_graph": 0.6,
        "retrieve_clinical_data": 0.5,
    },
}
DEFAULT_INTENT_SCORE = 0.5
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

class ToolPlanner:
    def __init__(self, coverage_threshold: float = 0.9):
        self.coverage_threshold = coverage_threshold
        self._load_persona_map()

    def _load_persona_map(self):
        """Loads the persona-to-tool mapping from the central YAML config."""
        map_file = PROJECT_ROOT / "config" / "persona_tool_map.yml"
        try:
            with open(map_file, 'r') as f:
                self.persona_map = yaml.safe_load(f)
            logger.info(f"Successfully loaded persona-tool map from '{map_file}'.")
        except Exception as e:
            logger.error(f"FATAL: Could not load or parse persona-tool map from '{map_file}': {e}", exc_info=True)
            self.persona_map = {}

    def plan(self, query_meta: QueryMetadata, persona: str) -> List[ToolPlanItem]:
        """
        Creates a ranked tool plan by combining query intent with user persona preferences.
        """
        logger.info(f"Planning tools for intent '{query_meta.intent}' and persona '{persona}'")
        
        # 1. Get the list of preferred tools and their weights for the given persona
        persona_key = persona.lower().replace(" ", "_")
        persona_prefs = self.persona_map.get(persona_key, self.persona_map.get("default", []))

        if not persona_prefs:
            logger.warning(f"No tool preferences found for persona '{persona_key}' or default. Returning empty plan.")
            return []
            
        persona_tool_weights: Dict[str, float] = {p["tool_name"]: p["weight"] for p in persona_prefs}
        
        # 2. Get intent-based scores for tools relevant to the current query intent
        intent_scores = INTENT_TOOL_SCORES.get(query_meta.intent, {})

        # 3. Calculate a final score for each tool by multiplying persona weight and intent score
        scored_tools = []
        for tool_name, persona_weight in persona_tool_weights.items():
            intent_score = intent_scores.get(tool_name, DEFAULT_INTENT_SCORE)
            
            # The final score reflects both the persona's general preference and the tool's suitability for the task
            final_score = persona_weight * intent_score
            scored_tools.append({"name": tool_name, "score": final_score})

        # 4. Sort tools by their final score in descending order
        scored_tools.sort(key=lambda x: x["score"], reverse=True)

        # 5. Build the final plan, adding tools until the cumulative coverage threshold is met
        final_plan: List[ToolPlanItem] = []
        total_coverage = 0.0
        for tool in scored_tools:
            # We treat the score as the estimated coverage for this planning step
            estimated_coverage = round(tool["score"], 2)

            # Do not add tools with negligible contribution
            if estimated_coverage <= 0.1:
                continue

            plan_item = ToolPlanItem(tool_name=tool["name"], estimated_coverage=estimated_coverage)
            final_plan.append(plan_item)
            
            total_coverage += estimated_coverage
            if total_coverage >= self.coverage_threshold:
                logger.info(f"Coverage threshold of {self.coverage_threshold} met. Finalizing plan.")
                break
        
        logger.info(f"Generated tool plan: {[t.model_dump_json(indent=2) for t in final_plan]}")
        return final_plan


########################################################################
### FILE: src/prompts.py
########################################################################

# src/prompts.py

"""
Production-grade prompts for a robust RAG agent.
"""

# REFACTORED: The Cypher generation prompt is now more robust and correctly
# uses the {schema} placeholder to accept dynamic schemas.
CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema.

**Live Graph Schema:**
{schema}

**Instructions:**
1.  Analyze the schema to understand the available node labels, properties, and relationships.
2.  Construct a Cypher query that retrieves relevant information to answer the question.
3.  The query MUST be read-only (i.e., use `MATCH` and `RETURN`). Do not use `CREATE`, `MERGE`, `SET`, or `DELETE`.
4.  If possible, return a path `p` using `RETURN p` to show the full context of the connection.
5.  If the question cannot be answered with the given schema, or if it's not a question for a graph database, you MUST return the single word: `NONE`.
6.  Output ONLY the Cypher query or the word `NONE`. Do not add explanations, greetings, or markdown formatting like ```cypher.

**Example Question:** "What company sponsors Abaloparatide?"
**Example Valid Query:** MATCH p=(drug:Drug {{name: 'Abaloparatide'}})-[:SPONSORED_BY]->(sponsor:Sponsor) RETURN p

**Task:**
Generate a Cypher query for the question below.

**Question:** {question}
"""

SYNTHESIS_PROMPT = """
You are an AI assistant, an 'Inter-Expert Interpreter'. Your role is to deliver a comprehensive, accurate, and perfectly cited answer using ONLY the provided context.

**User's Question:** "{question}"

*** YOUR INSTRUCTIONS ***
1.  **Synthesize a Complete Answer**: Read all the provided context blocks and synthesize a single, cohesive answer to the user's question.
2.  **Cite Your Sources**: As you write, you MUST cite every fact. To do this, find the `Source Citation` for the context block you are using and place it directly after the fact it supports.
3.  **Create a Reference List**: After your main answer, create a "References" section. List each unique source you cited in a numbered list.
4.  **Be Honest**: If the context is insufficient to answer the question, you must state that clearly. Do not invent information.

---
**CONTEXT:**
{context_str}
---

**ANSWER:**
"""


########################################################################
### FILE: src/router/__init__.py
########################################################################




########################################################################
### FILE: src/router/persona_router.py
########################################################################

# src/routing/persona_router.py

import logging
import yaml
from pathlib import Path
from typing import List

from src.common_utils import get_project_root
from src.models import RetrievalPlan, NamespaceConfig # REFACTORED: Use Pydantic models

logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s')
logger = logging.getLogger(__name__)

# REFACTORED: A constant to scale retrieval depth by weight.
# A weight of 1.0 will result in top_k=15, 0.5 will result in top_k=7.
BASE_TOP_K = 15

class PersonaRouter:
    """
    Loads a persona-to-namespace map and creates a structured retrieval plan.
    It translates a persona into a list of weighted, configured namespaces to query.
    """
    def __init__(self, map_file_path: Path = None):
        """
        Initializes the router by loading the persona-to-namespace map.
        """
        if map_file_path is None:
            self.map_file_path = get_project_root() / "config" / "persona_namespace_map.yml"
        else:
            self.map_file_path = map_file_path

        try:
            with open(self.map_file_path, 'r') as f:
                self.persona_map = yaml.safe_load(f)
            logger.info(f"Successfully loaded persona map from {self.map_file_path}")
        except FileNotFoundError:
            logger.error(f"FATAL: Persona map file not found at {self.map_file_path}")
            # REFACTORED: Raise an exception instead of calling st.error
            raise
        except Exception as e:
            logger.error(f"FATAL: Error loading or parsing persona map file: {e}")
            self.persona_map = {}

    def get_retrieval_plan(self, persona: str) -> RetrievalPlan:
        """
        Gets the structured retrieval plan for a given persona.

        Args:
            persona: The name of the persona (e.g., 'Clinical Analyst').

        Returns:
            A RetrievalPlan object containing a list of configured namespaces.
        """
        normalized_persona = persona.lower().replace(" ", "_")
        persona_configs = self.persona_map.get(normalized_persona, self.persona_map.get('default', []))

        if not persona_configs:
            logger.warning(f"No plan found for persona '{normalized_persona}' or default. Returning empty plan.")
            return RetrievalPlan()

        # REFACTORED: Create NamespaceConfig models, now using the 'weight'
        namespace_configs = []
        for item in persona_configs:
            weight = item.get('weight', 1.0)
            # Dynamically calculate top_k based on weight
            top_k = int(max(3, BASE_TOP_K * weight))
            
            config = NamespaceConfig(
                namespace=item['namespace'],
                weight=weight,
                top_k=top_k
            )
            namespace_configs.append(config)
        
        plan = RetrievalPlan(namespaces=namespace_configs)
        logger.info(f"Retrieval plan for persona '{persona}': {plan.model_dump_json(indent=2)}")
        return plan


########################################################################
### FILE: src/router/tool_router.py
########################################################################

# FILE: src/router/tool_router.py
# V2.0: Modular dispatcher for the unified agent.
"""
Maps tool names from the planner to callable functions from the tool library.
This module replaces hardcoded routing with a pluggable, extensible registry.
"""
import logging
from typing import Callable, Dict

from src.models import QueryMetadata, ToolResult
from src.tools import retrievers # Import the entire retrievers module

logger = logging.getLogger(__name__)

class ToolRouter:
    """
    Executes a specific tool by name, providing a unified and safe interface.
    """
    def __init__(self):
        """
        Initializes the router by creating a registry of all available tools.
        The keys MUST match the 'tool_name' in `persona_tool_map.yml` and the planner logic.
        """
        self.registry: Dict[str, Callable[[str, QueryMetadata], ToolResult]] = {
            # Vector Search Tools
            "retrieve_clinical_data": retrievers.retrieve_clinical_data,
            "retrieve_summary_data": retrievers.retrieve_summary_data,
            "retrieve_general_text": retrievers.retrieve_general_text,
            
            # Graph Search Tool
            "query_knowledge_graph": retrievers.query_knowledge_graph,
            
            # Future tools can be added here easily:
            # "search_live_web": tools.search_live_web,
        }
        logger.info(f"ToolRouter initialized with {len(self.registry)} tools: {list(self.registry.keys())}")

    def execute_tool(self, tool_name: str, query: str, query_meta: QueryMetadata) -> ToolResult:
        """
        Looks up a tool by name in the registry and executes it.

        Args:
            tool_name: The name of the tool to execute.
            query: The original user query.
            query_meta: The structured metadata about the query.

        Returns:
            A ToolResult object with the outcome of the execution.
        """
        logger.info(f"[ToolRouter] Attempting to execute tool: '{tool_name}'")
        
        tool_function = self.registry.get(tool_name)

        if not tool_function:
            logger.warning(f"[ToolRouter] Tool '{tool_name}' not found in registry. Returning a failure result.")
            return ToolResult(
                tool_name=tool_name, 
                success=False, 
                content="[Error: Tool not implemented or misconfigured]"
            )

        try:
            # Call the registered function with the required arguments
            return tool_function(query, query_meta)
        except Exception as e:
            # This is a critical safety net. If a tool fails unexpectedly,
            # it won't crash the entire agent.
            logger.error(f"[ToolRouter] Tool '{tool_name}' failed with an unhandled exception: {e}", exc_info=True)
            return ToolResult(
                tool_name=tool_name, 
                success=False, 
                content=f"An unexpected error occurred in the tool: {e}"
            )


########################################################################
### FILE: src/tools/__init__.py
########################################################################




########################################################################
### FILE: src/tools/client.py
########################################################################

# FILE: src/tools/clients.py
# V2.0: Consolidated, cached client initializers.
"""
Single source for initializing and retrieving external service clients.
Uses caching to prevent re-initialization on every call.
"""

import os
import logging
from functools import lru_cache

import pinecone
import neo4j
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# --- Client Initializers (Cached for Performance) ---

@lru_cache(maxsize=1)
def get_google_ai_client() -> genai:
    """Initializes and returns the Google AI client."""
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")
        genai.configure(api_key=api_key)
        logger.info("Google AI client configured successfully.")
        return genai
    except Exception as e:
        logger.error(f"Failed to configure Google AI client: {e}")
        return None

@lru_cache(maxsize=1)
def get_pinecone_index() -> pinecone.Index:
    """Initializes and returns the Pinecone index handle."""
    try:
        api_key = os.getenv("PINECONE_API_KEY")
        index_name = os.getenv("PINECONE_INDEX_NAME")
        if not api_key or not index_name:
            raise ValueError("PINECONE_API_KEY or PINECONE_INDEX_NAME not set.")
        
        # Updated initialization for latest pinecone-client versions
        pc = pinecone.Pinecone(api_key=api_key)
        index = pc.Index(index_name)
        logger.info(f"Pinecone index '{index_name}' connected successfully.")
        return index
    except Exception as e:
        logger.error(f"Failed to connect to Pinecone index: {e}")
        return None

@lru_cache(maxsize=1)
def get_neo4j_driver() -> neo4j.Driver:
    """Initializes and returns the Neo4j driver."""
    try:
        uri = os.getenv("NEO4J_URI")
        user = os.getenv("NEO4J_USERNAME", "neo4j")
        password = os.getenv("NEO4J_PASSWORD")
        if not all([uri, user, password]):
            raise ValueError("Neo4j connection details (URI, USERNAME, PASSWORD) not set.")

        driver = neo4j.GraphDatabase.driver(uri, auth=(user, password))
        driver.verify_connectivity()
        logger.info("Neo4j driver connected successfully.")
        return driver
    except Exception as e:
        logger.error(f"Failed to create Neo4j driver: {e}")
        return None


########################################################################
### FILE: src/tools/retrievers.py
########################################################################

# FILE: src/tools/retrievers.py
# V2.0: Centralized, tool-based retrieval functions.
"""
This module contains the actual "tools" the agent can execute.
Each function corresponds to a tool name, fetches data from a source,
and returns a standardized ToolResult.
"""

import logging
from typing import List

import neo4j

from src.tools.clients import get_google_ai_client, get_pinecone_index, get_neo4j_driver
from src.models import ToolResult, QueryMetadata
from src.prompts import CYPHER_GENERATION_PROMPT

logger = logging.getLogger(__name__)

# --- Helper Functions ---

def _format_pinecone_results(matches: List[dict]) -> List[str]:
    """Standardizes Pinecone matches into a list of content strings with clear citations."""
    contents = []
    for match in matches:
        metadata = match.get('metadata', {})
        text = metadata.get('text', 'No content available.')
        doc_id = metadata.get('doc_id', 'Unknown Document')
        page_num = metadata.get('page_number') or metadata.get('page', 'N/A')
        score = match.get('score', 0.0)
        
        # Format a clean citation string for the LLM to use
        citation = f"[Source: {doc_id}, Page: {page_num}, Similarity: {score:.2f}]"
        contents.append(f"{text}\n{citation}")
        
    return contents

def _serialize_neo4j_path(path: neo4j.graph.Path) -> str:
    """Helper to convert a Neo4j Path into a readable text representation."""
    nodes_str = []
    for i, node in enumerate(path.nodes):
        node_label = next(iter(node.labels), "Node")
        name = node.get('name', node.get('id', 'Unknown'))
        nodes_str.append(f"({name}:{node_label})")
        if i < len(path.relationships):
            rel = path.relationships[i]
            nodes_str.append(f"-[{rel.type}]->")
    return "".join(nodes_str)

# --- Vector Search Tools ---

def _vector_search_tool(query: str, namespace: str, tool_name: str, top_k: int = 7) -> ToolResult:
    """Generic, reusable vector search tool for a specific Pinecone namespace."""
    pinecone_index = get_pinecone_index()
    embedding_client = get_google_ai_client()
    if not pinecone_index or not embedding_client:
        return ToolResult(tool_name=tool_name, success=False, content="Vector search client not available.")

    try:
        query_embedding = embedding_client.embed_content(
            model='models/text-embedding-004',  # Using a modern, recommended model
            content=query,
            task_type="retrieval_query"
        )['embedding']
        
        response = pinecone_index.query(
            namespace=namespace,
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True
        )
        
        if not response.get('matches'):
            return ToolResult(tool_name=tool_name, success=True, content="No relevant information was found for this query.")

        content_list = _format_pinecone_results(response['matches'])
        return ToolResult(tool_name=tool_name, success=True, content="\n---\n".join(content_list))

    except Exception as e:
        logger.error(f"Error in '{tool_name}' tool: {e}", exc_info=True)
        return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred while executing the tool: {e}")

# These are the concrete tool functions the ToolRouter will call.
# Each one calls the generic helper with its specific namespace.
def retrieve_clinical_data(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-clinical", "retrieve_clinical_data")

def retrieve_summary_data(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-summary", "retrieve_summary_data")

def retrieve_general_text(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-text", "retrieve_general_text")

# --- Graph Database Tool ---

def query_knowledge_graph(query: str, query_meta: QueryMetadata) -> ToolResult:
    """Generates and executes a Cypher query against the knowledge graph."""
    tool_name = "query_knowledge_graph"
    if not query_meta.question_is_graph_suitable:
        return ToolResult(
            tool_name=tool_name,
            success=True, # Succeeded in its decision to not run
            content="This question was determined to be unsuitable for the knowledge graph."
        )

    llm = get_google_ai_client().GenerativeModel('gemini-1.5-flash-latest')
    driver = get_neo4j_driver()
    if not llm or not driver:
        return ToolResult(tool_name=tool_name, success=False, content="LLM or Neo4j client is not available.")

    # Step 1: Generate the Cypher query
    try:
        # A more efficient schema retrieval for LLM prompts
        with driver.session() as session:
            schema_data = session.run("CALL db.schema.visualization()").data()
        schema_str = f"Node labels and properties: {schema_data[0]['nodes']}\nRelationship types: {schema_data[0]['relationships']}"

        prompt = CYPHER_GENERATION_PROMPT.format(schema=schema_str, question=query)
        response = llm.generate_content(prompt)
        cypher_query = response.text.strip().replace("```cypher", "").replace("```", "")
        
        if "none" in cypher_query.lower() or "match" not in cypher_query.lower():
            logger.warning(f"LLM decided not to generate a Cypher query for: '{query}'")
            return ToolResult(tool_name=tool_name, success=True, content="Could not generate a suitable graph query for this question.")

        logger.info(f"Generated Cypher: {cypher_query}")
    except Exception as e:
        logger.error(f"Error during Cypher generation: {e}", exc_info=True)
        return ToolResult(tool_name=tool_name, success=False, content=f"Cypher generation failed: {e}")

    # Step 2: Execute the generated Cypher query
    try:
        with driver.session() as session:
            records = session.run(cypher_query).data()

        if not records:
            return ToolResult(tool_name=tool_name, success=True, content="The graph query executed successfully but found no results.")
        
        results = []
        for record in records:
            if "p" in record and isinstance(record["p"], neo4j.graph.Path):
                results.append(_serialize_neo4j_path(record["p"]))
            else:
                # Fallback for non-path results (e.g., RETURN count(n))
                results.append(str(record))
        
        return ToolResult(tool_name=tool_name, success=True, content="\n".join(results))
    
    except Exception as e:
        logger.error(f"Error during Cypher execution for query '{cypher_query}': {e}", exc_info=True)
        return ToolResult(tool_name=tool_name, success=False, content=f"The generated Cypher query failed during execution: {e}")


########################################################################
### FILE: streamlit_app.py
########################################################################

# FILE: streamlit_app.py
# V2.1: Final UI connected to the unified, logging-integrated Agent.

import streamlit as st
import logging
from dotenv import load_dotenv

# --- CRITICAL: Load environment variables at the very top ---
# This ensures all modules can access them when imported.
load_dotenv()

# Now import project modules
from src.agent import Agent
from src.tools.clients import get_google_ai_client # Used for a pre-flight check

# --- Page and Logging Configuration ---
st.set_page_config(
    page_title="Persona RAG Chatbot",
    page_icon="🧠",
    layout="wide",
    initial_sidebar_state="expanded",
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - [%(name)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)

# --- Session State Initialization ---
if "agent" not in st.session_state:
    st.session_state.agent = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_persona" not in st.session_state:
    st.session_state.current_persona = "clinical_analyst"

# --- Helper Functions ---
@st.cache_resource
def initialize_agent():
    """
    Initializes the agent once and caches it.
    Returns the agent instance or None if initialization fails.
    """
    # Pre-flight check for API key to provide a better error message.
    if not get_google_ai_client():
        st.error("Google API Key is not configured. Please set the GOOGLE_API_KEY in your .env file.", icon="🚨")
        return None
    try:
        agent = Agent()
        logger.info("Unified agent initialized successfully and cached for the session.")
        return agent
    except Exception as e:
        st.error(f"Fatal error during agent initialization: {e}", icon="🚨")
        logger.error(f"Agent initialization failed: {e}", exc_info=True)
        return None

def reset_chat(persona_name: str):
    """Resets the chat history for a new conversation."""
    st.session_state.messages = [
        {"role": "assistant", "content": f"Hi! I'm now acting as a **{persona_name}**. How can I help you?"}
    ]

# --- Sidebar ---
with st.sidebar:
    st.header("🤖 Persona RAG Chatbot")
    st.markdown("Select a persona to tailor my retrieval strategy and answers to your specific role.")

    persona_options = {
        'Clinical Analyst': 'clinical_analyst',
        'Health Economist': 'health_economist',
        'Regulatory Specialist': 'regulatory_specialist',
    }
    
    current_display_name = [k for k, v in persona_options.items() if v == st.session_state.current_persona][0]
    
    selected_persona_name = st.radio(
        "**Choose your Persona:**",
        options=persona_options.keys(),
        index=list(persona_options.keys()).index(current_display_name),
        key="persona_selector"
    )
    
    selected_persona_key = persona_options[selected_persona_name]

    if selected_persona_key != st.session_state.current_persona:
        st.session_state.current_persona = selected_persona_key
        reset_chat(selected_persona_name)
        st.rerun()

    st.divider()
    if st.button("🔄 Clear Chat History", use_container_width=True):
        reset_chat(selected_persona_name)
        st.rerun()

# --- Main Chat Interface ---
st.title("Persona-Aware RAG Agent")
st.caption(f"Currently acting as: **{selected_persona_name}**")

# Initialize agent on first run and handle potential failure.
if st.session_state.agent is None:
    st.session_state.agent = initialize_agent()
    # If this is the first run, set the initial message.
    if not st.session_state.messages:
        reset_chat(selected_persona_name)

# Display chat messages
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"], unsafe_allow_html=True)

# Handle user input
if prompt := st.chat_input("Ask your question..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.agent:
            with st.spinner(f"Thinking as a {selected_persona_name}..."):
                response = st.session_state.agent.run(prompt, persona=st.session_state.current_persona)
                st.markdown(response, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            st.error("Agent is not available due to an initialization error. Please check the terminal logs.")
            st.stop()


########################################################################
### FILE: tests/planner/test_query_classifier.py
########################################################################

"""Unit tests for QueryClassifier.

Run with:
    pytest -q tests/planner/test_query_classifier.py
"""
from src.planner.query_classifier import QueryClassifier, QueryMeta, Intent, Complexity


def test_basic_classification_stub(monkeypatch):
    """The stub Gemini client always returns the same JSON → make sure mapping works."""

    qc = QueryClassifier()
    q = "What is the PBAC outcome for belantamab mafodotin?"
    meta: QueryMeta = qc(q)

    assert isinstance(meta, QueryMeta)
    assert meta.intent in {"factual", "comparative", "procedural", "citation", "other"}
    assert meta.complexity in {"simple", "moderate", "complex"}
    assert 0 <= meta.confidence_needed <= 1

    # Stub response in query_classifier.GeminiClient.call_gemini → 'factual', 'simple', 0.75
    assert meta.intent == "factual"
    assert meta.complexity == "simple"
    assert meta.confidence_needed == 0.75


def test_cache(monkeypatch):
    """Ensure LRU cache hits (no second Gemini call)."""

    calls = []

    def fake_call(payload):
        calls.append(payload)
        return {"intent": "other", "complexity": "moderate", "confidence": 0.5}

    monkeypatch.setattr("src.planner.query_classifier.GeminiClient.call_gemini", fake_call)

    qc = QueryClassifier(cache_size=2)
    q = "Explain indirect costs in PBAC submissions"
    first = qc(q)
    second = qc(q)  # cached

    assert first == second
    # Gemini called exactly once
    assert len(calls) == 1
