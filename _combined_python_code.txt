========================================================================
  Combined Python Files from Git Repository: /home/mit/persona_rag_chatbot
  Generated on: Thu 31 Jul 2025 20:38:28 AEST
========================================================================



########################################################################
### FILE: src/__init__.py
########################################################################




########################################################################
### FILE: src/agent.py
########################################################################

# FILE: src/agent.py
# V5.5 (Definitive Prompt Fix): Filtered out empty tool results before synthesis to prevent LLM confusion.

import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import List, Tuple

from src.tools.clients import get_generative_model, get_flash_model, DEFAULT_REQUEST_OPTIONS
from src.models import ToolResult, QueryMetadata, ToolPlanItem
from src.planner.query_classifier import QueryClassifier
from src.planner.tool_planner import ToolPlanner
from src.planner.persona_classifier import PersonaClassifier
from src.planner.query_rewriter import QueryRewriter
from src.router.tool_router import ToolRouter
from src.fallback import should_trigger_fallback
from src.prompts import DECOMPOSITION_PROMPT, REASONING_SYNTHESIS_PROMPT, DIRECT_SYNTHESIS_PROMPT

logger = logging.getLogger(__name__)
LOG_PATH = Path("trace_logs.jsonl")

class Timer:
    def __init__(self, name): self.name = name
    def __enter__(self): self.start = time.perf_counter(); return self
    def __exit__(self, *args): self.end = time.perf_counter(); logger.info(f"[TIMER] {self.name} took {(self.end - self.start) * 1000:.2f} ms")

def log_trace(query: str, persona: str, query_meta: QueryMetadata, tool_plan: List[ToolPlanItem], tool_results: List[ToolResult], final_answer: str, total_latency_sec: float):
    # (Code unchanged)
    trace_record = {
        "timestamp": datetime.utcnow().isoformat() + "Z", "query": query, "persona": persona,
        "intent": query_meta.intent if query_meta else "classification_failed",
        "graph_suitable": query_meta.question_is_graph_suitable if query_meta else "unknown",
        "tool_plan": [t.model_dump() for t in tool_plan] if tool_plan else [],
        "tool_results": [r.model_dump() for r in tool_results] if tool_results else [],
        "final_answer_preview": final_answer[:200] + "..." if final_answer else "N/A",
        "total_latency_sec": round(total_latency_sec, 3)
    }
    try:
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(trace_record) + "\n")
    except Exception as e:
        logger.error(f"Failed to write to trace log: {e}", exc_info=True)


class Agent:
    def __init__(self, confidence_threshold: float = 0.85):
        # (Code unchanged)
        self.classifier = QueryClassifier()
        self.planner = ToolPlanner(coverage_threshold=confidence_threshold)
        self.router = ToolRouter()
        self.persona_classifier = PersonaClassifier()
        self.rewriter = QueryRewriter()
        self.llm = get_generative_model('gemini-1.5-pro-latest')
        self.decomposer_llm = get_flash_model('gemini-1.5-flash-latest')

    def _run_single_rag_step(self, query: str, persona: str) -> Tuple[str, QueryMetadata, List[ToolPlanItem], List[ToolResult]]:
        final_answer, query_meta, tool_plan, results = "", None, [], []
        with Timer(f"Single RAG Step for '{query[:30]}...'"):
            with Timer("Query Classification"):
                query_meta = self.classifier.classify(query)
            if not query_meta: return "I had trouble understanding the sub-question.", query_meta, tool_plan, results

            with Timer("Tool Planning"):
                tool_plan = self.planner.plan(query_meta, persona)
            if not tool_plan: return "I don't have a configured strategy for this sub-question.", query_meta, tool_plan, results

            results = [self.router.execute_tool(item.tool_name, query, query_meta) for item in tool_plan]

            # --- START OF DEFINITIVE FIX ---
            # The previous logic was too simple. We must ONLY use content from tools that
            # were successful AND returned non-empty content. This prevents the final
            # prompt from being "poisoned" with empty results, which confuses the LLM.
            
            successful_results = [res for res in results if res and res.success and res.content and res.content.strip()]
            
            if not successful_results:
                logger.warning("All tools ran but returned no content. Triggering fallback.")
                return "I searched but could not find any relevant details for this step.", query_meta, tool_plan, results

            successful_content = [res.content for res in successful_results]
            # --- END OF DEFINITIVE FIX ---
            
            formatted_context = "\n---\n".join(successful_content)
            final_prompt = DIRECT_SYNTHESIS_PROMPT.format(question=query, context_str=formatted_context)
            
            with Timer("Direct Synthesis LLM Call (Flash)"):
                if not self.decomposer_llm: return "Flash model not available.", query_meta, tool_plan, results
                response = self.decomposer_llm.generate_content(final_prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            
            final_answer = response.text
            return final_answer, query_meta, tool_plan, results

    def run(self, query: str, persona: str, chat_history: List[str]) -> str:
        # (Code is identical to the last correct version, no changes needed here)
        run_start_time = time.perf_counter()
        final_answer, final_query_meta, final_tool_plan, final_tool_results = "", None, [], []
        try:
            with Timer("Full Agent Run"):
                if not self.llm or not self.decomposer_llm: return "Error: AI model not available."
                with Timer("Query Rewriting"):
                    rewritten_query = self.rewriter.rewrite(query, chat_history)
                with Timer("Decomposition"):
                    try:
                        decomp_prompt = DECOMPOSITION_PROMPT.format(chat_history="\n- ".join(chat_history), question=rewritten_query)
                        decomp_response = self.decomposer_llm.generate_content(decomp_prompt, request_options=DEFAULT_REQUEST_OPTIONS)
                        if not decomp_response.parts: raise ValueError("LLM returned empty/blocked response")
                        plan_data = json.loads(decomp_response.text)
                    except (json.JSONDecodeError, ValueError, Exception) as e:
                        logger.warning(f"Could not decompose query: {e}. Defaulting to single-step plan.")
                        plan_data = {"requires_decomposition": False, "plan": [rewritten_query]}
                
                requires_decomposition = plan_data.get("requires_decomposition", False)
                plan = plan_data.get("plan", [rewritten_query])

                if not requires_decomposition or len(plan) == 1:
                    logger.info(f"Executing single-step plan for query: '{plan[0]}'")
                    with Timer("Persona Classification"):
                        chosen_persona = self.persona_classifier.classify(plan[0]) if persona == "automatic" else persona
                    persona_display_name = " ".join(word.capitalize() for word in chosen_persona.split("_"))
                    synthesis_result, final_query_meta, final_tool_plan, final_tool_results = self._run_single_rag_step(plan[0], chosen_persona)
                    if persona == "automatic": final_answer = f"Acting as a **{persona_display_name}**, here is what I found:\n\n{synthesis_result}"
                    else: final_answer = synthesis_result
                else:
                    logger.info(f"Executing multi-step plan for query: '{rewritten_query}'")
                    scratchpad, all_sub_results = [], []
                    for sub_query in plan:
                        logger.info(f"  -> Executing sub-query: '{sub_query}'")
                        with Timer("Persona Classification (sub-query)"):
                            sub_persona = self.persona_classifier.classify(sub_query)
                        sub_answer, sub_meta, sub_plan, sub_results = self._run_single_rag_step(sub_query, sub_persona)
                        observation = f"Sub-Question: {sub_query}\nFinding: {sub_answer}"
                        scratchpad.append(observation)
                        final_query_meta, final_tool_plan = sub_meta, sub_plan
                        all_sub_results.extend(sub_results)
                    final_tool_results = all_sub_results
                    with Timer("Reasoning Synthesis LLM Call (Pro)"):
                        synthesis_prompt = REASONING_SYNTHESIS_PROMPT.format(question=rewritten_query, scratchpad="\n\n---\n\n".join(scratchpad))
                        synthesis_response = self.llm.generate_content(synthesis_prompt, request_options=DEFAULT_REQUEST_OPTIONS)
                    final_answer = synthesis_response.text
                return final_answer
        except Exception as e:
            logger.error(f"An unexpected error occurred during agent run: {e}", exc_info=True)
            return "I encountered a critical error. Please check the system logs."
        finally:
            run_end_time = time.perf_counter()
            total_duration_sec = run_end_time - run_start_time
            log_trace(query, persona, final_query_meta, final_tool_plan, final_tool_results, final_answer, total_duration_sec)


########################################################################
### FILE: src/fallback.py
########################################################################

# FILE: src/fallback.py
# Phase 3.1: FallbackLayer — graceful UX when all tools return empty or fail

import logging
from typing import List
from src.models import ToolResult

logger = logging.getLogger(__name__)

FALLBACK_QUESTIONS = [
    "Would you like to compare two drugs instead?",
    "Can I help you find a sponsor for a specific medicine?",
    "Do you want to search the original PDF documents directly?"
]


def should_trigger_fallback(results: List[ToolResult]) -> bool:
    """
    Returns True if all tools failed or produced no meaningful content.
    """
    if not results:
        logger.info("Fallback triggered: no tool results returned.")
        return True

    empty_or_failed = [r for r in results if not r.success or not r.content or len(r.content.strip()) < 5]
    if len(empty_or_failed) == len(results):
        logger.info("Fallback triggered: all tools failed or content was empty.")
        return True

    return False


def render_fallback_message(query: str) -> str:
    """
    Returns a polite fallback message with suggested next questions.
    """
    msg = f"""
I'm sorry — based on the current documents and tools, I couldn't find sufficient information to answer your question:

"{query}"

However, here are some things you can try next:

"""
    for i, q in enumerate(FALLBACK_QUESTIONS, start=1):
        msg += f"{i}. {q}\n"

    msg += "\nYou can also try rephrasing your question for better results."
    return msg.strip()



########################################################################
### FILE: src/models.py
########################################################################

from pydantic import BaseModel
from typing import List, Optional, Literal


QueryIntent = Literal[
    "specific_fact_lookup",
    "simple_summary",
    "comparative_analysis",
    "general_qa",
    "unknown"
]


class QueryMetadata(BaseModel):
    intent: QueryIntent
    keywords: List[str]
    question_is_graph_suitable: bool


class ToolPlanItem(BaseModel):
    tool_name: str
    estimated_coverage: float


class ToolResult(BaseModel):
    tool_name: str
    success: bool
    content: str
    estimated_coverage: float = 0.0


class ContextItem(BaseModel):
    content: str
    source: Optional[dict] = None


class Source(BaseModel):
    type: str
    document_id: Optional[str] = None
    page_numbers: Optional[List[int]] = None
    source_url: Optional[str] = None
    retrieval_score: Optional[float] = None
    query: Optional[str] = None

class NamespaceConfig(BaseModel):
    namespace: str
    weight: float
    top_k: int

class RetrievalPlan(BaseModel):
    namespaces: List[NamespaceConfig] = []


########################################################################
### FILE: src/planner/__init__.py
########################################################################




########################################################################
### FILE: src/planner/persona_classifier.py
########################################################################

# FILE: src/planner/persona_classifier.py
# V1.1 (Resilience Fix): Added request_options to the generate_content call.

import logging
from typing import Literal
# --- DEFINITIVE FIX: Import the config and model getter ---
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS

logger = logging.getLogger(__name__)

Persona = Literal["clinical_analyst", "health_economist", "regulatory_specialist"]

PERSONA_CLASSIFICATION_PROMPT = """
You are an expert request router. Your task is to analyze the user's question and determine which specialist persona is best equipped to answer it. You must choose from the available personas and provide ONLY the persona's key name as your response.

**Available Personas & Their Expertise:**

1.  **`clinical_analyst`**:
    *   Focuses on: Clinical trial data, drug efficacy, safety profiles, patient outcomes, medical conditions, and mechanisms of action.
    *   Keywords: treat, condition, indication, dosage, patients, trial, effective, side effects.
    *   Choose this persona for questions about the medical and scientific aspects of a drug.

2.  **`health_economist`**:
    *   Focuses on: Cost-effectiveness, pricing, market access, economic evaluations, and healthcare policy implications.
    *   Keywords: cost, price, economic, budget, financial, value, policy, summary.
    *   Choose this persona for questions about the financial or policy-level impact of a drug.

3.  **`regulatory_specialist`**:
    *   Focuses on: Submission types, meeting agendas, regulatory pathways (e.g., PBS listing types), sponsors, and official guidelines.
    *   Keywords: sponsor, submission, listing, agenda, meeting, guideline, change, status.
    *   Choose this persona for questions about the process and logistics of drug approval and listing.

**User Question:**
"{question}"

**Instructions:**
- Read the user's question carefully.
- Compare it against the expertise of each persona.
- Return ONLY the single key name (e.g., `clinical_analyst`) of the best-fitting persona. Do not add any explanation or other text.
"""

class PersonaClassifier:
    def __init__(self):
        # --- DEFINITIVE FIX: Use the new centralized model getter ---
        self.llm = get_flash_model()
        if not self.llm:
            logger.error("FATAL: Gemini client not initialized, PersonaClassifier will not work.")

    def classify(self, query: str) -> Persona:
        """Classifies the query and returns the most appropriate persona key."""
        if not self.llm:
            return "regulatory_specialist" # A safe default

        try:
            prompt = PERSONA_CLASSIFICATION_PROMPT.format(question=query)
            # --- DEFINITIVE FIX: Add request_options to the call ---
            response = self.llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            persona_key = response.text.strip()

            if persona_key in ["clinical_analyst", "health_economist", "regulatory_specialist"]:
                logger.info(f"Query classified for persona: '{persona_key}'")
                return persona_key
            else:
                logger.warning(f"Persona classification returned an invalid key: '{persona_key}'. Falling back to default.")
                return "regulatory_specialist"
        except Exception as e:
            logger.error(f"Persona classification failed: {e}", exc_info=True)
            return "regulatory_specialist" # Fallback on error


########################################################################
### FILE: src/planner/query_classifier.py
########################################################################

# FILE: src/planner/query_classifier.py
# V1.1 (Resilience Fix): Added request_options to the generate_content call.

import logging
from typing import Optional
from src.models import QueryMetadata
from src.prompts import QUERY_CLASSIFICATION_PROMPT
# --- DEFINITIVE FIX: Import the config and model getter ---
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS

logger = logging.getLogger(__name__)

class QueryClassifier:
    def __init__(self):
        # --- DEFINITIVE FIX: Use the new centralized model getter ---
        self.model = get_flash_model()

    def classify(self, query: str) -> Optional[QueryMetadata]:
        """Classifies a user query into intent, keywords, and graph suitability."""
        if not self.model:
            logger.error("QueryClassifier model is not available.")
            return None
        logger.info(f"Classifying query: {query}")
        try:
            prompt = QUERY_CLASSIFICATION_PROMPT + f"\n\nUser Query: {query}"
            # --- DEFINITIVE FIX: Add request_options to the call ---
            response = self.model.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            parsed_json = response.text.strip()
            metadata = QueryMetadata.model_validate_json(parsed_json)
            logger.info(f"Classification result: {metadata.model_dump_json(indent=2)}")
            return metadata
        except Exception as e:
            logger.error(f"Query classification failed: {e}", exc_info=True)
            return None


########################################################################
### FILE: src/planner/query_rewriter.py
########################################################################

# FILE: src/planner/query_rewriter.py
# V2.0 (Performance Fix): Added a heuristic check to bypass the LLM call for non-conversational queries.

import logging
from typing import List
# --- DEFINITIVE FIX: Import the config and model getter ---
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS

logger = logging.getLogger(__name__)

# --- DEFINITIVE FIX: Heuristic pre-flight check ---
# A list of common words that indicate a query is conversational and likely needs context from chat history.
# We check for these words (as whole words, case-insensitively) before making a slow API call.
CONVERSATIONAL_TRIGGERS = {"it", "its", "they", "them", "that", "those", "this", "these"}

REWRITE_PROMPT = """
You are an expert query analyst. Your task is to rewrite a user's latest question into a standalone question that can be understood without the context of the chat history.

**CRITICAL RULES:**
1.  **If the "Latest User Question" is already a complete, standalone question, you MUST return it exactly as it is.** Do not modify it.
2.  If the "Latest User Question" contains pronouns (like "it", "its", "they") or ambiguous references ("this drug", "that"), use the "Chat History" to resolve these references and create a complete question.
3.  Your output MUST be only the rewritten question. Do not add any commentary.

**EXAMPLE 1 (Rewrite Needed):**
- **Chat History:**
  - user: Who is the sponsor for Esketamine?
  - assistant: Janssen-Cilag Pty Ltd. is the sponsor for Esketamine.
- **Latest User Question:** what is its use?
- **Your Rewritten Question:** What is the use of Esketamine?

**EXAMPLE 2 (No Rewrite Needed):**
- **Chat History:**
  - user: Who is the sponsor for Esketamine?
  - assistant: Janssen-Cilag Pty Ltd. is the sponsor for Esketamine.
- **Latest User Question:** What is the dosage form for Fruquintinib?
- **Your Rewritten Question:** What is the dosage form for Fruquintinib?

**TASK:**
- **Chat History:**
{chat_history}
- **Latest User Question:** {question}

**Your Rewritten Question:**
"""

class QueryRewriter:
    def __init__(self):
        # --- DEFINITIVE FIX: Use the new centralized model getter ---
        self.llm = get_flash_model()
        if not self.llm:
            logger.error("FATAL: Gemini client not initialized, QueryRewriter will not work.")

    def rewrite(self, query: str, chat_history: List[str]) -> str:
        """Rewrites a conversational query into a standalone query, with a performance-enhancing pre-check."""
        if not self.llm or not chat_history:
            return query # If no history or no LLM, cannot rewrite.

        # --- DEFINITIVE FIX: Performance optimization ---
        # Check if the query contains any conversational trigger words.
        # This avoids a slow network call for the majority of queries which are already standalone.
        query_words = set(query.lower().split())
        if not CONVERSATIONAL_TRIGGERS.intersection(query_words):
            logger.info(f"Query deemed standalone. Bypassing LLM rewrite. Query: '{query}'")
            return query
        # --- End of performance optimization ---

        try:
            formatted_history = "\n  - ".join(chat_history)
            prompt = REWRITE_PROMPT.format(chat_history=formatted_history, question=query)
            
            # --- DEFINITIVE FIX: Add request_options to the call ---
            response = self.llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            rewritten_query = response.text.strip()
            
            if rewritten_query:
                logger.info(f"Original query: '{query}' -> Rewritten query: '{rewritten_query}'")
                return rewritten_query
            else:
                logger.warning("Query rewrite resulted in an empty string. Using original query.")
                return query
        except Exception as e:
            logger.error(f"Query rewriting failed: {e}", exc_info=True)
            return query # Fallback to original query on error


########################################################################
### FILE: src/planner/tool_planner.py
########################################################################

# FILE: src/planner/tool_planner.py
# V2.0: Persona-Aware Tool Planner
import logging
import yaml
from pathlib import Path
from typing import List, Dict

from src.models import QueryMetadata, ToolPlanItem

logger = logging.getLogger(__name__)

# Base scores for tools based on query intent. A higher score is better.
# These tool names MUST match the names in `persona_tool_map.yml` and the tool router.
INTENT_TOOL_SCORES = {
    # If the user wants a specific fact...
    "specific_fact_lookup": {
        "query_knowledge_graph": 0.9,
        "retrieve_clinical_data": 0.7,
        "retrieve_general_text": 0.6,
        "retrieve_summary_data": 0.4,
    },
    # If the user wants a high-level summary...
    "simple_summary": {
        "retrieve_summary_data": 0.9,
        "retrieve_general_text": 0.8,
        "retrieve_clinical_data": 0.5,
        "query_knowledge_graph": 0.4,
    },
    # If the user wants to compare things...
    "comparative_analysis": {
        "retrieve_clinical_data": 0.8,
        "retrieve_summary_data": 0.8,
        "retrieve_general_text": 0.7,
        "query_knowledge_graph": 0.5,
    },
    # For general questions...
    "general_qa": {
        "retrieve_general_text": 0.9,
        "retrieve_summary_data": 0.7,
        "query_knowledge_graph": 0.6,
        "retrieve_clinical_data": 0.5,
    },
}
DEFAULT_INTENT_SCORE = 0.5
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

class ToolPlanner:
    def __init__(self, coverage_threshold: float = 0.9):
        self.coverage_threshold = coverage_threshold
        self._load_persona_map()

    def _load_persona_map(self):
        """Loads the persona-to-tool mapping from the central YAML config."""
        map_file = PROJECT_ROOT / "config" / "persona_tool_map.yml"
        try:
            with open(map_file, 'r') as f:
                self.persona_map = yaml.safe_load(f)
            logger.info(f"Successfully loaded persona-tool map from '{map_file}'.")
        except Exception as e:
            logger.error(f"FATAL: Could not load or parse persona-tool map from '{map_file}': {e}", exc_info=True)
            self.persona_map = {}

    def plan(self, query_meta: QueryMetadata, persona: str) -> List[ToolPlanItem]:
        """
        Creates a ranked tool plan by combining query intent with user persona preferences.
        """
        logger.info(f"Planning tools for intent '{query_meta.intent}' and persona '{persona}'")
        
        # 1. Get the list of preferred tools and their weights for the given persona
        persona_key = persona.lower().replace(" ", "_")
        persona_prefs = self.persona_map.get(persona_key, self.persona_map.get("default", []))

        if not persona_prefs:
            logger.warning(f"No tool preferences found for persona '{persona_key}' or default. Returning empty plan.")
            return []
            
        persona_tool_weights: Dict[str, float] = {p["tool_name"]: p["weight"] for p in persona_prefs}
        
        # 2. Get intent-based scores for tools relevant to the current query intent
        intent_scores = INTENT_TOOL_SCORES.get(query_meta.intent, {})

        # 3. Calculate a final score for each tool by multiplying persona weight and intent score
        scored_tools = []
        for tool_name, persona_weight in persona_tool_weights.items():
            intent_score = intent_scores.get(tool_name, DEFAULT_INTENT_SCORE)
            
            # The final score reflects both the persona's general preference and the tool's suitability for the task
            final_score = persona_weight * intent_score
            scored_tools.append({"name": tool_name, "score": final_score})

        # 4. Sort tools by their final score in descending order
        scored_tools.sort(key=lambda x: x["score"], reverse=True)

        # 5. Build the final plan, adding tools until the cumulative coverage threshold is met
        final_plan: List[ToolPlanItem] = []
        total_coverage = 0.0
        for tool in scored_tools:
            # We treat the score as the estimated coverage for this planning step
            estimated_coverage = round(tool["score"], 2)

            # Do not add tools with negligible contribution
            if estimated_coverage <= 0.1:
                continue

            plan_item = ToolPlanItem(tool_name=tool["name"], estimated_coverage=estimated_coverage)
            final_plan.append(plan_item)
            
            total_coverage += estimated_coverage
            if total_coverage >= self.coverage_threshold:
                logger.info(f"Coverage threshold of {self.coverage_threshold} met. Finalizing plan.")
                break
        
        logger.info(f"Generated tool plan: {[t.model_dump_json(indent=2) for t in final_plan]}")
        return final_plan


########################################################################
### FILE: src/prompts.py
########################################################################

# FILE: src/prompts.py
# V3.0 (ReAct Architecture): Added prompts for multi-step reasoning.

"""
Production-grade prompts for a robust RAG agent. This version supports both
direct RAG and a multi-step ReAct (Reason+Act) style reasoning loop.
"""

QUERY_CLASSIFICATION_PROMPT = """
You are an expert query analysis agent. Your task is to analyze the user's question and provide a structured JSON output with three fields: 'intent', 'keywords', and 'question_is_graph_suitable'.

1.  **'intent'**: Classify the user's goal into one of these categories:
    *   "specific_fact_lookup": For questions seeking a single, direct answer (e.g., "What company sponsors Drug X?").
    *   "simple_summary": For questions asking for a general overview (e.g., "Tell me about Drug Y.").
    *   "comparative_analysis": For questions that compare two or more items (e.g., "Compare Drug A and Drug B.").
    *   "general_qa": For all other questions.

2.  **'keywords'**: Extract the most important nouns and proper nouns from the question, such as drug names, company names, or medical conditions. Return them as a list of strings.

3.  **'question_is_graph_suitable'**: Return `true` if the question involves relationships between entities (e.g., drug-to-sponsor, drug-to-condition), which are suitable for a knowledge graph. Otherwise, return `false`.

Output ONLY the raw JSON object. Do not add explanations or markdown formatting.
"""

CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema.

**Live Graph Schema:**
{schema}

**CRITICAL Instructions:**
1.  Your `RETURN` clause should look like this: `RETURN p, properties(r) as rel_props`.
2.  The relationship in your `MATCH` clause must be assigned to a variable `r`.
3.  Query against the `name_normalized` property for all `WHERE` clauses on nodes.
4.  If the question cannot be answered, you MUST return the single word: `NONE`.
5.  Output ONLY the Cypher query or the word `NONE`.

**Example Question:** "What company sponsors Abaloparatide?"
**Example Valid Query:** MATCH p=(drug:Entity)-[r:HASSPONSOR]->(sponsor:Entity) WHERE drug.name_normalized = 'abaloparatide' RETURN p, properties(r) as rel_props

**Question:** {question}
"""

# --- START OF DEFINITIVE FIX ---

DECOMPOSITION_PROMPT = """
You are a master query planner. Your goal is to determine if a user's question can be answered in a single step or if it requires decomposition into multiple, simpler sub-questions.

Analyze the user's question and the chat history.

**Decision Criteria:**
- **Single Step:** If the question asks for a direct fact, a summary of a single topic, or a simple definition.
- **Decomposition:** If the question requires comparing information from two or more distinct topics (e.g., two drugs, two meetings), finding an intersection of two sets of information (e.g., "sponsors who submitted in BOTH meetings"), or involves a sequence of steps.

**Output Schema:**
You MUST output a single, valid JSON object with two keys:
1.  `requires_decomposition`: A boolean (`true` or `false`).
2.  `plan`: A list of strings.
    - If `requires_decomposition` is `false`, the plan should contain a single item: the original question.
    - If `requires_decomposition` is `true`, the plan should contain two or more simple, answerable sub-questions that build on each other to answer the original question.

**Example 1 (Single Step):**
- User Question: "What is the use of Esketamine?"
- Your JSON Output:
{{
  "requires_decomposition": false,
  "plan": ["What is the use of Esketamine?"]
}}

**Example 2 (Decomposition):**
- User Question: "Which companies submitted drugs in both the March 2024 and May 2024 PBAC meetings?"
- Your JSON Output:
{{
  "requires_decomposition": true,
  "plan": [
    "List all sponsors who made submissions in the March 2024 PBAC meeting documents.",
    "List all sponsors who made submissions in the May 2024 PBAC meeting documents."
  ]
}}

**TASK:**
- Chat History: {chat_history}
- User Question: {question}

Now, generate the JSON output.
"""

# --- END OF DEFINITIVE FIX ---

REASONING_SYNTHESIS_PROMPT = """
You are a highly intelligent synthesis agent. Your task is to answer a user's complex original question based on a series of observations you have made by answering simpler sub-questions.

**User's Original Question:** "{question}"

**Your Observations (Scratchpad):**
---
{scratchpad}
---

**CRITICAL INSTRUCTIONS:**
1.  Read the user's original question and all your observations from the scratchpad.
2.  Synthesize a final, comprehensive answer to the original question.
3.  **Do not show your step-by-step reasoning.** Just provide the final, clean answer.
4.  If your observations are insufficient to answer the question, clearly state what information you found and why it is not enough.
5.  Include citations from your observations where appropriate.

**Final Answer:**
"""

# --- END: New Prompts for ReAct Agent ---


# --- START OF DEFINITIVE, HARDENED PROMPT ---
DIRECT_SYNTHESIS_PROMPT = """
You are a document analysis bot. Your ONLY job is to answer the user's question using the provided evidence.

**TASK:**
1.  Read the User's Question.
2.  Read the Evidence blocks. Each block has a piece of text and a citation link.
3.  Synthesize a direct answer to the question.
4.  **IMPERATIVE:** You MUST end your answer with the citation link from the evidence you used.
5.  If the evidence does not contain the answer, you MUST state "The provided evidence does not contain the answer." and nothing else.

**EXAMPLE:**
User's Question: "What is the sponsor for DrugX?"
Evidence:
---
Evidence from graph: DrugX has sponsor CompanyY.
Citation: <a href="..." target="_blank">Document A (Page 5)</a>
---
Your Answer:
The sponsor for DrugX is CompanyY. <a href="..." target="_blank">Document A (Page 5)</a>

---
**User's Question:** "{question}"
---
**Evidence:**
{context_str}
---
**Your Answer:**
"""
# --- END OF DEFINITIVE, HARDENED PROMPT ---


########################################################################
### FILE: src/router/__init__.py
########################################################################




########################################################################
### FILE: src/router/tool_router.py
########################################################################

# FILE: src/router/tool_router.py
# V4.1 (Definitive Fix): Added the missing `return` statement to the success path.

import logging
from typing import Callable, Dict

from src.models import QueryMetadata, ToolResult
from src.tools import retrievers

logger = logging.getLogger(__name__)

class ToolRouter:
    def __init__(self):
        self.registry: Dict[str, Callable[[str, QueryMetadata], ToolResult]] = {
            "retrieve_clinical_data": retrievers.retrieve_clinical_data,
            "retrieve_summary_data": retrievers.retrieve_summary_data,
            "retrieve_general_text": retrievers.retrieve_general_text,
            "query_knowledge_graph": retrievers.query_knowledge_graph,
        }
        logger.info(f"ToolRouter initialized with {len(self.registry)} tools.")

    def execute_tool(self, tool_name: str, query: str, query_meta: QueryMetadata) -> ToolResult:
        logger.info(f"[ToolRouter] Executing tool: '{tool_name}'")
        tool_function = self.registry.get(tool_name)
        if not tool_function:
            logger.warning(f"Tool '{tool_name}' not found in registry.")
            return ToolResult(tool_name=tool_name, success=False, content="[Error: Tool not implemented]")
        try:
            # --- START OF DEFINITIVE FIX ---
            # The previous version was missing this `return` statement.
            # This caused all successful tool runs to return `None` to the agent.
            return tool_function(query, query_meta)
            # --- END OF DEFINITIVE FIX ---
        except Exception as e:
            logger.error(f"[ToolRouter] Tool '{tool_name}' failed: {e}", exc_info=True)
            return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred: {e}")


########################################################################
### FILE: src/tools/__init__.py
########################################################################




########################################################################
### FILE: src/tools/clients.py
########################################################################

# FILE: src/tools/clients.py
# V2.4 (Critical Error Fix): Correctly implemented the Retry policy using a `predicate` function.

import os
import logging
from functools import lru_cache

import pinecone
import neo4j
import google.generativeai as genai
from google.generativeai.client import get_default_generative_client
from google.api_core.retry import Retry
from google.api_core.client_options import ClientOptions
from google.api_core import exceptions as google_exceptions
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# --- START OF DEFINITIVE FIX: Correct Retry Implementation ---

def is_service_unavailable(exc: Exception) -> bool:
    """Predicate function to check if an exception is a ServiceUnavailable error."""
    return isinstance(exc, google_exceptions.ServiceUnavailable)

# Define a more robust retry strategy for API calls.
DEFAULT_RETRY = Retry(
    # Use the predicate function to decide if we should retry.
    predicate=is_service_unavailable,
    initial=1.0,      # Start with a 1-second delay
    maximum=10.0,     # Maximum delay of 10 seconds
    multiplier=2.0,   # Double the delay each time
    deadline=30.0,    # Total deadline for all retries, including the initial call
)

# This remains the same.
DEFAULT_REQUEST_OPTIONS = {"retry": DEFAULT_RETRY, "timeout": 15.0}

# --- END OF DEFINITIVE FIX ---


# --- Client Initializers (Cached for Performance) ---

@lru_cache(maxsize=1)
def get_google_ai_client() -> genai:
    """Initializes and returns the Google AI client."""
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")
        genai.configure(api_key=api_key)
        get_default_generative_client()._client_options = ClientOptions(api_endpoint="generativelanguage.googleapis.com")
        
        logger.info("Google AI client configured successfully.")
        return genai
    except Exception as e:
        logger.error(f"Failed to configure Google AI client: {e}")
        return None

# --- Model Getters (No changes needed here) ---

@lru_cache(maxsize=2)
def get_generative_model(model_name: str = 'gemini-1.5-pro-latest') -> genai.GenerativeModel:
    client = get_google_ai_client()
    if not client: return None
    logger.info(f"Requesting GenerativeModel: {model_name}")
    return client.GenerativeModel(model_name)

@lru_cache(maxsize=2)
def get_flash_model(model_name: str = 'gemini-1.5-flash-latest') -> genai.GenerativeModel:
    client = get_google_ai_client()
    if not client: return None
    logger.info(f"Requesting Flash Model: {model_name}")
    return client.GenerativeModel(model_name)


@lru_cache(maxsize=1)
def get_pinecone_index() -> pinecone.Index:
    try:
        api_key = os.getenv("PINECONE_API_KEY")
        index_name = os.getenv("PINECONE_INDEX_NAME")
        if not api_key or not index_name:
            raise ValueError("PINECONE_API_KEY or PINECONE_INDEX_NAME not set.")
        
        pc = pinecone.Pinecone(api_key=api_key)
        index = pc.Index(index_name)
        logger.info(f"Pinecone index '{index_name}' connected successfully.")
        return index
    except Exception as e:
        logger.error(f"Failed to connect to Pinecone index: {e}")
        return None

@lru_cache(maxsize=1)
def get_neo4j_driver() -> neo4j.Driver:
    try:
        uri = os.getenv("NEO4J_URI")
        user = os.getenv("NEO4J_USERNAME", "neo4j")
        password = os.getenv("NEO4J_PASSWORD")
        if not all([uri, user, password]):
            raise ValueError("Neo4j connection details (URI, USERNAME, PASSWORD) not set.")

        driver = neo4j.GraphDatabase.driver(uri, auth=(user, password))
        driver.verify_connectivity()
        logger.info("Neo4j driver connected successfully.")
        return driver
    except Exception as e:
        logger.error(f"Failed to create Neo4j driver: {e}")
        return None


########################################################################
### FILE: src/tools/retrievers.py
########################################################################

# FILE: src/tools/retrievers.py
# V5.4 (Definitive Serializer Fix): Added logic to handle list-based path results from Neo4j.

import logging
import time
from typing import List, Dict, Any
import neo4j
import google.generativeai as genai

from src.tools.clients import get_flash_model, get_pinecone_index, get_neo4j_driver, DEFAULT_REQUEST_OPTIONS
from src.models import ToolResult, QueryMetadata
from src.prompts import CYPHER_GENERATION_PROMPT

logger = logging.getLogger(__name__)

class Timer:
    def __init__(self, name): self.name = name
    def __enter__(self): self.start = time.perf_counter(); return self
    def __exit__(self, *args): self.end = time.perf_counter(); logger.info(f"[TIMER] {self.name} took {(self.end - self.start) * 1000:.2f} ms")

def _format_pinecone_results(matches: List[dict]) -> List[str]:
    contents = []
    MAX_PAGES_TO_SHOW = 4
    for match in matches:
        metadata = match.get('metadata', {})
        text = metadata.get('text', 'No content available.')
        doc_id = metadata.get('doc_id', 'Unknown Document')
        page_numbers_raw = metadata.get('page_numbers', [])
        url = metadata.get('source_pdf_url', '#')
        page_str, link_url = "N/A", url
        if page_numbers_raw:
            unique_pages = sorted(list(set(page_numbers_raw)))
            if len(unique_pages) > MAX_PAGES_TO_SHOW:
                page_str = ", ".join(map(str, unique_pages[:MAX_PAGES_TO_SHOW])) + ", ..."
            else:
                page_str = ", ".join(map(str, unique_pages))
            if unique_pages:
                link_url = f"{url}#page={unique_pages[0]}"
        citation_text = f"{doc_id} (Page {page_str})"
        citation = f'<a href="{link_url}" target="_blank">{citation_text}</a>'
        contents.append(f"Evidence from document: {text}\nCitation: {citation}")
    return contents

def _serialize_neo4j_path(record: Dict[str, Any]) -> str:
    path_data = record.get("p")
    rel_props = record.get("rel_props")

    if not path_data:
        return ""

    subject_name, predicate_type, object_name = None, None, None

    try:
        if isinstance(path_data, neo4j.graph.Path):
            subject_name = path_data.start_node.get('name')
            predicate_type = path_data.relationships[0].type
            object_name = path_data.end_node.get('name')
        
        elif isinstance(path_data, list) and len(path_data) == 3 and all(isinstance(i, (dict, str)) for i in path_data):
            subject_name = path_data[0].get('name')
            predicate_type = path_data[1]
            object_name = path_data[2].get('name')
        
        elif isinstance(path_data, dict) and 'start' in path_data and 'end' in path_data:
            subject_name = path_data['start'].get('properties', {}).get('name')
            predicate_type = path_data.get('segments', [{}])[0].get('relationship', {}).get('type')
            object_name = path_data['end'].get('properties', {}).get('name')

        if not all([subject_name, predicate_type, object_name]):
            logger.warning(f"Could not fully parse Neo4j path data with any known method: {path_data}")
            return ""

        predicate_str = predicate_type.replace('_', ' ').lower()
        text_representation = f"{subject_name} {predicate_str} {object_name}."
        
        citation_text, link_url = "Knowledge Graph", "#"
        if isinstance(rel_props, dict):
            doc_id = rel_props.get('doc_id')
            url = rel_props.get('source_pdf_url')
            page_num_str = rel_props.get('page_numbers', 'N/A')
            if doc_id:
                citation_text = f"{doc_id} (Page {page_num_str})"
            if url:
                first_page = page_num_str.split(',')[0].strip()
                link_url = f"{url}#page={first_page}" if first_page.isdigit() else url
        
        citation = f'<a href="{link_url}" target="_blank">{citation_text}</a>'
        return f"Evidence from graph: {text_representation}\nCitation: {citation}"

    except (AttributeError, IndexError, TypeError) as e:
        logger.error(f"Failed to serialize Neo4j record due to parsing error: {e}", exc_info=True)
        return ""

def _vector_search_tool(query: str, namespace: str, tool_name: str, top_k: int = 7) -> ToolResult:
    with Timer(f"Tool: {tool_name}"):
        pinecone_index = get_pinecone_index()
        if not pinecone_index or not genai:
            return ToolResult(tool_name=tool_name, success=False, content="Vector search client not available.")
        try:
            with Timer("Embedding Generation"):
                query_embedding = genai.embed_content(model='models/text-embedding-004', content=query, task_type="retrieval_query")
            with Timer("Pinecone Query"):
                response = pinecone_index.query(namespace=namespace, vector=query_embedding['embedding'], top_k=top_k, include_metadata=True)
            if not response.get('matches'):
                return ToolResult(tool_name=tool_name, success=True, content="")
            content_list = _format_pinecone_results(response['matches'])
            return ToolResult(tool_name=tool_name, success=True, content="\n---\n".join(content_list))
        except Exception as e:
            logger.error(f"Error in vector search tool '{tool_name}': {e}", exc_info=True)
            return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred: {e}")

def retrieve_clinical_data(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-clinical", "retrieve_clinical_data")

def retrieve_summary_data(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-summary", "retrieve_summary_data")

def retrieve_general_text(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-text", "retrieve_general_text")

def query_knowledge_graph(query: str, query_meta: QueryMetadata) -> ToolResult:
    with Timer("Tool: query_knowledge_graph"):
        tool_name = "query_knowledge_graph"
        if not query_meta.question_is_graph_suitable:
            return ToolResult(tool_name=tool_name, success=True, content="")
        llm, driver = get_flash_model(), get_neo4j_driver()
        if not llm or not driver:
            return ToolResult(tool_name=tool_name, success=False, content="LLM or Neo4j client not available.")
        try:
            with Timer("KG Schema Fetch"):
                with driver.session() as session:
                    schema_data = session.run("CALL db.schema.visualization()").data()
            schema_str = f"Node labels: {schema_data[0]['nodes']}\nRelationships: {schema_data[0]['relationships']}"
            prompt = CYPHER_GENERATION_PROMPT.format(schema=schema_str, question=query)
            with Timer("Cypher Generation LLM Call"):
                response = llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            cypher_query = response.text.strip().replace("```cypher", "").replace("```", "")
            if "none" in cypher_query.lower() or "match" not in cypher_query.lower():
                return ToolResult(tool_name=tool_name, success=True, content="")
            logger.info(f"Generated Cypher: {cypher_query}")
        except Exception as e:
            return ToolResult(tool_name=tool_name, success=False, content=f"Cypher generation failed: {e}")
        try:
            with Timer("KG Query Execution"):
                with driver.session() as session:
                    records = session.run(cypher_query).data()
            if not records:
                return ToolResult(tool_name=tool_name, success=True, content="")
            results = [_serialize_neo4j_path(record) for record in records if record.get("p")]
            results = [res for res in results if res]
            return ToolResult(tool_name=tool_name, success=True, content="\n".join(results))
        except Exception as e:
            return ToolResult(tool_name=tool_name, success=False, content=f"Cypher execution failed: {e}")


########################################################################
### FILE: streamlit_app.py
########################################################################

# FILE: streamlit_app.py
# V3.1 (Definitive Fix): Corrected the agent call to be synchronous, fixing the asyncio ValueError.

import streamlit as st
import logging
import os
from dotenv import load_dotenv

# --- CRITICAL: Load environment variables at the very top ---
load_dotenv()

# --- Page and Logging Configuration ---
st.set_page_config(
    page_title="Persona RAG Chatbot",
    page_icon="🧠",
    layout="wide",
    initial_sidebar_state="expanded",
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - [%(name)s:%(lineno)d] - %(message)s'
)
logging.getLogger('streamlit').setLevel(logging.WARNING)
logging.getLogger('watchdog').setLevel(logging.WARNING)
logging.getLogger('PIL').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)

# Now import project modules
from src.agent import Agent
from src.tools.clients import get_google_ai_client # Used for a pre-flight check


# --- Session State Initialization ---
if "agent" not in st.session_state:
    st.session_state.agent = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_persona" not in st.session_state:
    st.session_state.current_persona = "automatic"

# --- Helper Functions (unchanged) ---
@st.cache_resource
def initialize_agent():
    if not get_google_ai_client():
        st.error("Google API Key is not configured. Please set the GOOGLE_API_KEY in your .env file.", icon="🚨")
        return None
    try:
        agent = Agent()
        logger.info("Unified agent initialized successfully and cached for the session.")
        return agent
    except Exception as e:
        st.error(f"Fatal error during agent initialization: {e}", icon="🚨")
        logger.error(f"Agent initialization failed: {e}", exc_info=True)
        return None

def reset_chat(persona_name: str):
    display_name = "Automatic" if persona_name == "Automatic (Recommended)" else persona_name
    st.session_state.messages = [
        {"role": "assistant", "content": f"Hi! I'm now acting in **{display_name}** mode. How can I help you?"}
    ]

# --- Sidebar (unchanged) ---
with st.sidebar:
    st.header("🤖 Persona RAG Chatbot")
    st.markdown("Select a persona to tailor my retrieval strategy and answers to your specific role.")
    persona_options = {
        'Automatic (Recommended)': 'automatic',
        'Clinical Analyst': 'clinical_analyst',
        'Health Economist': 'health_economist',
        'Regulatory Specialist': 'regulatory_specialist',
    }
    current_display_name = [k for k, v in persona_options.items() if v == st.session_state.current_persona][0]
    selected_persona_name = st.radio(
        "**Choose your Mode:**",
        options=persona_options.keys(),
        index=list(persona_options.keys()).index(current_display_name),
        key="persona_selector"
    )
    selected_persona_key = persona_options[selected_persona_name]
    if selected_persona_key != st.session_state.current_persona:
        st.session_state.current_persona = selected_persona_key
        reset_chat(selected_persona_name)
        st.rerun()
    st.divider()
    if st.button("🔄 Clear Chat History", use_container_width=True):
        reset_chat(selected_persona_name)
        st.rerun()
    st.divider()
    st.header("🧪 Evaluation Questions")
    st.markdown("Use these questions to test the agent's capabilities with the 2025 data.")

    with st.expander("🎯 Fact Retrieval (2025 Data)", expanded=True):
        questions = {
            "Sponsor Lookup (July 2025)": "What company is the sponsor for Abaloparatide?",
            "Indication Lookup (March 2025)": "What is Amivantamab used to treat?",
            "Trade Name Lookup (May 2025)": "What is the trade name for Dostarlimab?",
            "Dosage Form (July 2025)": "What is the dosage form of Apomorphine?",
        }
        for name, q in questions.items():
            if st.button(f"{name}: {q}", key=q, use_container_width=True):
                st.session_state.run_prompt = q

    with st.expander("⚖️ Multi-Step Reasoning (2025 Data)"):
        questions = {
            "Intersection Query": "Which sponsors made submissions in both the March 2025 and July 2025 meetings?",
            "Comparative Query": "Compare the submission purposes for Acalabrutinib and Alectinib in the 2025 meetings.",
            "Multi-Hop Query": "What is the indication for the drug whose trade name is Cabometyx?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q
    
    with st.expander("📋 Summarization (2025 Data)"):
        questions = {
            "Summarize a Meeting": "Provide a summary of the key submissions from the May 2025 PBAC meeting.",
            "Summarize by Theme": "Summarize all submissions related to oncology in the March 2025 documents.",
            "Synthesize High-Level Goal": "Based on the agendas, what appears to be the main focus of the PBAC's work in 2025?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q

    with st.expander("🤔 Challenging / Ambiguous Questions"):
        questions = {
            "Test Data Boundaries": "What was the final PBAC decision on Ribociclib from the July 2025 meeting?",
            "Test Fallback Logic (No Price)": "What is the price of Tirzepatide?",
            "Out of Scope (External Knowledge)": "What are the latest FDA guidelines on biosimilars?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q

# --- Main Chat Interface (unchanged) ---
st.title("Persona-Aware RAG Agent")
if st.session_state.current_persona == "automatic":
    st.caption("Currently in **Automatic Mode** (selects best persona per query)")
else:
    st.caption(f"Currently acting as: **{selected_persona_name}**")
with st.container(border=True):
    st.info("""
    **Welcome! This is an advanced chatbot designed to answer questions about pharmaceutical and regulatory documents.** 
    
    Its unique feature is the ability to tailor its information retrieval strategy based on the professional role you select in the sidebar.

    **How to use this demo:**
    1.  **Choose Your Mode:** Select 'Automatic' (Recommended) or a specific persona from the sidebar.
    2.  **Ask a Question:** Use the pre-defined 'Evaluation Questions' or type your own question in the chat box below.
    """)
    st.markdown("<p style='text-align: center; color: grey;'>A not-for-profit demonstration project by <b>EVIL_MIT</b></p>", unsafe_allow_html=True)

if st.session_state.agent is None:
    st.session_state.agent = initialize_agent()
    if not st.session_state.messages:
        reset_chat(selected_persona_name)

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"], unsafe_allow_html=True)

prompt_from_button = st.session_state.pop("run_prompt", None)
prompt_from_input = st.chat_input("Ask your question...")
prompt = prompt_from_button or prompt_from_input

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.agent:
            spinner_text = "Thinking..."
            if st.session_state.current_persona != "automatic":
                 spinner_text = f"Thinking as a {selected_persona_name}..."
            
            with st.spinner(spinner_text):
                history_for_rewrite = [f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:-1]]
                
                # --- START OF DEFINITIVE FIX ---
                # REMOVED asyncio.run() as agent.run is a synchronous function.
                # This was the cause of the ValueError crash.
                response = st.session_state.agent.run(
                    prompt, 
                    persona=st.session_state.current_persona,
                    chat_history=history_for_rewrite
                )
                # --- END OF DEFINITIVE FIX ---
                
                st.markdown(response, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            st.error("Agent is not available due to an initialization error. Please check the terminal logs.")
            st.stop()
    
    if prompt_from_button:
        st.rerun()