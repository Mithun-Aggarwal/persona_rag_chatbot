========================================================================
  Combined Python Files from Git Repository: /home/mit/persona_rag_chatbot
  Generated on: Mon 28 Jul 2025 17:22:59 AEST
========================================================================



########################################################################
### FILE: src/agent.py
########################################################################

# src/agent.py

import logging
from typing import List
import google.generativeai as genai

from src import tools, prompts, retrievers
from src.models import ContextItem
from src.routing.persona_router import PersonaRouter

logger = logging.getLogger(__name__)

class MainAgent:
    def __init__(self, persona: str):
        self.persona = persona
        self.router = PersonaRouter()
        self.llm = genai.GenerativeModel('gemini-1.5-flash-latest')

    def _generate_cypher(self, query: str) -> str | None:
        logger.info("Attempting to generate Cypher query...")
        try:
            # This now correctly calls the new function in tools.py
            live_schema = tools.get_neo4j_schema()
            if "Error:" in live_schema:
                logger.error(f"Could not generate Cypher, failed to get schema: {live_schema}")
                return None
        except Exception as e:
            logger.error(f"Could not generate Cypher, failed to get schema: {e}")
            return None

        prompt = prompts.CYPHER_GENERATION_PROMPT.format(
            schema=live_schema,
            question=query
        )
        try:
            response = self.llm.generate_content(prompt)
            cypher_query = response.text.strip().replace("```cypher", "").replace("```", "") # Clean up markdown

            if "NONE" in cypher_query.upper() or "MATCH" not in cypher_query.upper():
                logger.warning("LLM determined question is not suitable for graph query.")
                return None

            logger.info(f"Successfully generated Cypher: {cypher_query}")
            return cypher_query
        except Exception as e:
            logger.error(f"Error during Cypher generation: {e}")
            return None

    def _format_context_with_citations(self, context: List[ContextItem]) -> str:
        logger.info("Formatting context and creating citation markers.")
        context_str = ""
        source_map = {}
        ref_counter = 1

        for item in context:
            source = item.source
            if source.type in ["graph_path", "graph_record"]:
                # Use a cleaner key for graph results
                source_key = f"Graph DB Record (Query: '{source.query[:60]}...')"
            elif source.document_id and source.page_numbers:
                page_str = ", ".join(map(str, sorted(list(set(source.page_numbers)))))
                source_key = f"Document: {source.document_id}, Page(s): {page_str}"
            elif source.document_id:
                source_key = f"Document: {source.document_id}"
            else:
                continue # Skip items with no identifiable source

            if source_key not in source_map:
                source_map[source_key] = f"[{ref_counter}]"
                ref_counter += 1

            citation_marker = source_map[source_key]

            context_str += f"--- Context Source ---\n"
            context_str += f"Source Citation: {citation_marker}\n"
            context_str += f"Content: {item.content}\n\n"

        # Only add reference list if there are sources
        if source_map:
            reference_list = "\n".join([f"{num} {key}" for key, num in source_map.items()])
            context_str += f"\n--- Available References ---\n{reference_list}\n"
        return context_str

    def run(self, query: str) -> str:
        logger.info(f"\U0001F7E2 Agent starting run for persona '{self.persona}' with query: '{query}'")

        retrieval_plan = self.router.get_retrieval_plan(self.persona)
        retrieved_context: List[ContextItem] = []

        # Retrieve from vector stores first
        unique_content = set()
        for config in retrieval_plan.namespaces:
            logger.info(f"\U0001F50D Executing vector search on namespace: {config.namespace} with top_k: {config.top_k}")
            # This now correctly calls the function in retrievers.py
            pinecone_results = retrievers.vector_search(query=query, namespace=config.namespace, top_k=config.top_k)
            for res in pinecone_results:
                if res.content not in unique_content:
                    retrieved_context.append(res)
                    unique_content.add(res.content)
        
        # Then, attempt to retrieve from graph store
        cypher_query = self._generate_cypher(query)
        if cypher_query:
            logger.info(f"\U0001F578️ Executing graph search with Cypher: {cypher_query}")
            # This now correctly calls the function in retrievers.py
            graph_results = retrievers.graph_search(cypher_query=cypher_query)
            retrieved_context.extend(graph_results)

        if not retrieved_context:
            logger.warning("No information found from any retriever.")
            return "Based on the information available to me, I could not find a sufficient answer to your question."

        logger.info(f"Retrieved {len(retrieved_context)} total context items. Synthesizing answer.")
        formatted_context = self._format_context_with_citations(retrieved_context)
        final_prompt = prompts.SYNTHESIS_PROMPT.format(question=query, context_str=formatted_context)

        try:
            final_answer = self.llm.generate_content(final_prompt).text
        except Exception as e:
            logger.error(f"Error during final synthesis: {e}")
            return "I apologize, but I encountered an error while formulating a response."

        logger.info("Agent run completed successfully.")
        return final_answer


########################################################################
### FILE: src/common_utils.py
########################################################################

# src/common_utils.py

"""
Common utility functions shared across the application.
"""

from pathlib import Path

def get_project_root() -> Path:
    """
    Returns the absolute path to the project root directory.

    This is a robust way to reference files (like configs) from anywhere
    within the project, regardless of where the script is run from.
    """
    # We assume this file is in 'src/'. The project root is one level up.
    return Path(__file__).parent.parent.resolve()


########################################################################
### FILE: src/prompts.py
########################################################################

# src/prompts.py

"""
Production-grade prompts for a robust RAG agent.
"""

# REFACTORED: The Cypher generation prompt is now more robust and correctly
# uses the {schema} placeholder to accept dynamic schemas.
CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema.

**Live Graph Schema:**
{schema}

**Instructions:**
1.  Analyze the schema to understand the available node labels, properties, and relationships.
2.  Construct a Cypher query that retrieves relevant information to answer the question.
3.  The query MUST be read-only (i.e., use `MATCH` and `RETURN`). Do not use `CREATE`, `MERGE`, `SET`, or `DELETE`.
4.  If possible, return a path `p` using `RETURN p` to show the full context of the connection.
5.  If the question cannot be answered with the given schema, or if it's not a question for a graph database, you MUST return the single word: `NONE`.
6.  Output ONLY the Cypher query or the word `NONE`. Do not add explanations, greetings, or markdown formatting like ```cypher.

**Example Question:** "What company sponsors Abaloparatide?"
**Example Valid Query:** MATCH p=(drug:Drug {{name: 'Abaloparatide'}})-[:SPONSORED_BY]->(sponsor:Sponsor) RETURN p

**Task:**
Generate a Cypher query for the question below.

**Question:** {question}
"""

SYNTHESIS_PROMPT = """
You are an AI assistant, an 'Inter-Expert Interpreter'. Your role is to deliver a comprehensive, accurate, and perfectly cited answer using ONLY the provided context.

**User's Question:** "{question}"

*** YOUR INSTRUCTIONS ***
1.  **Synthesize a Complete Answer**: Read all the provided context blocks and synthesize a single, cohesive answer to the user's question.
2.  **Cite Your Sources**: As you write, you MUST cite every fact. To do this, find the `Source Citation` for the context block you are using and place it directly after the fact it supports.
3.  **Create a Reference List**: After your main answer, create a "References" section. List each unique source you cited in a numbered list.
4.  **Be Honest**: If the context is insufficient to answer the question, you must state that clearly. Do not invent information.

---
**CONTEXT:**
{context_str}
---

**ANSWER:**
"""


########################################################################
### FILE: src/tools.py
########################################################################

# src/tools.py

"""
Low-level tools and client initializers.

This module is responsible for setting up and providing access to external
services like databases and APIs (Pinecone, Neo4j, Google AI). It reads
credentials from environment variables. It has NO dependencies on other
modules in this project to prevent circular imports.
"""

import os
import logging
from functools import lru_cache

import pinecone
import neo4j
import google.generativeai as genai

logger = logging.getLogger(__name__)

# --- Client Initializers (Cached for Performance) ---

@lru_cache(maxsize=1)
def get_google_ai_client() -> genai:
    """Initializes and returns the Google AI client."""
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")
        genai.configure(api_key=api_key)
        logger.info("Google AI client configured successfully.")
        return genai
    except Exception as e:
        logger.error(f"Failed to configure Google AI client: {e}")
        return None

@lru_cache(maxsize=1)
def get_pinecone_index() -> pinecone.Index:
    """Initializes and returns the Pinecone index handle."""
    try:
        api_key = os.getenv("PINECONE_API_KEY")
        index_name = os.getenv("PINECONE_INDEX_NAME")
        if not api_key or not index_name:
            raise ValueError("PINECONE_API_KEY or PINECONE_INDEX_NAME not set.")

        pc = pinecone.Pinecone(api_key=api_key)
        index = pc.Index(index_name)
        logger.info(f"Pinecone index '{index_name}' connected successfully.")
        return index
    except Exception as e:
        logger.error(f"Failed to connect to Pinecone index: {e}")
        return None

@lru_cache(maxsize=1)
def get_neo4j_driver() -> neo4j.GraphDatabase.driver:
    """Initializes and returns the Neo4j driver."""
    try:
        uri = os.getenv("NEO4J_URI")
        user = os.getenv("NEO4J_USERNAME")
        password = os.getenv("NEO4J_PASSWORD")
        if not all([uri, user, password]):
            raise ValueError("Neo4j connection details (URI, USERNAME, PASSWORD) not set.")

        driver = neo4j.GraphDatabase.driver(uri, auth=(user, password))
        driver.verify_connectivity()
        logger.info("Neo4j driver connected successfully.")
        return driver
    except Exception as e:
        logger.error(f"Failed to create Neo4j driver: {e}")
        return None

# --- Graph Schema Utility ---

@lru_cache(maxsize=1)
def get_neo4j_schema() -> str:
    """
    Retrieves the schema from Neo4j for use in LLM prompts.
    This includes node labels, properties, and relationship types.
    """
    driver = get_neo4j_driver()
    if not driver:
        return "Error: Neo4j driver not available."

    schema_query = """
    CALL db.schema.visualization()
    """
    try:
        with driver.session() as session:
            result = session.run(schema_query)
            # The result from schema visualization is complex; we need to simplify it.
            # A simpler approach for LLM prompts is often a text-based description.
            
            # Get node labels and properties
            node_schema_query = "CALL db.labels() YIELD label CALL db.propertyKeys() YIELD propertyKey WITH label, collect(propertyKey) AS properties RETURN label, properties"
            node_props = session.run(node_schema_query).data()
            
            # Get relationship schema
            rel_schema_query = """
            MATCH (n)-[r]->(m)
            RETURN DISTINCT type(r) AS rel_type, labels(n) AS from_labels, labels(m) AS to_labels
            LIMIT 50
            """
            rel_types = session.run(rel_schema_query).data()

            schema_str = "Graph Schema:\n"
            schema_str += "Node Labels and Properties:\n"
            for item in node_props:
                 # Check if the node label exists and has properties
                if item['label'] and item['properties']:
                    # This is a simplification; a more robust version would check properties per label
                    # For now, we list all possible properties under each label for prompt context
                    # A better query: "MATCH (n:{label}) UNWIND keys(n) as key RETURN distinct key"
                    schema_str += f"- Node '{item['label']}'\n"
            
            schema_str += "\nRelationship Types and Connections:\n"
            for item in rel_types:
                from_node = item['from_labels'][0] if item['from_labels'] else "Node"
                to_node = item['to_labels'][0] if item['to_labels'] else "Node"
                schema_str += f"- ({from_node})-[:{item['rel_type']}]->({to_node})\n"

            if not node_props and not rel_types:
                return "Schema not found or database is empty."

            return schema_str

    except Exception as e:
        logger.error(f"Failed to retrieve Neo4j schema: {e}")
        return f"Error retrieving schema: {e}"


########################################################################
### FILE: streamlit_app.py
########################################################################

# streamlit_app.py

import streamlit as st
import logging
from pathlib import Path
from dotenv import load_dotenv

# --- CRITICAL: Load environment variables at the very top ---
load_dotenv()

# Now import project modules
from src.agent import MainAgent

# --- Page Configuration ---
st.set_page_config(
    page_title="Persona RAG Chatbot",
    page_icon="🤖",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Logging ---
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - [%(name)s:%(lineno)d] - %(message)s'
)

# --- Session State Initialization ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "How can I help you today as a Clinical Analyst?"}]
if "agent" not in st.session_state:
    st.session_state.agent = None
if "current_persona" not in st.session_state:
    st.session_state.current_persona = "clinical_analyst" # Use the key from the YAML file


# --- Helper function to re-initialize agent when persona changes ---
def initialize_agent(persona_key: str):
    persona_name = persona_key.replace('_', ' ').title()
    try:
        st.session_state.agent = MainAgent(persona=persona_key)
        st.session_state.current_persona = persona_key
        st.toast(f"Agent activated for '{persona_name}' persona.", icon="🧠")
        logger.info(f"Agent initialized for persona: {persona_key}")
    except Exception as e:
        st.error(f"Failed to initialize agent for '{persona_name}': {e}", icon="🚨")
        logger.error(f"Agent initialization failed for {persona_name}: {e}")
        st.session_state.agent = None

# --- Sidebar ---
with st.sidebar:
    st.header("🤖 Persona RAG Chatbot")

    persona_options = {
        'Clinical Analyst': 'clinical_analyst',
        'Health Economist': 'health_economist',
        'Regulatory Specialist': 'regulatory_specialist',
    }
    
    # Get the display name from the key stored in session state
    persona_display_name = [k for k, v in persona_options.items() if v == st.session_state.current_persona][0]
    
    selected_persona_name = st.radio(
        "**Choose your Persona:**",
        options=persona_options.keys(),
        index=list(persona_options.keys()).index(persona_display_name)
    )

    selected_persona_key = persona_options[selected_persona_name]

    # If persona has changed, re-initialize the agent
    if selected_persona_key != st.session_state.current_persona:
        initialize_agent(selected_persona_key)
        # Reset chat for the new persona
        st.session_state.messages = [{"role": "assistant", "content": f"How can I help you today as a {selected_persona_name}?"}]
        st.rerun()

    st.divider()
    st.markdown("### 🧪 Test Questions")

    test_questions = [
        "What company sponsors Abaloparatide?",
        "Tell me about the submission for non-small cell lung cancer.",
        "Which condition does Abaloparatide treat?",
        "Was the sponsor Janssen involved in any 2025 submissions?",
        "What is the cost-effectiveness of drugs for lung cancer?", # Good for Health Economist
    ]
    
    with st.expander("Example Questions"):
        for q in test_questions:
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q
                st.session_state.chat_input = "" # Clear input box

# --- Main Section ---
st.title(f"Persona: {selected_persona_name}")

# Initialize agent on first run
if not st.session_state.agent:
    initialize_agent(st.session_state.current_persona)

# --- Chat History Display ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Handle User Input ---
prompt_from_button = st.session_state.pop("run_prompt", None)
prompt_from_input = st.chat_input("Your question...", key="chat_input_box")
prompt = prompt_from_button or prompt_from_input

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.agent:
            with st.spinner(f"Thinking as a {selected_persona_name}..."):
                try:
                    response = st.session_state.agent.run(prompt)
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    error_message = f"An unexpected error occurred: {e}"
                    st.error(error_message)
                    logger.error(f"Error during agent run: {e}", exc_info=True)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
        else:
            st.error("The agent is not initialized. Please check the logs for errors.")

    # Rerun to clear the input box if a button was used, or to reflect the new state
    st.rerun()