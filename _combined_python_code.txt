========================================================================
  Combined Python Files from Git Repository: /home/mit/persona_rag_chatbot
  Generated on: Mon 28 Jul 2025 19:03:26 AEST
========================================================================



########################################################################
### FILE: src/__init__.py
########################################################################




########################################################################
### FILE: src/agent.py
########################################################################

# FILE: src/agent.py
# V2.5 (Definitive Fix): Simplifies the context formatting logic completely.
# It now directly joins the clean, pre-formatted output from the tools.

import logging
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List

from src.tools.clients import get_google_ai_client
from src.models import ToolResult, QueryMetadata, ToolPlanItem
from src.planner.query_classifier import QueryClassifier
from src.planner.tool_planner import ToolPlanner
from src.router.tool_router import ToolRouter
from src.fallback import should_trigger_fallback, render_fallback_message
from src.prompts import SYNTHESIS_PROMPT

logger = logging.getLogger(__name__)
LOG_PATH = Path("trace_logs.jsonl")

class Timer:
    def __enter__(self):
        self.start = time.perf_counter()
        self.duration = -1.0
        return self
    def __exit__(self, *args):
        self.end = time.perf_counter()
        self.duration = self.end - self.start

def log_trace(query: str, persona: str, query_meta: QueryMetadata, tool_plan: List[ToolPlanItem], tool_results: List[ToolResult], final_answer: str, total_latency_sec: float):
    trace_record = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "query": query, "persona": persona,
        "intent": query_meta.intent if query_meta else "classification_failed",
        "graph_suitable": query_meta.question_is_graph_suitable if query_meta else "unknown",
        "tool_plan": [t.model_dump() for t in tool_plan],
        "tool_results": [r.model_dump() for r in tool_results],
        "final_answer_preview": final_answer[:200] + "..." if final_answer else "N/A",
        "total_latency_sec": round(total_latency_sec, 3)
    }
    try:
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(trace_record) + "\n")
        logger.info(f"Trace logged successfully to {LOG_PATH.resolve()}")
    except Exception as e:
        logger.error(f"Failed to write trace log: {e}", exc_info=True)

class Agent:
    def __init__(self, confidence_threshold: float = 0.85):
        self.classifier = QueryClassifier()
        self.planner = ToolPlanner(coverage_threshold=confidence_threshold)
        self.router = ToolRouter()
        genai_client = get_google_ai_client()
        self.llm = genai_client.GenerativeModel('gemini-1.5-pro-latest') if genai_client else None
        if not self.llm: logger.error("FATAL: Gemini client could not be initialized.")

    # --- START OF DEFINITIVE FIX ---
    # REMOVED the old _format_context_for_synthesis function entirely.
    # --- END OF DEFINITIVE FIX ---

    def run(self, query: str, persona: str) -> str:
        query_meta, tool_plan, results, final_answer = None, [], [], ""
        timer = Timer()
        try:
            with timer:
                if not self.llm: return "Error: AI model not available."
                query_meta = self.classifier.classify(query)
                if not query_meta: return "I'm sorry, I had trouble understanding your question."

                tool_plan = self.planner.plan(query_meta, persona)
                if not tool_plan: return "I'm sorry, I don't have a configured strategy for this persona."

                for plan_item in tool_plan:
                    results.append(self.router.execute_tool(plan_item.tool_name, query, query_meta))

                if should_trigger_fallback(results): return render_fallback_message(query)

                # --- START OF DEFINITIVE FIX ---
                # This is the new, simplified context assembly logic.
                # It filters for successful tools with real content and joins them.
                successful_content = [
                    res.content for res in results 
                    if res.success and res.content and "no relevant information" not in res.content.lower()
                ]
                formatted_context = "\n---\n".join(successful_content)
                # --- END OF DEFINITIVE FIX ---

                if not formatted_context:
                    return "I searched for information but could not find any relevant details."
                
                final_prompt = SYNTHESIS_PROMPT.format(question=query, context_str=formatted_context)
                final_answer = self.llm.generate_content(final_prompt).text
                return final_answer
        except Exception as e:
            logger.error(f"An unexpected error occurred during agent run: {e}", exc_info=True)
            final_answer = f"I encountered a critical error. Please check the system logs."
            return final_answer
        finally:
            log_trace(query, persona, query_meta, tool_plan, results, final_answer, timer.duration)


########################################################################
### FILE: src/fallback.py
########################################################################

# FILE: src/fallback.py
# Phase 3.1: FallbackLayer — graceful UX when all tools return empty or fail

import logging
from typing import List
from src.models import ToolResult

logger = logging.getLogger(__name__)

FALLBACK_QUESTIONS = [
    "Would you like to compare two drugs instead?",
    "Can I help you find a sponsor for a specific medicine?",
    "Do you want to search the original PDF documents directly?"
]


def should_trigger_fallback(results: List[ToolResult]) -> bool:
    """
    Returns True if all tools failed or produced no meaningful content.
    """
    if not results:
        logger.info("Fallback triggered: no tool results returned.")
        return True

    empty_or_failed = [r for r in results if not r.success or not r.content or len(r.content.strip()) < 5]
    if len(empty_or_failed) == len(results):
        logger.info("Fallback triggered: all tools failed or content was empty.")
        return True

    return False


def render_fallback_message(query: str) -> str:
    """
    Returns a polite fallback message with suggested next questions.
    """
    msg = f"""
I'm sorry — based on the current documents and tools, I couldn't find sufficient information to answer your question:

"{query}"

However, here are some things you can try next:

"""
    for i, q in enumerate(FALLBACK_QUESTIONS, start=1):
        msg += f"{i}. {q}\n"

    msg += "\nYou can also try rephrasing your question for better results."
    return msg.strip()



########################################################################
### FILE: src/models.py
########################################################################

from pydantic import BaseModel
from typing import List, Optional, Literal


QueryIntent = Literal[
    "specific_fact_lookup",
    "simple_summary",
    "comparative_analysis",
    "general_qa",
    "unknown"
]


class QueryMetadata(BaseModel):
    intent: QueryIntent
    keywords: List[str]
    question_is_graph_suitable: bool


class ToolPlanItem(BaseModel):
    tool_name: str
    estimated_coverage: float


class ToolResult(BaseModel):
    tool_name: str
    success: bool
    content: str
    estimated_coverage: float = 0.0


class ContextItem(BaseModel):
    content: str
    source: Optional[dict] = None


class Source(BaseModel):
    type: str
    document_id: Optional[str] = None
    page_numbers: Optional[List[int]] = None
    source_url: Optional[str] = None
    retrieval_score: Optional[float] = None
    query: Optional[str] = None

class NamespaceConfig(BaseModel):
    namespace: str
    weight: float
    top_k: int

class RetrievalPlan(BaseModel):
    namespaces: List[NamespaceConfig] = []


########################################################################
### FILE: src/planner/__init__.py
########################################################################




########################################################################
### FILE: src/planner/query_classifier.py
########################################################################

# FILE: src/planner/query_classifier.py
# Phase 1.1: QueryClassifier — interprets user query using Gemini and returns structured metadata

import logging
import google.generativeai as genai
from typing import Optional
from src.models import QueryMetadata
from src.prompts import QUERY_CLASSIFICATION_PROMPT

logger = logging.getLogger(__name__)

class QueryClassifier:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-flash-latest')

    def classify(self, query: str) -> Optional[QueryMetadata]:
        """Classifies a user query into intent, keywords, and graph suitability."""
        logger.info(f"Classifying query: {query}")
        try:
            prompt = QUERY_CLASSIFICATION_PROMPT + f"\n\nUser Query: {query}"
            response = self.model.generate_content(prompt)
            parsed_json = response.text.strip()
            metadata = QueryMetadata.model_validate_json(parsed_json)
            logger.info(f"Classification result: {metadata.model_dump_json(indent=2)}")
            return metadata
        except Exception as e:
            logger.error(f"Query classification failed: {e}")
            return None

# Unit test: test with canned queries
if __name__ == "__main__":
    qc = QueryClassifier()
    test_queries = [
        "What company sponsors Abaloparatide?",
        "Compare the clinical outcomes of Drug A vs Drug B",
        "Tell me about submissions for lung cancer.",
        "What is the patient population for the March 2025 submission?"
    ]
    for q in test_queries:
        print(qc.classify(q))



########################################################################
### FILE: src/planner/tool_planner.py
########################################################################

# FILE: src/planner/tool_planner.py
# V2.0: Persona-Aware Tool Planner
import logging
import yaml
from pathlib import Path
from typing import List, Dict

from src.models import QueryMetadata, ToolPlanItem

logger = logging.getLogger(__name__)

# Base scores for tools based on query intent. A higher score is better.
# These tool names MUST match the names in `persona_tool_map.yml` and the tool router.
INTENT_TOOL_SCORES = {
    # If the user wants a specific fact...
    "specific_fact_lookup": {
        "query_knowledge_graph": 0.9,
        "retrieve_clinical_data": 0.7,
        "retrieve_general_text": 0.6,
        "retrieve_summary_data": 0.4,
    },
    # If the user wants a high-level summary...
    "simple_summary": {
        "retrieve_summary_data": 0.9,
        "retrieve_general_text": 0.8,
        "retrieve_clinical_data": 0.5,
        "query_knowledge_graph": 0.4,
    },
    # If the user wants to compare things...
    "comparative_analysis": {
        "retrieve_clinical_data": 0.8,
        "retrieve_summary_data": 0.8,
        "retrieve_general_text": 0.7,
        "query_knowledge_graph": 0.5,
    },
    # For general questions...
    "general_qa": {
        "retrieve_general_text": 0.9,
        "retrieve_summary_data": 0.7,
        "query_knowledge_graph": 0.6,
        "retrieve_clinical_data": 0.5,
    },
}
DEFAULT_INTENT_SCORE = 0.5
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

class ToolPlanner:
    def __init__(self, coverage_threshold: float = 0.9):
        self.coverage_threshold = coverage_threshold
        self._load_persona_map()

    def _load_persona_map(self):
        """Loads the persona-to-tool mapping from the central YAML config."""
        map_file = PROJECT_ROOT / "config" / "persona_tool_map.yml"
        try:
            with open(map_file, 'r') as f:
                self.persona_map = yaml.safe_load(f)
            logger.info(f"Successfully loaded persona-tool map from '{map_file}'.")
        except Exception as e:
            logger.error(f"FATAL: Could not load or parse persona-tool map from '{map_file}': {e}", exc_info=True)
            self.persona_map = {}

    def plan(self, query_meta: QueryMetadata, persona: str) -> List[ToolPlanItem]:
        """
        Creates a ranked tool plan by combining query intent with user persona preferences.
        """
        logger.info(f"Planning tools for intent '{query_meta.intent}' and persona '{persona}'")
        
        # 1. Get the list of preferred tools and their weights for the given persona
        persona_key = persona.lower().replace(" ", "_")
        persona_prefs = self.persona_map.get(persona_key, self.persona_map.get("default", []))

        if not persona_prefs:
            logger.warning(f"No tool preferences found for persona '{persona_key}' or default. Returning empty plan.")
            return []
            
        persona_tool_weights: Dict[str, float] = {p["tool_name"]: p["weight"] for p in persona_prefs}
        
        # 2. Get intent-based scores for tools relevant to the current query intent
        intent_scores = INTENT_TOOL_SCORES.get(query_meta.intent, {})

        # 3. Calculate a final score for each tool by multiplying persona weight and intent score
        scored_tools = []
        for tool_name, persona_weight in persona_tool_weights.items():
            intent_score = intent_scores.get(tool_name, DEFAULT_INTENT_SCORE)
            
            # The final score reflects both the persona's general preference and the tool's suitability for the task
            final_score = persona_weight * intent_score
            scored_tools.append({"name": tool_name, "score": final_score})

        # 4. Sort tools by their final score in descending order
        scored_tools.sort(key=lambda x: x["score"], reverse=True)

        # 5. Build the final plan, adding tools until the cumulative coverage threshold is met
        final_plan: List[ToolPlanItem] = []
        total_coverage = 0.0
        for tool in scored_tools:
            # We treat the score as the estimated coverage for this planning step
            estimated_coverage = round(tool["score"], 2)

            # Do not add tools with negligible contribution
            if estimated_coverage <= 0.1:
                continue

            plan_item = ToolPlanItem(tool_name=tool["name"], estimated_coverage=estimated_coverage)
            final_plan.append(plan_item)
            
            total_coverage += estimated_coverage
            if total_coverage >= self.coverage_threshold:
                logger.info(f"Coverage threshold of {self.coverage_threshold} met. Finalizing plan.")
                break
        
        logger.info(f"Generated tool plan: {[t.model_dump_json(indent=2) for t in final_plan]}")
        return final_plan


########################################################################
### FILE: src/prompts.py
########################################################################

# FILE: src/prompts.py
# V2.1: Updated Cypher prompt for case-insensitive matching.

"""
Production-grade prompts for a robust RAG agent.
"""

QUERY_CLASSIFICATION_PROMPT = """
You are an expert query analysis agent. Your task is to analyze the user's question and provide a structured JSON output with three fields: 'intent', 'keywords', and 'question_is_graph_suitable'.

1.  **'intent'**: Classify the user's goal into one of these categories:
    *   "specific_fact_lookup": For questions seeking a single, direct answer (e.g., "What company sponsors Drug X?").
    *   "simple_summary": For questions asking for a general overview (e.g., "Tell me about Drug Y.").
    *   "comparative_analysis": For questions that compare two or more items (e.g., "Compare Drug A and Drug B.").
    *   "general_qa": For all other questions.

2.  **'keywords'**: Extract the most important nouns and proper nouns from the question, such as drug names, company names, or medical conditions. Return them as a list of strings.

3.  **'question_is_graph_suitable'**: Return `true` if the question involves relationships between entities (e.g., drug-to-sponsor, drug-to-condition), which are suitable for a knowledge graph. Otherwise, return `false`.

Output ONLY the raw JSON object. Do not add explanations or markdown formatting.

Example Question: "What is the cost-effectiveness of Abaloparatide for treating osteoporosis, and who is the sponsor?"
Example JSON Output:
{
  "intent": "specific_fact_lookup",
  "keywords": ["Abaloparatide", "osteoporosis", "cost-effectiveness", "sponsor"],
  "question_is_graph_suitable": true
}
"""

# REFACTORED: The Cypher generation prompt is now more robust and correctly
# uses the {schema} placeholder to accept dynamic schemas.
CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema.

**Live Graph Schema:**
{schema}

**CRITICAL Instructions:**
1.  **Use the Normalized Property:** You MUST query against the `name_normalized` property for all `WHERE` clauses. The user's input should be converted to lowercase. For example: `WHERE a.name_normalized = 'abaloparatide'`.
2.  Construct a Cypher query that retrieves relevant information to answer the question.
3.  The query MUST be read-only (i.e., use `MATCH` and `RETURN`). Do not use `CREATE`, `MERGE`, `SET`, or `DELETE`.
4.  If possible, return a path `p` using `RETURN p` to show the full context of the connection.
5.  If the question cannot be answered with the given schema, or if it's not a question for a graph database, you MUST return the single word: `NONE`.
6.  Output ONLY the Cypher query or the word `NONE`. Do not add explanations, greetings, or markdown formatting like ```cypher.

**Example Question:** "What company sponsors Abaloparatide?"
**Example Valid Query:** MATCH p=(drug:Entity)-[:HASSPONSOR]->(sponsor:Entity) WHERE drug.name_normalized = 'abaloparatide' RETURN p

**Task:**
Generate a Cypher query for the question below.

**Question:** {question}
"""

# --- START OF FINAL FIX ---
# This new SYNTHESIS_PROMPT is much more directive and strict.
SYNTHESIS_PROMPT = """
You are a highly precise AI assistant for pharmaceutical and regulatory analysis. Your task is to answer a user's question based *only* on the provided evidence. You must follow all rules strictly.

**User's Question:** "{question}"

*** YOUR INSTRUCTIONS ***

**Rule 1: Synthesize a Factual Answer**
- Read all the provided "Evidence" blocks below.
- Formulate a single, comprehensive answer to the user's question.
- **Do not mention the tools** (e.g., "query_knowledge_graph", "retrieve_clinical_data"). The user only cares about the information, not the source tool.

**Rule 2: Cite Every Fact with Clickable Links**
- The evidence contains Markdown-formatted citations like `[Source: DOC_ID, Page: X](URL)`.
- As you write your answer, you MUST place the corresponding citation immediately after the sentence or clause it supports.
- If multiple pieces of evidence support a single sentence, include all their citations. For example: `This is a fact [Source: Doc A, Page: 1](url) [Source: Doc B, Page: 5](url).`

**Rule 3: Create a Clean Reference List**
- After your answer, create a "References" section.
- List each unique clickable Markdown citation link that you used in your answer.
- **Do NOT list tool names or any other text in the references.**

**Rule 4: Honesty is Critical**
- If the provided evidence is insufficient to answer the question, you MUST state that clearly. For example: "Based on the available documents, I could not find information about X."
- Do not invent, infer, or use outside knowledge.

**Example of a PERFECT response:**
Theramex Australia Pty Ltd sponsors the drug Abaloparatide Source: July-2025-PBAC-Meeting-v4, Page: 2. It is indicated for the treatment of severe established osteoporosis Source: July-2025-PBAC-Meeting-v4, Page: 2.
References
Source: July-2025-PBAC-Meeting-v4, Page: 2
Generated code
---
**Evidence:**
{context_str}
---

**ANSWER:**
"""
# --- END OF FINAL FIX ---


########################################################################
### FILE: src/router/__init__.py
########################################################################




########################################################################
### FILE: src/router/tool_router.py
########################################################################

# FILE: src/router/tool_router.py
# V2.0: Modular dispatcher for the unified agent.
"""
Maps tool names from the planner to callable functions from the tool library.
This module replaces hardcoded routing with a pluggable, extensible registry.
"""
import logging
from typing import Callable, Dict

from src.models import QueryMetadata, ToolResult
from src.tools import retrievers # Import the entire retrievers module

logger = logging.getLogger(__name__)

class ToolRouter:
    """
    Executes a specific tool by name, providing a unified and safe interface.
    """
    def __init__(self):
        """
        Initializes the router by creating a registry of all available tools.
        The keys MUST match the 'tool_name' in `persona_tool_map.yml` and the planner logic.
        """
        self.registry: Dict[str, Callable[[str, QueryMetadata], ToolResult]] = {
            # Vector Search Tools
            "retrieve_clinical_data": retrievers.retrieve_clinical_data,
            "retrieve_summary_data": retrievers.retrieve_summary_data,
            "retrieve_general_text": retrievers.retrieve_general_text,
            
            # Graph Search Tool
            "query_knowledge_graph": retrievers.query_knowledge_graph,
            
            # Future tools can be added here easily:
            # "search_live_web": tools.search_live_web,
        }
        logger.info(f"ToolRouter initialized with {len(self.registry)} tools: {list(self.registry.keys())}")

    def execute_tool(self, tool_name: str, query: str, query_meta: QueryMetadata) -> ToolResult:
        """
        Looks up a tool by name in the registry and executes it.

        Args:
            tool_name: The name of the tool to execute.
            query: The original user query.
            query_meta: The structured metadata about the query.

        Returns:
            A ToolResult object with the outcome of the execution.
        """
        logger.info(f"[ToolRouter] Attempting to execute tool: '{tool_name}'")
        
        tool_function = self.registry.get(tool_name)

        if not tool_function:
            logger.warning(f"[ToolRouter] Tool '{tool_name}' not found in registry. Returning a failure result.")
            return ToolResult(
                tool_name=tool_name, 
                success=False, 
                content="[Error: Tool not implemented or misconfigured]"
            )

        try:
            # Call the registered function with the required arguments
            return tool_function(query, query_meta)
        except Exception as e:
            # This is a critical safety net. If a tool fails unexpectedly,
            # it won't crash the entire agent.
            logger.error(f"[ToolRouter] Tool '{tool_name}' failed with an unhandled exception: {e}", exc_info=True)
            return ToolResult(
                tool_name=tool_name, 
                success=False, 
                content=f"An unexpected error occurred in the tool: {e}"
            )


########################################################################
### FILE: src/tools/__init__.py
########################################################################




########################################################################
### FILE: src/tools/clients.py
########################################################################

# FILE: src/tools/clients.py
# V2.0: Consolidated, cached client initializers.
"""
Single source for initializing and retrieving external service clients.
Uses caching to prevent re-initialization on every call.
"""

import os
import logging
from functools import lru_cache

import pinecone
import neo4j
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# --- Client Initializers (Cached for Performance) ---

@lru_cache(maxsize=1)
def get_google_ai_client() -> genai:
    """Initializes and returns the Google AI client."""
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")
        genai.configure(api_key=api_key)
        logger.info("Google AI client configured successfully.")
        return genai
    except Exception as e:
        logger.error(f"Failed to configure Google AI client: {e}")
        return None

@lru_cache(maxsize=1)
def get_pinecone_index() -> pinecone.Index:
    """Initializes and returns the Pinecone index handle."""
    try:
        api_key = os.getenv("PINECONE_API_KEY")
        index_name = os.getenv("PINECONE_INDEX_NAME")
        if not api_key or not index_name:
            raise ValueError("PINECONE_API_KEY or PINECONE_INDEX_NAME not set.")
        
        # Updated initialization for latest pinecone-client versions
        pc = pinecone.Pinecone(api_key=api_key)
        index = pc.Index(index_name)
        logger.info(f"Pinecone index '{index_name}' connected successfully.")
        return index
    except Exception as e:
        logger.error(f"Failed to connect to Pinecone index: {e}")
        return None

@lru_cache(maxsize=1)
def get_neo4j_driver() -> neo4j.Driver:
    """Initializes and returns the Neo4j driver."""
    try:
        uri = os.getenv("NEO4J_URI")
        user = os.getenv("NEO4J_USERNAME", "neo4j")
        password = os.getenv("NEO4J_PASSWORD")
        if not all([uri, user, password]):
            raise ValueError("Neo4j connection details (URI, USERNAME, PASSWORD) not set.")

        driver = neo4j.GraphDatabase.driver(uri, auth=(user, password))
        driver.verify_connectivity()
        logger.info("Neo4j driver connected successfully.")
        return driver
    except Exception as e:
        logger.error(f"Failed to create Neo4j driver: {e}")
        return None


########################################################################
### FILE: src/tools/retrievers.py
########################################################################

# FILE: src/tools/retrievers.py
# V3.0 (Production Ready): Final, confirmed version.

import logging
from typing import List
import neo4j

from src.tools.clients import get_google_ai_client, get_pinecone_index, get_neo4j_driver
from src.models import ToolResult, QueryMetadata
from src.prompts import CYPHER_GENERATION_PROMPT

logger = logging.getLogger(__name__)

# --- Helper Functions ---

def _format_pinecone_results(matches: List[dict]) -> List[str]:
    contents = []
    for match in matches:
        metadata = match.get('metadata', {})
        text = metadata.get('text', 'No content available.')
        doc_id = metadata.get('doc_id', 'Unknown Document')
        page_num = metadata.get('page_number', 'N/A')
        url = metadata.get('source_pdf_url', '#')
        citation = f"[Source: {doc_id}, Page: {page_num}]({url})"
        contents.append(f"{text}\n{citation}")
    return contents

def _serialize_neo4j_path(path: neo4j.graph.Path) -> str:
    citations = set()
    if len(path.nodes) == 2 and len(path.relationships) == 1:
        subject = path.start_node.get('name', 'Unknown')
        predicate = path.relationships[0].type.replace('_', ' ').lower()
        object_val = path.end_node.get('name', 'Unknown')
        rel = path.relationships[0]
        
        doc_id = rel.get('doc_id', 'Unknown Document')
        url = rel.get('source_pdf_url', '#')
        page_num = rel.get('page_numbers', 'N/A')
        citation = f"[Source: {doc_id}, Page: {page_num}]({url})"
        citations.add(citation)
        
        text_representation = f"{subject} {predicate} {object_val}."
        citation_str = " ".join(sorted(list(citations)))
        return f"{text_representation}\n{citation_str}"

    return f"A complex relationship was found in the knowledge graph.\n[Source: Knowledge Graph, Page: N/A](#)"

def _vector_search_tool(query: str, namespace: str, tool_name: str, top_k: int = 7) -> ToolResult:
    pinecone_index = get_pinecone_index()
    embedding_client = get_google_ai_client()
    if not pinecone_index or not embedding_client: return ToolResult(tool_name=tool_name, success=False, content="Vector search client not available.")
    try:
        query_embedding = embedding_client.embed_content(model='models/text-embedding-004', content=query, task_type="retrieval_query")['embedding']
        response = pinecone_index.query(namespace=namespace, vector=query_embedding, top_k=top_k, include_metadata=True)
        if not response.get('matches'): return ToolResult(tool_name=tool_name, success=True, content="")
        content_list = _format_pinecone_results(response['matches'])
        return ToolResult(tool_name=tool_name, success=True, content="\n---\n".join(content_list))
    except Exception as e:
        return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred in the tool: {e}")

def retrieve_clinical_data(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-clinical", "retrieve_clinical_data")
def retrieve_summary_data(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-summary", "retrieve_summary_data")
def retrieve_general_text(query: str, query_meta: QueryMetadata) -> ToolResult:
    return _vector_search_tool(query, "pbac-text", "retrieve_general_text")

def query_knowledge_graph(query: str, query_meta: QueryMetadata) -> ToolResult:
    tool_name = "query_knowledge_graph"
    if not query_meta.question_is_graph_suitable: return ToolResult(tool_name=tool_name, success=True, content="")
    llm = get_google_ai_client().GenerativeModel('gemini-1.5-flash-latest')
    driver = get_neo4j_driver()
    if not llm or not driver: return ToolResult(tool_name=tool_name, success=False, content="LLM or Neo4j client is not available.")
    try:
        with driver.session() as session:
            schema_data = session.run("CALL db.schema.visualization()").data()
        schema_str = f"Node labels and properties: {schema_data[0]['nodes']}\nRelationship types: {schema_data[0]['relationships']}"
        prompt = CYPHER_GENERATION_PROMPT.format(schema=schema_str, question=query)
        response = llm.generate_content(prompt)
        cypher_query = response.text.strip().replace("```cypher", "").replace("```", "")
        if "none" in cypher_query.lower() or "match" not in cypher_query.lower(): return ToolResult(tool_name=tool_name, success=True, content="")
        logger.info(f"Generated Cypher: {cypher_query}")
    except Exception as e:
        return ToolResult(tool_name=tool_name, success=False, content=f"Cypher generation failed: {e}")
    try:
        with driver.session() as session:
            records = session.run(cypher_query).data()
        if not records: return ToolResult(tool_name=tool_name, success=True, content="")
        results = [
            _serialize_neo4j_path(record["p"]) if "p" in record and isinstance(record["p"], neo4j.graph.Path) else f"{str(record)}\n[Source: Knowledge Graph, Page: N/A](#)"
            for record in records
        ]
        return ToolResult(tool_name=tool_name, success=True, content="\n".join(results))
    except Exception as e:
        return ToolResult(tool_name=tool_name, success=False, content=f"Cypher execution failed: {e}")


########################################################################
### FILE: streamlit_app.py
########################################################################

# FILE: streamlit_app.py
# V2.2: UI updated with categorized test questions for robust evaluation.

import streamlit as st
import logging
from dotenv import load_dotenv

# --- CRITICAL: Load environment variables at the very top ---
# This ensures all modules can access them when imported.
load_dotenv()

# Now import project modules
from src.agent import Agent
from src.tools.clients import get_google_ai_client # Used for a pre-flight check

# --- Page and Logging Configuration ---
st.set_page_config(
    page_title="Persona RAG Chatbot",
    page_icon="🧠",
    layout="wide",
    initial_sidebar_state="expanded",
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - [%(name)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)

# --- Session State Initialization ---
if "agent" not in st.session_state:
    st.session_state.agent = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_persona" not in st.session_state:
    st.session_state.current_persona = "clinical_analyst"

# --- Helper Functions ---
@st.cache_resource
def initialize_agent():
    """
    Initializes the agent once and caches it.
    Returns the agent instance or None if initialization fails.
    """
    # Pre-flight check for API key to provide a better error message.
    if not get_google_ai_client():
        st.error("Google API Key is not configured. Please set the GOOGLE_API_KEY in your .env file.", icon="🚨")
        return None
    try:
        agent = Agent()
        logger.info("Unified agent initialized successfully and cached for the session.")
        return agent
    except Exception as e:
        st.error(f"Fatal error during agent initialization: {e}", icon="🚨")
        logger.error(f"Agent initialization failed: {e}", exc_info=True)
        return None

def reset_chat(persona_name: str):
    """Resets the chat history for a new conversation."""
    st.session_state.messages = [
        {"role": "assistant", "content": f"Hi! I'm now acting as a **{persona_name}**. How can I help you?"}
    ]

# --- Sidebar ---
with st.sidebar:
    st.header("🤖 Persona RAG Chatbot")
    st.markdown("Select a persona to tailor my retrieval strategy and answers to your specific role.")

    persona_options = {
        'Clinical Analyst': 'clinical_analyst',
        'Health Economist': 'health_economist',
        'Regulatory Specialist': 'regulatory_specialist',
    }
    
    current_display_name = [k for k, v in persona_options.items() if v == st.session_state.current_persona][0]
    
    selected_persona_name = st.radio(
        "**Choose your Persona:**",
        options=persona_options.keys(),
        index=list(persona_options.keys()).index(current_display_name),
        key="persona_selector"
    )
    
    selected_persona_key = persona_options[selected_persona_name]

    if selected_persona_key != st.session_state.current_persona:
        st.session_state.current_persona = selected_persona_key
        reset_chat(selected_persona_name)
        st.rerun()

    st.divider()
    if st.button("🔄 Clear Chat History", use_container_width=True):
        reset_chat(selected_persona_name)
        st.rerun()

    st.divider()
    st.header("🧪 Evaluation Questions")
    st.markdown("Use these questions to test the agent's capabilities.")

    # --- NEW: Categorized Test Questions ---
    with st.expander("🎯 Fact Retrieval (High Precision)"):
        questions = {
            "Graph-based query": "What company sponsors the drug Abaloparatide?",
            "Specific indication": "Which condition is Amivantamab intended to treat?",
            "Dosage form lookup": "What is the dosage form of Daratumumab?",
            "Sponsor for combination": "Which sponsor is associated with the combination of Dabrafenib and Trametinib?",
        }
        for name, q in questions.items():
            if st.button(f"{name}: {q}", key=q, use_container_width=True):
                st.session_state.run_prompt = q

    with st.expander("⚖️ Comparative Analysis"):
        questions = {
            "Compare two drugs for NSCLC": "Compare Amivantamab and Osimertinib for non-small cell lung cancer.",
            "Compare submissions by date": "What were the key differences in submissions between the March and July 2025 PBAC meetings?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q
    
    with st.expander("📋 Summarization"):
        questions = {
            "Summarize a drug's submission": "Provide a summary of the PBAC submission for Alectinib.",
            "Summarize a meeting": "Summarize all submissions related to oncology in the May 2025 meeting.",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q

    with st.expander("🤔 Challenging / Ambiguous Questions"):
        questions = {
            "Test fallback logic": "What is the price of Abaloparatide?", # Price is explicitly excluded from consumer comments
            "Broad, multi-hop query": "Find sponsors who made submissions for both lung cancer and melanoma in 2025.",
            "Out of scope": "What are the latest FDA guidelines?", # Tests if it hallucinates or admits it doesn't know
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q

# --- Main Chat Interface ---
st.title("Persona-Aware RAG Agent")
st.caption(f"Currently acting as: **{selected_persona_name}**")

# Initialize agent on first run and handle potential failure.
if st.session_state.agent is None:
    st.session_state.agent = initialize_agent()
    # If this is the first run, set the initial message.
    if not st.session_state.messages:
        reset_chat(selected_persona_name)

# Display chat messages
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"], unsafe_allow_html=True)

# Handle user input from both chat box and sidebar buttons
prompt_from_button = st.session_state.pop("run_prompt", None)
prompt_from_input = st.chat_input("Ask your question...")
prompt = prompt_from_button or prompt_from_input

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.agent:
            with st.spinner(f"Thinking as a {selected_persona_name}..."):
                response = st.session_state.agent.run(prompt, persona=st.session_state.current_persona)
                st.markdown(response, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            st.error("Agent is not available due to an initialization error. Please check the terminal logs.")
            st.stop()
    
    # Rerun to clear input box if a button was used
    if prompt_from_button:
        st.rerun()