========================================================================
  Combined Python Files from Git Repository: /home/mit/persona_rag_chatbot
  Generated on: Mon 04 Aug 2025 19:03:24 AEST
========================================================================



########################################################################
### FILE: src/__init__.py
########################################################################




########################################################################
### FILE: src/agent.py
########################################################################

# FILE: src/agent.py
# V8.2 (Final Polish): Includes a corrected JSON parser and final presentation logic.

import json
import logging
import time
import re
from datetime import datetime
from pathlib import Path
from typing import List, Tuple
from concurrent.futures import ThreadPoolExecutor

from src.tools.clients import get_generative_model, get_flash_model, DEFAULT_REQUEST_OPTIONS
from src.models import ToolResult, QueryMetadata, ToolPlanItem
from src.planner.query_classifier import QueryClassifier
from src.planner.tool_planner import ToolPlanner
from src.planner.persona_classifier import PersonaClassifier
from src.planner.query_rewriter import QueryRewriter
from src.router.tool_router import ToolRouter
from src.prompts import DECOMPOSITION_PROMPT, REASONING_SYNTHESIS_PROMPT, DIRECT_SYNTHESIS_PROMPT, RERANKING_PROMPT

logger = logging.getLogger(__name__)
LOG_PATH = Path("trace_logs.jsonl")

# --- DEFINITIVE FIX: Robust JSON Parser that handles objects AND arrays ---
def extract_json_from_response(text: str) -> dict | list:
    """Finds and parses the first valid JSON object or array from a string."""
    # Regex to find JSON wrapped in markdown, supporting both objects {} and arrays []
    match = re.search(r'```json\s*([\[\{].*?[\]\}])\s*```', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    try:
        # Fallback to parsing the entire string
        return json.loads(text)
    except json.JSONDecodeError:
        logger.warning(f"Could not parse JSON from response text: {text}")
        return {} # Return empty dict on failure


class Timer:
    def __init__(self, name): self.name = name
    def __enter__(self): self.start = time.perf_counter(); return self
    def __exit__(self, *args): self.end = time.perf_counter(); logger.info(f"[TIMER] {self.name} took {(self.end - self.start) * 1000:.2f} ms")

def log_trace(query: str, persona: str, query_meta: QueryMetadata, tool_plan: List[ToolPlanItem], tool_results: List[ToolResult], final_answer: str, total_latency_sec: float):
    trace_record = { "timestamp": datetime.utcnow().isoformat() + "Z", "query": query, "persona": persona, "intent": query_meta.intent if query_meta else "classification_failed", "graph_suitable": query_meta.question_is_graph_suitable if query_meta else "unknown", "tool_plan": [t.model_dump() for t in tool_plan] if tool_plan else [], "tool_results": [r.model_dump() for r in tool_results] if tool_results else [], "final_answer_preview": final_answer[:200] + "..." if final_answer else "N/A", "total_latency_sec": round(total_latency_sec, 3) }
    try:
        with open(LOG_PATH, "a", encoding="utf-8") as f: f.write(json.dumps(trace_record) + "\n")
    except Exception as e:
        logger.error(f"Failed to write to trace log: {e}", exc_info=True)


class Agent:
    def __init__(self, confidence_threshold: float = 0.85):
        self.classifier = QueryClassifier()
        self.planner = ToolPlanner(coverage_threshold=confidence_threshold)
        self.router = ToolRouter()
        self.persona_classifier = PersonaClassifier()
        self.rewriter = QueryRewriter()
        self.llm = get_generative_model('gemini-1.5-pro-latest')
        self.synthesis_llm = get_flash_model('gemini-1.5-flash-latest')
        self.reranker_llm = get_flash_model('gemini-1.5-flash-latest')

    def _rerank_with_gemini(self, query: str, documents: List[str]) -> List[str]:
        if not self.reranker_llm or not documents: return documents
        formatted_docs = "\n\n".join([f"DOCUMENT[{i}]:\n{doc}" for i, doc in enumerate(documents)])
        prompt = RERANKING_PROMPT.format(question=query, documents=formatted_docs)
        try:
            with Timer("Re-ranking with Gemini"):
                response = self.reranker_llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
                best_indices = extract_json_from_response(response.text)
                if not isinstance(best_indices, list):
                    logger.warning("Gemini re-ranker did not return a list. Falling back.")
                    return documents[:5]
                reranked_docs = [documents[i] for i in best_indices if i < len(documents)]
                logger.info(f"Re-ranked {len(documents)} snippets down to {len(reranked_docs)} using Gemini.")
                return reranked_docs
        except Exception as e:
            logger.error(f"Gemini re-ranking failed: {e}. Falling back to top 5.", exc_info=False)
            return documents[:5]

    def _run_single_rag_step(self, query: str, persona: str) -> Tuple[str, QueryMetadata, List[ToolPlanItem], List[ToolResult]]:
        with Timer(f"Single RAG Step for '{query[:30]}...'"):
            query_meta = self.classifier.classify(query)
            if not query_meta: return "I had trouble understanding the query.", None, [], []

            tool_plan = self.planner.plan(query_meta, persona)
            if not tool_plan: return "I don't have a strategy for this query.", query_meta, [], []

            with ThreadPoolExecutor(max_workers=len(tool_plan)) as executor:
                futures = [executor.submit(self.router.execute_tool, item.tool_name, query, query_meta) for item in tool_plan]
                results = [future.result() for future in futures]

            all_docs = []
            for res in results:
                if res and res.success and res.content.strip():
                    all_docs.extend(res.content.split("\n---\n"))

            if not all_docs:
                return "I searched but could not find any relevant details.", query_meta, tool_plan, results

            ranked_docs = self._rerank_with_gemini(query, all_docs)
            if not ranked_docs:
                return "I found some information, but it did not seem relevant.", query_meta, tool_plan, results

            evidence_texts, citation_links = [], []
            for doc in ranked_docs:
                parts = doc.split("\nCitation: ")
                evidence_texts.append(parts[0])
                if len(parts) > 1:
                    citation_links.append(parts[1])

            formatted_context = "\n\n".join([f"EVIDENCE [{i+1}]:\n{text}" for i, text in enumerate(evidence_texts)])
            final_prompt = DIRECT_SYNTHESIS_PROMPT.format(question=query, context_str=formatted_context)
            
            with Timer("Direct Synthesis LLM Call (Flash)"):
                response = self.synthesis_llm.generate_content(final_prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            answer_text = response.text.strip()

            used_indices = {int(m) - 1 for m in re.findall(r'\[(\d+)\]', answer_text)}
            
            final_response = answer_text
            if used_indices:
                references_section = "\n\n**References**\n"
                unique_used_links = set()
                used_links_ordered = []
                for idx in sorted(list(used_indices)):
                    if idx < len(citation_links):
                        link = citation_links[idx]
                        if link not in unique_used_links:
                            unique_used_links.add(link)
                            used_links_ordered.append(link)
                references_section += "\n".join([f"{i+1}. {link}" for i, link in enumerate(used_links_ordered)])
                final_response += references_section

            return final_response, query_meta, tool_plan, results

    def run(self, query: str, persona: str, chat_history: List[str]) -> str:
        run_start_time = time.perf_counter()
        final_answer, final_query_meta, final_tool_plan, final_tool_results = "", None, [], []
        try:
            with Timer("Full Agent Run"):
                rewritten_query = self.rewriter.rewrite(query, chat_history)
                chosen_persona = self.persona_classifier.classify(rewritten_query) if persona == "automatic" else persona
                persona_display_name = " ".join(word.capitalize() for word in chosen_persona.split("_"))
                
                with Timer("Decomposition"):
                    decomp_prompt = DECOMPOSITION_PROMPT.format(chat_history="\n- ".join(chat_history), question=rewritten_query)
                    decomp_response = self.synthesis_llm.generate_content(decomp_prompt, request_options=DEFAULT_REQUEST_OPTIONS)
                    plan_data = extract_json_from_response(decomp_response.text)

                requires_decomposition = plan_data.get("requires_decomposition", False)
                plan = plan_data.get("plan", [rewritten_query])

                if not requires_decomposition or len(plan) <= 1:
                    logger.info(f"Executing single-step plan for query: '{plan[0]}'")
                    synthesis_result, final_query_meta, final_tool_plan, final_tool_results = self._run_single_rag_step(plan[0], chosen_persona)
                else:
                    logger.info(f"Executing multi-step plan for query: '{rewritten_query}'")
                    scratchpad = []
                    with ThreadPoolExecutor(max_workers=len(plan)) as executor:
                        sub_futures = [executor.submit(self._run_single_rag_step, sub_q, chosen_persona) for sub_q in plan]
                        sub_results_list = [future.result() for future in sub_futures]
                    
                    for i, sub_q in enumerate(plan):
                        sub_answer, _, _, sub_tool_results = sub_results_list[i]
                        observation = f"Sub-Question: {sub_q}\nFinding: {sub_answer}"
                        scratchpad.append(observation)
                        if sub_tool_results: final_tool_results.extend(sub_tool_results)

                    with Timer("Reasoning Synthesis LLM Call (Pro)"):
                        synthesis_prompt = REASONING_SYNTHESIS_PROMPT.format(question=rewritten_query, scratchpad="\n\n---\n\n".join(scratchpad))
                        synthesis_response = self.llm.generate_content(synthesis_prompt, request_options=DEFAULT_REQUEST_OPTIONS)
                    synthesis_result = synthesis_response.text
                
                final_answer = f"Acting as a **{persona_display_name}**, here is what I found:\n\n{synthesis_result}" if persona == "automatic" else synthesis_result
                return final_answer
        except Exception as e:
            logger.error(f"An unexpected error occurred during agent run: {e}", exc_info=True)
            return "I encountered a critical error. Please check the system logs."
        finally:
            run_end_time = time.perf_counter()
            log_trace(query, persona, final_query_meta, final_tool_plan, final_tool_results, final_answer, run_end_time - run_start_time)


########################################################################
### FILE: src/common/utils.py
########################################################################

# FILE: src/common/utils.py
import yaml
from pathlib import Path

def load_config(file_path: str) -> dict:
    with open(Path(file_path), 'r') as f:
        return yaml.safe_load(f)


########################################################################
### FILE: src/fallback.py
########################################################################

# FILE: src/fallback.py
# Phase 3.1: FallbackLayer — graceful UX when all tools return empty or fail

import logging
from typing import List
from src.models import ToolResult

logger = logging.getLogger(__name__)

FALLBACK_QUESTIONS = [
    "Would you like to compare two drugs instead?",
    "Can I help you find a sponsor for a specific medicine?",
    "Do you want to search the original PDF documents directly?"
]


def should_trigger_fallback(results: List[ToolResult]) -> bool:
    """
    Returns True if all tools failed or produced no meaningful content.
    """
    if not results:
        logger.info("Fallback triggered: no tool results returned.")
        return True

    empty_or_failed = [r for r in results if not r.success or not r.content or len(r.content.strip()) < 5]
    if len(empty_or_failed) == len(results):
        logger.info("Fallback triggered: all tools failed or content was empty.")
        return True

    return False


def render_fallback_message(query: str) -> str:
    """
    Returns a polite fallback message with suggested next questions.
    """
    msg = f"""
I'm sorry — based on the current documents and tools, I couldn't find sufficient information to answer your question:

"{query}"

However, here are some things you can try next:

"""
    for i, q in enumerate(FALLBACK_QUESTIONS, start=1):
        msg += f"{i}. {q}\n"

    msg += "\nYou can also try rephrasing your question for better results."
    return msg.strip()



########################################################################
### FILE: src/models.py
########################################################################

from pydantic import BaseModel, Field
from typing import List, Optional, Literal


QueryIntent = Literal[
    "specific_fact_lookup",
    "simple_summary",
    "comparative_analysis",
    "general_qa",
    "unknown"
]


class QueryMetadata(BaseModel):
    intent: QueryIntent
    keywords: List[str]
    question_is_graph_suitable: bool
    themes: Optional[List[str]] = Field(default_factory=list, description="High-level themes for metadata filtering.")


class ToolPlanItem(BaseModel):
    tool_name: str
    estimated_coverage: float


class ToolResult(BaseModel):
    tool_name: str
    success: bool
    content: str
    estimated_coverage: float = 0.0


class ContextItem(BaseModel):
    content: str
    source: Optional[dict] = None


class Source(BaseModel):
    type: str
    document_id: Optional[str] = None
    page_numbers: Optional[List[int]] = None
    source_url: Optional[str] = None
    retrieval_score: Optional[float] = None
    query: Optional[str] = None

class NamespaceConfig(BaseModel):
    namespace: str
    weight: float
    top_k: int

class RetrievalPlan(BaseModel):
    namespaces: List[NamespaceConfig] = []


########################################################################
### FILE: src/planner/__init__.py
########################################################################




########################################################################
### FILE: src/planner/persona_classifier.py
########################################################################

# FILE: src/planner/persona_classifier.py
# V1.1 (Resilience Fix): Added request_options to the generate_content call.

import logging
from typing import Literal
# --- DEFINITIVE FIX: Import the config and model getter ---
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS

logger = logging.getLogger(__name__)

Persona = Literal["clinical_analyst", "health_economist", "regulatory_specialist"]

PERSONA_CLASSIFICATION_PROMPT = """
You are an expert request router. Your task is to analyze the user's question and determine which specialist persona is best equipped to answer it. You must choose from the available personas and provide ONLY the persona's key name as your response.

**Available Personas & Their Expertise:**

1.  **`clinical_analyst`**:
    *   Focuses on: Clinical trial data, drug efficacy, safety profiles, patient outcomes, medical conditions, and mechanisms of action.
    *   Keywords: treat, condition, indication, dosage, patients, trial, effective, side effects.
    *   Choose this persona for questions about the medical and scientific aspects of a drug.

2.  **`health_economist`**:
    *   Focuses on: Cost-effectiveness, pricing, market access, economic evaluations, and healthcare policy implications.
    *   Keywords: cost, price, economic, budget, financial, value, policy, summary.
    *   Choose this persona for questions about the financial or policy-level impact of a drug.

3.  **`regulatory_specialist`**:
    *   Focuses on: Submission types, meeting agendas, regulatory pathways (e.g., PBS listing types), sponsors, and official guidelines.
    *   Keywords: sponsor, submission, listing, agenda, meeting, guideline, change, status.
    *   Choose this persona for questions about the process and logistics of drug approval and listing.

**User Question:**
"{question}"

**Instructions:**
- Read the user's question carefully.
- Compare it against the expertise of each persona.
- Return ONLY the single key name (e.g., `clinical_analyst`) of the best-fitting persona. Do not add any explanation or other text.
"""

class PersonaClassifier:
    def __init__(self):
        # --- DEFINITIVE FIX: Use the new centralized model getter ---
        self.llm = get_flash_model()
        if not self.llm:
            logger.error("FATAL: Gemini client not initialized, PersonaClassifier will not work.")

    def classify(self, query: str) -> Persona:
        """Classifies the query and returns the most appropriate persona key."""
        if not self.llm:
            return "regulatory_specialist" # A safe default

        try:
            prompt = PERSONA_CLASSIFICATION_PROMPT.format(question=query)
            # --- DEFINITIVE FIX: Add request_options to the call ---
            response = self.llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            persona_key = response.text.strip()

            if persona_key in ["clinical_analyst", "health_economist", "regulatory_specialist"]:
                logger.info(f"Query classified for persona: '{persona_key}'")
                return persona_key
            else:
                logger.warning(f"Persona classification returned an invalid key: '{persona_key}'. Falling back to default.")
                return "regulatory_specialist"
        except Exception as e:
            logger.error(f"Persona classification failed: {e}", exc_info=True)
            return "regulatory_specialist" # Fallback on error


########################################################################
### FILE: src/planner/query_classifier.py
########################################################################

# FILE: src/planner/query_classifier.py
# V1.4 (Final Fix): Corrected Pydantic model import and literal values.

import logging
import json
import re
from typing import Optional

# --- DEFINITIVE FIX: Import the correct model from src.models ---
from src.models import QueryMetadata, QueryIntent
from src.prompts import QUERY_CLASSIFICATION_PROMPT_V2 as QUERY_CLASSIFICATION_PROMPT
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS

logger = logging.getLogger(__name__)

def extract_json_from_response(text: str) -> dict:
    """Finds and parses the first valid JSON object from a string."""
    match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        logger.warning(f"Could not parse JSON from response text: {text}")
        return {}


class QueryClassifier:
    def __init__(self):
        self.model = get_flash_model()

    def classify(self, query: str) -> Optional[QueryMetadata]:
        if not self.model: return None
        logger.info(f"Classifying query: {query}")
        try:
            prompt = QUERY_CLASSIFICATION_PROMPT + f"\n\nUser Query: {query}"
            response = self.model.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            
            json_data = extract_json_from_response(response.text)
            
            # --- DEFINITIVE FIX: Ensure intent matches the allowed literals ---
            if "intent" in json_data and json_data["intent"] == "comparison":
                json_data["intent"] = "comparative_analysis"
            
            metadata = QueryMetadata.model_validate(json_data)
            
            logger.info(f"Classification result: {metadata.model_dump_json(indent=2)}")
            return metadata
        except Exception as e:
            logger.error(f"Query classification failed: {e}", exc_info=True)
            return None


########################################################################
### FILE: src/planner/query_rewriter.py
########################################################################

# FILE: src/planner/query_rewriter.py
# V2.0 (Performance Fix): Added a heuristic check to bypass the LLM call for non-conversational queries.

import logging
from typing import List
# --- DEFINITIVE FIX: Import the config and model getter ---
from src.tools.clients import get_flash_model, DEFAULT_REQUEST_OPTIONS

logger = logging.getLogger(__name__)

# --- DEFINITIVE FIX: Heuristic pre-flight check ---
# A list of common words that indicate a query is conversational and likely needs context from chat history.
# We check for these words (as whole words, case-insensitively) before making a slow API call.
CONVERSATIONAL_TRIGGERS = {"it", "its", "they", "them", "that", "those", "this", "these"}

REWRITE_PROMPT = """
You are an expert query analyst. Your task is to rewrite a user's latest question into a standalone question that can be understood without the context of the chat history.

**CRITICAL RULES:**
1.  **If the "Latest User Question" is already a complete, standalone question, you MUST return it exactly as it is.** Do not modify it.
2.  If the "Latest User Question" contains pronouns (like "it", "its", "they") or ambiguous references ("this drug", "that"), use the "Chat History" to resolve these references and create a complete question.
3.  Your output MUST be only the rewritten question. Do not add any commentary.

**EXAMPLE 1 (Rewrite Needed):**
- **Chat History:**
  - user: Who is the sponsor for Esketamine?
  - assistant: Janssen-Cilag Pty Ltd. is the sponsor for Esketamine.
- **Latest User Question:** what is its use?
- **Your Rewritten Question:** What is the use of Esketamine?

**EXAMPLE 2 (No Rewrite Needed):**
- **Chat History:**
  - user: Who is the sponsor for Esketamine?
  - assistant: Janssen-Cilag Pty Ltd. is the sponsor for Esketamine.
- **Latest User Question:** What is the dosage form for Fruquintinib?
- **Your Rewritten Question:** What is the dosage form for Fruquintinib?

**TASK:**
- **Chat History:**
{chat_history}
- **Latest User Question:** {question}

**Your Rewritten Question:**
"""

class QueryRewriter:
    def __init__(self):
        # --- DEFINITIVE FIX: Use the new centralized model getter ---
        self.llm = get_flash_model()
        if not self.llm:
            logger.error("FATAL: Gemini client not initialized, QueryRewriter will not work.")

    def rewrite(self, query: str, chat_history: List[str]) -> str:
        """Rewrites a conversational query into a standalone query, with a performance-enhancing pre-check."""
        if not self.llm or not chat_history:
            return query # If no history or no LLM, cannot rewrite.

        # --- DEFINITIVE FIX: Performance optimization ---
        # Check if the query contains any conversational trigger words.
        # This avoids a slow network call for the majority of queries which are already standalone.
        query_words = set(query.lower().split())
        if not CONVERSATIONAL_TRIGGERS.intersection(query_words):
            logger.info(f"Query deemed standalone. Bypassing LLM rewrite. Query: '{query}'")
            return query
        # --- End of performance optimization ---

        try:
            formatted_history = "\n  - ".join(chat_history)
            prompt = REWRITE_PROMPT.format(chat_history=formatted_history, question=query)
            
            # --- DEFINITIVE FIX: Add request_options to the call ---
            response = self.llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            rewritten_query = response.text.strip()
            
            if rewritten_query:
                logger.info(f"Original query: '{query}' -> Rewritten query: '{rewritten_query}'")
                return rewritten_query
            else:
                logger.warning("Query rewrite resulted in an empty string. Using original query.")
                return query
        except Exception as e:
            logger.error(f"Query rewriting failed: {e}", exc_info=True)
            return query # Fallback to original query on error


########################################################################
### FILE: src/planner/tool_planner.py
########################################################################

# FILE: src/planner/tool_planner.py
# V2.0: Persona-Aware Tool Planner
import logging
import yaml
from pathlib import Path
from typing import List, Dict

from src.models import QueryMetadata, ToolPlanItem

logger = logging.getLogger(__name__)

# Base scores for tools based on query intent. A higher score is better.
# These tool names MUST match the names in `persona_tool_map.yml` and the tool router.
INTENT_TOOL_SCORES = {
    # If the user wants a specific fact...
    "specific_fact_lookup": {
        "query_knowledge_graph": 0.9,
        "retrieve_clinical_data": 0.7,
        "retrieve_general_text": 0.6,
        "retrieve_summary_data": 0.4,
    },
    # If the user wants a high-level summary...
    "simple_summary": {
        "retrieve_summary_data": 0.9,
        "retrieve_general_text": 0.8,
        "retrieve_clinical_data": 0.5,
        "query_knowledge_graph": 0.4,
    },
    # If the user wants to compare things...
    "comparative_analysis": {
        "retrieve_clinical_data": 0.8,
        "retrieve_summary_data": 0.8,
        "retrieve_general_text": 0.7,
        "query_knowledge_graph": 0.5,
    },
    # For general questions...
    "general_qa": {
        "retrieve_general_text": 0.9,
        "retrieve_summary_data": 0.7,
        "query_knowledge_graph": 0.6,
        "retrieve_clinical_data": 0.5,
    },
}
DEFAULT_INTENT_SCORE = 0.5
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

class ToolPlanner:
    def __init__(self, coverage_threshold: float = 0.9):
        self.coverage_threshold = coverage_threshold
        self._load_persona_map()

    def _load_persona_map(self):
        """Loads the persona-to-tool mapping from the central YAML config."""
        map_file = PROJECT_ROOT / "config" / "persona_tool_map.yml"
        try:
            with open(map_file, 'r') as f:
                self.persona_map = yaml.safe_load(f)
            logger.info(f"Successfully loaded persona-tool map from '{map_file}'.")
        except Exception as e:
            logger.error(f"FATAL: Could not load or parse persona-tool map from '{map_file}': {e}", exc_info=True)
            self.persona_map = {}

    def plan(self, query_meta: QueryMetadata, persona: str) -> List[ToolPlanItem]:
        """
        Creates a ranked tool plan by combining query intent with user persona preferences.
        """
        logger.info(f"Planning tools for intent '{query_meta.intent}' and persona '{persona}'")
        
        # 1. Get the list of preferred tools and their weights for the given persona
        persona_key = persona.lower().replace(" ", "_")
        persona_prefs = self.persona_map.get(persona_key, self.persona_map.get("default", []))

        if not persona_prefs:
            logger.warning(f"No tool preferences found for persona '{persona_key}' or default. Returning empty plan.")
            return []
            
        persona_tool_weights: Dict[str, float] = {p["tool_name"]: p["weight"] for p in persona_prefs}
        
        # 2. Get intent-based scores for tools relevant to the current query intent
        intent_scores = INTENT_TOOL_SCORES.get(query_meta.intent, {})

        # 3. Calculate a final score for each tool by multiplying persona weight and intent score
        scored_tools = []
        for tool_name, persona_weight in persona_tool_weights.items():
            intent_score = intent_scores.get(tool_name, DEFAULT_INTENT_SCORE)
            
            # The final score reflects both the persona's general preference and the tool's suitability for the task
            final_score = persona_weight * intent_score
            scored_tools.append({"name": tool_name, "score": final_score})

        # 4. Sort tools by their final score in descending order
        scored_tools.sort(key=lambda x: x["score"], reverse=True)

        # 5. Build the final plan, adding tools until the cumulative coverage threshold is met
        final_plan: List[ToolPlanItem] = []
        total_coverage = 0.0
        for tool in scored_tools:
            # We treat the score as the estimated coverage for this planning step
            estimated_coverage = round(tool["score"], 2)

            # Do not add tools with negligible contribution
            if estimated_coverage <= 0.1:
                continue

            plan_item = ToolPlanItem(tool_name=tool["name"], estimated_coverage=estimated_coverage)
            final_plan.append(plan_item)
            
            total_coverage += estimated_coverage
            if total_coverage >= self.coverage_threshold:
                logger.info(f"Coverage threshold of {self.coverage_threshold} met. Finalizing plan.")
                break
        
        logger.info(f"Generated tool plan: {[t.model_dump_json(indent=2) for t in final_plan]}")
        return final_plan


########################################################################
### FILE: src/prompts.py
########################################################################

# FILE: src/prompts.py
# V3.0 (ReAct Architecture): Added prompts for multi-step reasoning.

"""
Production-grade prompts for a robust RAG agent. This version supports both
direct RAG and a multi-step ReAct (Reason+Act) style reasoning loop.
"""

# ... (other prompts remain the same) ...

# --- DEFINITIVE FIX: A clearer, more accurate classification prompt ---
QUERY_CLASSIFICATION_PROMPT_V2 = """
You are an expert query analysis agent. Your task is to analyze the user's question and provide a structured JSON output with four fields.

**Fields to Generate:**

1.  `intent`: Classify the user's goal (e.g., "specific_fact_lookup", "simple_summary").
2.  `keywords`: Extract key nouns like drug names, company names, etc.
3.  `themes`: Extract high-level conceptual themes from the question. Choose from this list: ["Oncology", "Regulatory History", "Efficacy Results", "Safety Results", "Clinical Trial Design", "Pharmacoeconomic Analysis", "Dosage and Administration", "Drug/Therapy Description", "Indication/Population Description"]. Return an empty list if no theme applies.
4.  `question_is_graph_suitable`: Return `true` if the question asks for a direct relationship between two specific entities (e.g., "Who sponsors DrugX?", "What does DrugY treat?"). Return `false` for summaries, comparisons, or general questions.

**CRITICAL INSTRUCTIONS:**
- Output ONLY the raw JSON object. Do not add explanations or markdown code fences.
- If the question is "What is Amivantamab used to treat?", the relationship is (Amivantamab -> used to treat -> ?), so `question_is_graph_suitable` MUST be `true`.
- If the question is "Summarize the May meeting", there is no direct relationship, so `question_is_graph_suitable` MUST be `false`.
"""

CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema.

**Live Graph Schema:**
{schema}

**CRITICAL Instructions:**
1.  Your `RETURN` clause should look like this: `RETURN p, properties(r) as rel_props`.
2.  The relationship in your `MATCH` clause must be assigned to a variable `r`.
3.  Query against the `name_normalized` property for all `WHERE` clauses on nodes.
4.  If the question cannot be answered, you MUST return the single word: `NONE`.
5.  Output ONLY the Cypher query or the word `NONE`.

**Example Question:** "What company sponsors Abaloparatide?"
**Example Valid Query:** MATCH p=(drug:Entity)-[r:HASSPONSOR]->(sponsor:Entity) WHERE drug.name_normalized = 'abaloparatide' RETURN p, properties(r) as rel_props

**Question:** {question}
"""

# ... (other prompts) ...
CYPHER_GENERATION_PROMPT = "..."
DECOMPOSITION_PROMPT = "..."
REASONING_SYNTHESIS_PROMPT = "..."
DIRECT_SYNTHESIS_PROMPT = "..."

CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema.

**Live Graph Schema:**
{schema}

**CRITICAL Instructions:**
1.  Your `RETURN` clause should look like this: `RETURN p, properties(r) as rel_props`.
2.  The relationship in your `MATCH` clause must be assigned to a variable `r`.
3.  Query against the `name_normalized` property for all `WHERE` clauses on nodes.
4.  If the question cannot be answered, you MUST return the single word: `NONE`.
5.  Output ONLY the Cypher query or the word `NONE`.

**Example Question:** "What company sponsors Abaloparatide?"
**Example Valid Query:** MATCH p=(drug:Entity)-[r:HASSPONSOR]->(sponsor:Entity) WHERE drug.name_normalized = 'abaloparatide' RETURN p, properties(r) as rel_props

**Question:** {question}
"""

# --- START OF DEFINITIVE FIX ---

DECOMPOSITION_PROMPT = """
You are a master query planner. Your goal is to determine if a user's question can be answered in a single step or if it requires decomposition into multiple, simpler sub-questions.

Analyze the user's question and the chat history.

**Decision Criteria:**
- **Single Step:** If the question asks for a direct fact, a summary of a single topic, or a simple definition.
- **Decomposition:** If the question requires comparing information from two or more distinct topics (e.g., two drugs, two meetings), finding an intersection of two sets of information (e.g., "sponsors who submitted in BOTH meetings"), or involves a sequence of steps.

**Output Schema:**
You MUST output a single, valid JSON object with two keys:
1.  `requires_decomposition`: A boolean (`true` or `false`).
2.  `plan`: A list of strings.
    - If `requires_decomposition` is `false`, the plan should contain a single item: the original question.
    - If `requires_decomposition` is `true`, the plan should contain two or more simple, answerable sub-questions that build on each other to answer the original question.

**Example 1 (Single Step):**
- User Question: "What is the use of Esketamine?"
- Your JSON Output:
{{
  "requires_decomposition": false,
  "plan": ["What is the use of Esketamine?"]
}}

**Example 2 (Decomposition):**
- User Question: "Which companies submitted drugs in both the March 2024 and May 2024 PBAC meetings?"
- Your JSON Output:
{{
  "requires_decomposition": true,
  "plan": [
    "List all sponsors who made submissions in the March 2024 PBAC meeting documents.",
    "List all sponsors who made submissions in the May 2024 PBAC meeting documents."
  ]
}}

**TASK:**
- Chat History: {chat_history}
- User Question: {question}

Now, generate the JSON output.
"""

# --- END OF DEFINITIVE FIX ---

REASONING_SYNTHESIS_PROMPT = """
You are a highly intelligent synthesis agent. Your task is to answer a user's complex original question based on a series of observations you have made by answering simpler sub-questions.

**User's Original Question:** "{question}"

**Your Observations (Scratchpad):**
---
{scratchpad}
---

**CRITICAL INSTRUCTIONS:**
1.  Read the user's original question and all your observations from the scratchpad.
2.  Synthesize a final, comprehensive answer to the original question.
3.  **Do not show your step-by-step reasoning.** Just provide the final, clean answer.
4.  If your observations are insufficient to answer the question, clearly state what information you found and why it is not enough.
5.  Include citations from your observations where appropriate.

**Final Answer:**
"""

# --- END: New Prompts for ReAct Agent ---


# --- DEFINITIVE FIX: New prompt for in-line citations and clean responses ---
DIRECT_SYNTHESIS_PROMPT = """
You are a precise, professional document analysis bot. Your ONLY job is to answer the user's question based strictly on the provided evidence.

**TASK:**
1.  Read the User's Question.
2.  Read the numbered evidence blocks (`EVIDENCE [1]`, `EVIDENCE [2]`, etc.).
3.  Synthesize a direct, professional answer to the question.
4.  When you use information from a piece of evidence, you **MUST** cite it by placing its corresponding number in brackets, like `[1]`.
5.  Cite each piece of evidence you use. If multiple pieces of evidence support a single point, you can cite them together, like `[1][2]`.
6.  Your answer must be based **ONLY** on the provided evidence. Do not add outside knowledge.
7.  If the evidence is insufficient to answer the question, you **MUST** state that the provided evidence does not contain the answer.

**EXAMPLE:**
User's Question: "What is the sponsor and dosage form for Abaloparatide?"
Evidence:
EVIDENCE [1]:
Evidence from graph: ABALOPARATIDE has sponsor THERAMEX AUSTRALIA PTY LTD.

EVIDENCE [2]:
Evidence from document: The submission for Abaloparatide was for a 3mg dosage form.

Your Answer:
The sponsor for Abaloparatide is Theramex Australia Pty Ltd [1]. The dosage form submitted was 3 mg [2].

---
**User's Question:** "{question}"
---
**Evidence:**
{context_str}
---
**Your Answer:**
"""

# --- NEW PROMPT FOR GEMINI-BASED RE-RANKING ---
RERANKING_PROMPT = """
You are a highly intelligent and precise relevance-ranking model. Your task is to analyze a user's question and a list of retrieved documents, and then return a JSON list of the document indices that are most relevant for answering the question.

**CRITICAL INSTRUCTIONS:**
1.  Read the user's question to understand their core intent.
2.  Read each document, identified by its index (e.g., `DOCUMENT[0]`, `DOCUMENT[1]`).
3.  Determine which documents contain direct, explicit information that helps answer the question.
4.  Your output **MUST** be a single, valid JSON array of integers, representing the indices of the most relevant documents, sorted from most relevant to least relevant.
5.  Include **ONLY** the indices of documents that are directly relevant. If a document is only tangentially related, do not include its index.
6.  If **NO** documents are relevant, return an empty JSON array `[]`.
7.  Do not include more than the top 5 most relevant document indices.

**EXAMPLE:**
- **User Question:** "What is the dosage form for Apomorphine?"
- **Documents:**
  DOCUMENT[0]:
  Evidence from document: The sponsor for Apomorphine is STADA...
  Citation: <a...>
  
  DOCUMENT[1]:
  Evidence from document: Movapo® (apomorphine hydrochloride hemihydrate) is available as a solution for subcutaneous infusion...
  Citation: <a...>
  
  DOCUMENT[2]:
  Evidence from document: The PBAC recommended the listing of Abaloparatide...
  Citation: <a...>

- **Your JSON Output:**
  [1]

---
**TASK:**

**User Question:** "{question}"

**Documents:**
{documents}

**Your JSON Output:**
"""


########################################################################
### FILE: src/router/__init__.py
########################################################################




########################################################################
### FILE: src/router/tool_router.py
########################################################################

# FILE: src/router/tool_router.py
# V5.0 (Unified Tooling): Refactored to use only the two primary tools.

import logging
from typing import Callable, Dict

from src.models import QueryMetadata, ToolResult
from src.tools import retrievers

logger = logging.getLogger(__name__)

class ToolRouter:
    def __init__(self):
        # --- DEFINITIVE FIX: Register only the tools that now exist ---
        self.registry: Dict[str, Callable[[str, QueryMetadata], ToolResult]] = {
            "vector_search": retrievers.vector_search,
            "query_knowledge_graph": retrievers.query_knowledge_graph,
        }
        logger.info(f"ToolRouter initialized with {len(self.registry)} tools.")

    def execute_tool(self, tool_name: str, query: str, query_meta: QueryMetadata) -> ToolResult:
        logger.info(f"[ToolRouter] Executing tool: '{tool_name}'")
        tool_function = self.registry.get(tool_name)
        if not tool_function:
            logger.warning(f"Tool '{tool_name}' not found in registry.")
            return ToolResult(tool_name=tool_name, success=False, content="[Error: Tool not implemented]")
        try:
            return tool_function(query, query_meta)
        except Exception as e:
            logger.error(f"[ToolRouter] Tool '{tool_name}' failed: {e}", exc_info=True)
            return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred: {e}")


########################################################################
### FILE: src/tools/__init__.py
########################################################################




########################################################################
### FILE: src/tools/clients.py
########################################################################

# FILE: src/tools/clients.py
# V3.0 (Final Version): Removed all unnecessary Cohere client code.

import os
import logging
from functools import lru_cache

import pinecone
import neo4j
import google.generativeai as genai
from google.generativeai.client import get_default_generative_client
from google.api_core.retry import Retry
from google.api_core.client_options import ClientOptions
from google.api_core import exceptions as google_exceptions
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# --- Resilience Configuration ---
def is_service_unavailable(exc: Exception) -> bool:
    """Predicate function to check if an exception is a ServiceUnavailable error."""
    return isinstance(exc, google_exceptions.ServiceUnavailable)

DEFAULT_RETRY = Retry(
    predicate=is_service_unavailable,
    initial=1.0,      # Start with a 1-second delay
    maximum=10.0,     # Maximum delay of 10 seconds
    multiplier=2.0,   # Double the delay each time
    deadline=30.0,    # Total deadline for all retries
)

DEFAULT_REQUEST_OPTIONS = {"retry": DEFAULT_RETRY, "timeout": 15.0}


# --- Client Initializers (Cached for Performance) ---

@lru_cache(maxsize=1)
def get_google_ai_client() -> genai:
    """Initializes and returns the Google AI client."""
    try:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")
        genai.configure(api_key=api_key)
        # This line helps prevent certain region-based connection issues.
        get_default_generative_client()._client_options = ClientOptions(api_endpoint="generativelanguage.googleapis.com")
        logger.info("Google AI client configured successfully.")
        return genai
    except Exception as e:
        logger.error(f"Failed to configure Google AI client: {e}")
        return None

@lru_cache(maxsize=2)
def get_generative_model(model_name: str = 'gemini-1.5-pro-latest') -> genai.GenerativeModel:
    client = get_google_ai_client()
    if not client: return None
    logger.info(f"Requesting GenerativeModel: {model_name}")
    return client.GenerativeModel(model_name)

@lru_cache(maxsize=2)
def get_flash_model(model_name: str = 'gemini-1.5-flash-latest') -> genai.GenerativeModel:
    client = get_google_ai_client()
    if not client: return None
    logger.info(f"Requesting Flash Model: {model_name}")
    return client.GenerativeModel(model_name)

@lru_cache(maxsize=1)
def get_pinecone_index() -> pinecone.Index:
    """Initializes and returns the Pinecone index client."""
    try:
        api_key = os.getenv("PINECONE_API_KEY")
        index_name = os.getenv("PINECONE_INDEX_NAME")
        if not api_key or not index_name:
            raise ValueError("PINECONE_API_KEY or PINECONE_INDEX_NAME not set.")
        
        pc = pinecone.Pinecone(api_key=api_key)
        index = pc.Index(index_name)
        logger.info(f"Pinecone index '{index_name}' connected successfully.")
        return index
    except Exception as e:
        logger.error(f"Failed to connect to Pinecone index: {e}")
        return None

@lru_cache(maxsize=1)
def get_neo4j_driver() -> neo4j.Driver:
    """Initializes and returns the Neo4j graph database driver."""
    try:
        uri = os.getenv("NEO4J_URI")
        user = os.getenv("NEO4J_USERNAME", "neo4j")
        password = os.getenv("NEO4J_PASSWORD")
        if not all([uri, user, password]):
            raise ValueError("Neo4j connection details (URI, USERNAME, PASSWORD) not set.")

        driver = neo4j.GraphDatabase.driver(uri, auth=(user, password))
        driver.verify_connectivity()
        logger.info("Neo4j driver connected successfully.")
        return driver
    except Exception as e:
        logger.error(f"Failed to create Neo4j driver: {e}")
        return None


########################################################################
### FILE: src/tools/retrievers.py
########################################################################

# FILE: src/tools/retrievers.py
# V6.3 (Final Polish): Perfected page number formatting for a professional user experience.

import logging
import time
from typing import List, Dict, Any
import neo4j
import google.generativeai as genai

from src.tools.clients import get_flash_model, get_pinecone_index, get_neo4j_driver, DEFAULT_REQUEST_OPTIONS
from src.models import ToolResult, QueryMetadata
from src.prompts import CYPHER_GENERATION_PROMPT

logger = logging.getLogger(__name__)

class Timer:
    def __init__(self, name): self.name = name
    def __enter__(self): self.start = time.perf_counter(); return self
    def __exit__(self, *args): self.end = time.perf_counter(); logger.info(f"[TIMER] {self.name} took {(self.end - self.start) * 1000:.2f} ms")

def _format_pinecone_results(matches: List[dict]) -> List[str]:
    contents = []
    MAX_PAGES_TO_SHOW = 4

    for match in matches:
        metadata = match.get('metadata', {})
        text = metadata.get('text', 'No content available.')
        doc_id = metadata.get('doc_id', 'Unknown Document')
        page_numbers_raw = metadata.get('page_numbers', [])
        url = metadata.get('source_pdf_url', '#')
        
        page_str, link_url = "N/A", url

        if page_numbers_raw and all(isinstance(p, (str, int, float)) for p in page_numbers_raw):
            try:
                unique_pages = sorted(list(set(map(int, page_numbers_raw))))
                
                if len(unique_pages) > MAX_PAGES_TO_SHOW:
                    page_str = f"Pages {', '.join(map(str, unique_pages[:MAX_PAGES_TO_SHOW]))}, ..."
                elif len(unique_pages) > 1:
                    page_str = f"Pages {', '.join(map(str, unique_pages))}"
                else:
                    page_str = f"Page {unique_pages[0]}"
                link_url = f"{url}#page={unique_pages[0]}"
            except (ValueError, TypeError):
                 page_str, link_url = ", ".join(map(str, page_numbers_raw)), url
        
        citation = f'<a href="{link_url}" target="_blank">{doc_id} ({page_str})</a>'
        contents.append(f"Evidence from document: {text}\nCitation: {citation}")
    return contents

# (The rest of the file is unchanged from the last working version)
def _serialize_neo4j_path(record: Dict[str, Any]) -> str:
    path_data, rel_props = record.get("p"), record.get("rel_props")
    if not path_data: return ""
    subject_name, predicate_type, object_name = None, None, None
    try:
        if isinstance(path_data, neo4j.graph.Path):
            subject_name, predicate_type, object_name = path_data.start_node.get('name'), path_data.relationships[0].type, path_data.end_node.get('name')
        elif isinstance(path_data, list) and len(path_data) == 3:
            subject_name, predicate_type, object_name = path_data[0].get('name'), path_data[1], path_data[2].get('name')
        if not all([subject_name, predicate_type, object_name]): return ""
        predicate_str = predicate_type.replace('_', ' ').lower()
        text_representation = f"{subject_name} {predicate_str} {object_name}."
        citation_text, link_url = "Knowledge Graph", "#"
        if isinstance(rel_props, dict):
            doc_id, url, page_num = rel_props.get('doc_id'), rel_props.get('source_pdf_url'), rel_props.get('page_numbers', 'N/A')
            if doc_id: citation_text = f"{doc_id} (Page {page_num})"
            if url: link_url = f"{url}#page={str(page_num).split(',')[0]}"
        citation = f'<a href="{link_url}" target="_blank">{citation_text}</a>'
        return f"Evidence from graph: {text_representation}\nCitation: {citation}"
    except Exception as e:
        logger.warning(f"Could not serialize Neo4j path: {e}")
        return ""

def vector_search(query: str, query_meta: QueryMetadata) -> ToolResult:
    tool_name = "vector_search"; namespace = "pbac-text"
    with Timer(f"Tool: {tool_name}"):
        pinecone_index = get_pinecone_index()
        if not pinecone_index: return ToolResult(tool_name=tool_name, success=False, content="Pinecone not available.")
        metadata_filter = {}
        if query_meta and query_meta.themes:
            metadata_filter["semantic_purpose"] = {"$in": query_meta.themes}
            logger.info(f"Applying metadata filter: {metadata_filter}")
        try:
            query_embedding = genai.embed_content(model='models/text-embedding-004', content=query, task_type="retrieval_query")
            response = pinecone_index.query(namespace=namespace, vector=query_embedding['embedding'], top_k=10, include_metadata=True, filter=metadata_filter or None)
            if not response.get('matches'): return ToolResult(tool_name=tool_name, success=True, content="")
            content_list = _format_pinecone_results(response['matches'])
            return ToolResult(tool_name=tool_name, success=True, content="\n---\n".join(content_list))
        except Exception as e:
            logger.error(f"Error in vector search: {e}", exc_info=True)
            return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred: {e}")

def query_knowledge_graph(query: str, query_meta: QueryMetadata) -> ToolResult:
    tool_name = "query_knowledge_graph"
    with Timer(f"Tool: {tool_name}"):
        llm, driver = get_flash_model(), get_neo4j_driver()
        if not llm or not driver: return ToolResult(tool_name=tool_name, success=False, content="Clients not available.")
        try:
            with driver.session() as session: schema_data = session.run("CALL db.schema.visualization()").data()
            schema_str = f"Node labels: {schema_data[0]['nodes']}\nRelationships: {schema_data[0]['relationships']}"
            prompt = CYPHER_GENERATION_PROMPT.format(schema=schema_str, question=query)
            response = llm.generate_content(prompt, request_options=DEFAULT_REQUEST_OPTIONS)
            cypher_query = response.text.strip().replace("```cypher", "").replace("```", "")
            if "none" in cypher_query.lower() or "match" not in cypher_query.lower(): return ToolResult(tool_name=tool_name, success=True, content="")
            logger.info(f"Generated Cypher: {cypher_query}")
            with driver.session() as session: records = session.run(cypher_query).data()
            if not records: return ToolResult(tool_name=tool_name, success=True, content="")
            results = [_serialize_neo4j_path(record) for record in records if record.get("p")]
            return ToolResult(tool_name=tool_name, success=True, content="\n".join(filter(None, results)))
        except Exception as e:
            logger.error(f"Error in KG tool: {e}", exc_info=True)
            return ToolResult(tool_name=tool_name, success=False, content=f"An error occurred: {e}")


########################################################################
### FILE: streamlit_app.py
########################################################################

# FILE: streamlit_app.py
# V3.1 (Definitive Fix): Corrected the agent call to be synchronous, fixing the asyncio ValueError.

import streamlit as st
import logging
import os
from dotenv import load_dotenv

# --- CRITICAL: Load environment variables at the very top ---
load_dotenv()

# --- Page and Logging Configuration ---
st.set_page_config(
    page_title="Persona RAG Chatbot",
    page_icon="🧠",
    layout="wide",
    initial_sidebar_state="expanded",
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - [%(name)s:%(lineno)d] - %(message)s'
)
logging.getLogger('streamlit').setLevel(logging.WARNING)
logging.getLogger('watchdog').setLevel(logging.WARNING)
logging.getLogger('PIL').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)

# Now import project modules
from src.agent import Agent
from src.tools.clients import get_google_ai_client # Used for a pre-flight check


# --- Session State Initialization ---
if "agent" not in st.session_state:
    st.session_state.agent = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_persona" not in st.session_state:
    st.session_state.current_persona = "automatic"

# --- Helper Functions (unchanged) ---
@st.cache_resource
def initialize_agent():
    if not get_google_ai_client():
        st.error("Google API Key is not configured. Please set the GOOGLE_API_KEY in your .env file.", icon="🚨")
        return None
    try:
        agent = Agent()
        logger.info("Unified agent initialized successfully and cached for the session.")
        return agent
    except Exception as e:
        st.error(f"Fatal error during agent initialization: {e}", icon="🚨")
        logger.error(f"Agent initialization failed: {e}", exc_info=True)
        return None

def reset_chat(persona_name: str):
    display_name = "Automatic" if persona_name == "Automatic (Recommended)" else persona_name
    st.session_state.messages = [
        {"role": "assistant", "content": f"Hi! I'm now acting in **{display_name}** mode. How can I help you?"}
    ]

# --- Sidebar (unchanged) ---
with st.sidebar:
    st.header("🤖 Persona RAG Chatbot")
    st.markdown("Select a persona to tailor my retrieval strategy and answers to your specific role.")
    persona_options = {
        'Automatic (Recommended)': 'automatic',
        'Clinical Analyst': 'clinical_analyst',
        'Health Economist': 'health_economist',
        'Regulatory Specialist': 'regulatory_specialist',
    }
    current_display_name = [k for k, v in persona_options.items() if v == st.session_state.current_persona][0]
    selected_persona_name = st.radio(
        "**Choose your Mode:**",
        options=persona_options.keys(),
        index=list(persona_options.keys()).index(current_display_name),
        key="persona_selector"
    )
    selected_persona_key = persona_options[selected_persona_name]
    if selected_persona_key != st.session_state.current_persona:
        st.session_state.current_persona = selected_persona_key
        reset_chat(selected_persona_name)
        st.rerun()
    st.divider()
    if st.button("🔄 Clear Chat History", use_container_width=True):
        reset_chat(selected_persona_name)
        st.rerun()
    st.divider()
    st.header("🧪 Evaluation Questions")
    st.markdown("Use these questions to test the agent's capabilities with the 2025 data.")

    with st.expander("🎯 Fact Retrieval (2025 Data)", expanded=True):
        questions = {
            "Sponsor Lookup (July 2025)": "What company is the sponsor for Abaloparatide?",
            "Indication Lookup (March 2025)": "What is Amivantamab used to treat?",
            "Trade Name Lookup (May 2025)": "What is the trade name for Dostarlimab?",
            "Dosage Form (July 2025)": "What is the dosage form of Apomorphine?",
        }
        for name, q in questions.items():
            if st.button(f"{name}: {q}", key=q, use_container_width=True):
                st.session_state.run_prompt = q

    with st.expander("⚖️ Multi-Step Reasoning (2025 Data)"):
        questions = {
            "Intersection Query": "Which sponsors made submissions in both the March 2025 and July 2025 meetings?",
            "Comparative Query": "Compare the submission purposes for Acalabrutinib and Alectinib in the 2025 meetings.",
            "Multi-Hop Query": "What is the indication for the drug whose trade name is Cabometyx?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q
    
    with st.expander("📋 Summarization (2025 Data)"):
        questions = {
            "Summarize a Meeting": "Provide a summary of the key submissions from the May 2025 PBAC meeting.",
            "Summarize by Theme": "Summarize all submissions related to oncology in the March 2025 documents.",
            "Synthesize High-Level Goal": "Based on the agendas, what appears to be the main focus of the PBAC's work in 2025?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q

    with st.expander("🤔 Challenging / Ambiguous Questions"):
        questions = {
            "Test Data Boundaries": "What was the final PBAC decision on Ribociclib from the July 2025 meeting?",
            "Test Fallback Logic (No Price)": "What is the price of Tirzepatide?",
            "Out of Scope (External Knowledge)": "What are the latest FDA guidelines on biosimilars?",
        }
        for name, q in questions.items():
            if st.button(q, key=q, use_container_width=True):
                st.session_state.run_prompt = q

# --- Main Chat Interface (unchanged) ---
st.title("Persona-Aware RAG Agent")
if st.session_state.current_persona == "automatic":
    st.caption("Currently in **Automatic Mode** (selects best persona per query)")
else:
    st.caption(f"Currently acting as: **{selected_persona_name}**")
with st.container(border=True):
    st.info("""
    **Welcome! This is an advanced chatbot designed to answer questions about pharmaceutical and regulatory documents.** 
    
    Its unique feature is the ability to tailor its information retrieval strategy based on the professional role you select in the sidebar.

    **How to use this demo:**
    1.  **Choose Your Mode:** Select 'Automatic' (Recommended) or a specific persona from the sidebar.
    2.  **Ask a Question:** Use the pre-defined 'Evaluation Questions' or type your own question in the chat box below.
    """)
    st.markdown("<p style='text-align: center; color: grey;'>A not-for-profit demonstration project by <b>EVIL_MIT</b></p>", unsafe_allow_html=True)

if st.session_state.agent is None:
    st.session_state.agent = initialize_agent()
    if not st.session_state.messages:
        reset_chat(selected_persona_name)

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"], unsafe_allow_html=True)

prompt_from_button = st.session_state.pop("run_prompt", None)
prompt_from_input = st.chat_input("Ask your question...")
prompt = prompt_from_button or prompt_from_input

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if st.session_state.agent:
            spinner_text = "Thinking..."
            if st.session_state.current_persona != "automatic":
                 spinner_text = f"Thinking as a {selected_persona_name}..."
            
            with st.spinner(spinner_text):
                history_for_rewrite = [f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:-1]]
                
                # --- START OF DEFINITIVE FIX ---
                # REMOVED asyncio.run() as agent.run is now a synchronous function.
                # This was the cause of the ValueError crash.
                response = st.session_state.agent.run(
                    prompt, 
                    persona=st.session_state.current_persona,
                    chat_history=history_for_rewrite
                )
                # --- END OF DEFINITIVE FIX ---
                
                st.markdown(response, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            st.error("Agent is not available due to an initialization error. Please check the terminal logs.")
            st.stop()
    
    if prompt_from_button:
        st.rerun()


########################################################################
### FILE: verify_pinecone.py
########################################################################

# FILE: verify_pinecone.py
import os
import pinecone
from dotenv import load_dotenv

# --- Configuration ---
# Change this to the document ID you want to check for.
DOCUMENT_ID_TO_CHECK = "May-2025-PBAC-Meeting-v5"
NAMESPACE_TO_CHECK = "pbac-text" # This is our unified namespace

# --- Main Script ---
def verify_document_in_pinecone():
    """Connects to Pinecone and verifies if vectors for a specific doc_id exist."""
    print("--- Pinecone Data Verifier ---")
    
    # 1. Load Environment Variables
    load_dotenv()
    api_key = os.getenv("PINECONE_API_KEY")
    index_name = os.getenv("PINECONE_INDEX_NAME")

    if not api_key or not index_name:
        print("❌ ERROR: PINECONE_API_KEY or PINECONE_INDEX_NAME not set in your .env file.")
        return

    print(f"Connecting to index '{index_name}'...")
    try:
        pc = pinecone.Pinecone(api_key=api_key)
        index = pc.Index(index_name)
        stats = index.describe_index_stats()
        print(f"✅ Successfully connected. Index has {stats['total_vector_count']} total vectors.")
    except Exception as e:
        print(f"❌ ERROR: Could not connect to Pinecone. Details: {e}")
        return

    # 2. Perform the Metadata-Filtered Query
    print(f"\nSearching for vectors with doc_id: '{DOCUMENT_ID_TO_CHECK}' in namespace '{NAMESPACE_TO_CHECK}'...")
    
    try:
        # We don't need a real query vector; the filter is what matters.
        # We query for a zero vector of the correct dimension (768 for mpnet).
        query_vector = [0.0] * 768 
        
        response = index.query(
            namespace=NAMESPACE_TO_CHECK,
            vector=query_vector,
            top_k=10,
            include_metadata=True,
            filter={
                "doc_id": {"$eq": DOCUMENT_ID_TO_CHECK}
            }
        )

        # 3. Analyze and Report Results
        matches = response.get('matches', [])
        if not matches:
            print("\n" + "="*50)
            print(f"🔴 VERIFICATION FAILED: No vectors found for '{DOCUMENT_ID_TO_CHECK}'.")
            print("   This confirms the data was NOT uploaded correctly.")
            print("="*50 + "\n")
        else:
            print("\n" + "="*50)
            print(f"🟢 VERIFICATION SUCCESSFUL: Found {len(matches)} vectors for '{DOCUMENT_ID_TO_CHECK}'.")
            print("   The data is present in your Pinecone index.")
            print("\n--- Sample of first result's metadata ---")
            print(matches[0].metadata)
            print("="*50 + "\n")

    except Exception as e:
        print(f"❌ ERROR: An error occurred during the query. Details: {e}")

if __name__ == "__main__":
    verify_document_in_pinecone()