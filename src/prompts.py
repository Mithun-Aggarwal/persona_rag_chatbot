# FILE: src/prompts.py
# V3.0 (ReAct Architecture): Added prompts for multi-step reasoning.

"""
Production-grade prompts for a robust RAG agent. This version supports both
direct RAG and a multi-step ReAct (Reason+Act) style reasoning loop.
"""

# ... (other prompts remain the same) ...

# --- DEFINITIVE FIX: A clearer, more accurate classification prompt ---
QUERY_CLASSIFICATION_PROMPT_V2 = """
You are an expert query analysis agent. Your task is to analyze the user's question and provide a structured JSON output with four fields.

**Fields to Generate:**

1.  `intent`: Classify the user's goal (e.g., "specific_fact_lookup", "simple_summary").
2.  `keywords`: Extract key nouns like drug names, company names, etc.
3.  `themes`: Extract high-level conceptual themes from the question. Choose from this list: ["Oncology", "Regulatory History", "Efficacy Results", "Safety Results", "Clinical Trial Design", "Pharmacoeconomic Analysis", "Dosage and Administration", "Drug/Therapy Description", "Indication/Population Description"]. Return an empty list if no theme applies.
4.  `question_is_graph_suitable`: Return `true` if the question asks for a direct relationship between two specific entities (e.g., "Who sponsors DrugX?", "What does DrugY treat?"). Return `false` for summaries, comparisons, or general questions.

**CRITICAL INSTRUCTIONS:**
- Output ONLY the raw JSON object. Do not add explanations or markdown code fences.
- If the question is "What is Amivantamab used to treat?", the relationship is (Amivantamab -> used to treat -> ?), so `question_is_graph_suitable` MUST be `true`.
- If the question is "Summarize the May meeting", there is no direct relationship, so `question_is_graph_suitable` MUST be `false`.
"""

CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema.

**Live Graph Schema:**
{schema}

**CRITICAL Instructions:**
1.  Your `RETURN` clause should look like this: `RETURN p, properties(r) as rel_props`.
2.  The relationship in your `MATCH` clause must be assigned to a variable `r`.
3.  Query against the `name_normalized` property for all `WHERE` clauses on nodes.
4.  If the question cannot be answered, you MUST return the single word: `NONE`.
5.  Output ONLY the Cypher query or the word `NONE`.

**Example Question:** "What company sponsors Abaloparatide?"
**Example Valid Query:** MATCH p=(drug:Entity)-[r:HASSPONSOR]->(sponsor:Entity) WHERE drug.name_normalized = 'abaloparatide' RETURN p, properties(r) as rel_props

**Question:** {question}
"""

# ... (other prompts) ...
CYPHER_GENERATION_PROMPT = "..."
DECOMPOSITION_PROMPT = "..."
REASONING_SYNTHESIS_PROMPT = "..."
DIRECT_SYNTHESIS_PROMPT = "..."

CYPHER_GENERATION_PROMPT = """
You are an expert Neo4j Cypher query developer. Your task is to convert a user's question into a single, valid, read-only Cypher query based on the provided graph schema.

**Live Graph Schema:**
{schema}

**CRITICAL Instructions:**
1.  Your `RETURN` clause should look like this: `RETURN p, properties(r) as rel_props`.
2.  The relationship in your `MATCH` clause must be assigned to a variable `r`.
3.  Query against the `name_normalized` property for all `WHERE` clauses on nodes.
4.  If the question cannot be answered, you MUST return the single word: `NONE`.
5.  Output ONLY the Cypher query or the word `NONE`.

**Example Question:** "What company sponsors Abaloparatide?"
**Example Valid Query:** MATCH p=(drug:Entity)-[r:HASSPONSOR]->(sponsor:Entity) WHERE drug.name_normalized = 'abaloparatide' RETURN p, properties(r) as rel_props

**Question:** {question}
"""

# --- START OF DEFINITIVE FIX ---

DECOMPOSITION_PROMPT = """
You are a master query planner. Your goal is to determine if a user's question can be answered in a single step or if it requires decomposition into multiple, simpler sub-questions.

Analyze the user's question and the chat history.

**Decision Criteria:**
- **Single Step:** If the question asks for a direct fact, a summary of a single topic, or a simple definition.
- **Decomposition:** If the question requires comparing information from two or more distinct topics (e.g., two drugs, two meetings), finding an intersection of two sets of information (e.g., "sponsors who submitted in BOTH meetings"), or involves a sequence of steps.

**Output Schema:**
You MUST output a single, valid JSON object with two keys:
1.  `requires_decomposition`: A boolean (`true` or `false`).
2.  `plan`: A list of strings.
    - If `requires_decomposition` is `false`, the plan should contain a single item: the original question.
    - If `requires_decomposition` is `true`, the plan should contain two or more simple, answerable sub-questions that build on each other to answer the original question.

**Example 1 (Single Step):**
- User Question: "What is the use of Esketamine?"
- Your JSON Output:
{{
  "requires_decomposition": false,
  "plan": ["What is the use of Esketamine?"]
}}

**Example 2 (Decomposition):**
- User Question: "Which companies submitted drugs in both the March 2024 and May 2024 PBAC meetings?"
- Your JSON Output:
{{
  "requires_decomposition": true,
  "plan": [
    "List all sponsors who made submissions in the March 2024 PBAC meeting documents.",
    "List all sponsors who made submissions in the May 2024 PBAC meeting documents."
  ]
}}

**TASK:**
- Chat History: {chat_history}
- User Question: {question}

Now, generate the JSON output.
"""

# --- END OF DEFINITIVE FIX ---

REASONING_SYNTHESIS_PROMPT = """
You are a highly intelligent synthesis agent. Your task is to answer a user's complex original question based on a series of observations you have made by answering simpler sub-questions.

**User's Original Question:** "{question}"

**Your Observations (Scratchpad):**
---
{scratchpad}
---

**CRITICAL INSTRUCTIONS:**
1.  Read the user's original question and all your observations from the scratchpad.
2.  Synthesize a final, comprehensive answer to the original question.
3.  **Do not show your step-by-step reasoning.** Just provide the final, clean answer.
4.  If your observations are insufficient to answer the question, clearly state what information you found and why it is not enough.
5.  Include citations from your observations where appropriate.

**Final Answer:**
"""

# --- END: New Prompts for ReAct Agent ---


# --- DEFINITIVE FIX: New prompt for in-line citations and clean responses ---
DIRECT_SYNTHESIS_PROMPT = """
You are a precise, professional document analysis bot. Your ONLY job is to answer the user's question based strictly on the provided evidence.

**TASK:**
1.  Read the User's Question.
2.  Read the numbered evidence blocks (`EVIDENCE [1]`, `EVIDENCE [2]`, etc.).
3.  Synthesize a direct, professional answer to the question.
4.  When you use information from a piece of evidence, you **MUST** cite it by placing its corresponding number in brackets, like `[1]`.
5.  Cite each piece of evidence you use. If multiple pieces of evidence support a single point, you can cite them together, like `[1][2]`.
6.  Your answer must be based **ONLY** on the provided evidence. Do not add outside knowledge.
7.  If the evidence is insufficient to answer the question, you **MUST** state that the provided evidence does not contain the answer.

**EXAMPLE:**
User's Question: "What is the sponsor and dosage form for Abaloparatide?"
Evidence:
EVIDENCE [1]:
Evidence from graph: ABALOPARATIDE has sponsor THERAMEX AUSTRALIA PTY LTD.

EVIDENCE [2]:
Evidence from document: The submission for Abaloparatide was for a 3mg dosage form.

Your Answer:
The sponsor for Abaloparatide is Theramex Australia Pty Ltd [1]. The dosage form submitted was 3 mg [2].

---
**User's Question:** "{question}"
---
**Evidence:**
{context_str}
---
**Your Answer:**
"""

# --- NEW PROMPT FOR GEMINI-BASED RE-RANKING ---
RERANKING_PROMPT = """
You are a highly intelligent and precise relevance-ranking model. Your task is to analyze a user's question and a list of retrieved documents, and then return a JSON list of the document indices that are most relevant for answering the question.

**CRITICAL INSTRUCTIONS:**
1.  Read the user's question to understand their core intent.
2.  Read each document, identified by its index (e.g., `DOCUMENT[0]`, `DOCUMENT[1]`).
3.  Determine which documents contain direct, explicit information that helps answer the question.
4.  Your output **MUST** be a single, valid JSON array of integers, representing the indices of the most relevant documents, sorted from most relevant to least relevant.
5.  Include **ONLY** the indices of documents that are directly relevant. If a document is only tangentially related, do not include its index.
6.  If **NO** documents are relevant, return an empty JSON array `[]`.
7.  Do not include more than the top 5 most relevant document indices.

**EXAMPLE:**
- **User Question:** "What is the dosage form for Apomorphine?"
- **Documents:**
  DOCUMENT[0]:
  Evidence from document: The sponsor for Apomorphine is STADA...
  Citation: <a...>
  
  DOCUMENT[1]:
  Evidence from document: MovapoÂ® (apomorphine hydrochloride hemihydrate) is available as a solution for subcutaneous infusion...
  Citation: <a...>
  
  DOCUMENT[2]:
  Evidence from document: The PBAC recommended the listing of Abaloparatide...
  Citation: <a...>

- **Your JSON Output:**
  [1]

---
**TASK:**

**User Question:** "{question}"

**Documents:**
{documents}

**Your JSON Output:**
"""